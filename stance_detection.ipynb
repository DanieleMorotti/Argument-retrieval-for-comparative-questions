{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf7ac58-6b1d-4a86-a9a9-7aa225befd02",
   "metadata": {
    "id": "aaf7ac58-6b1d-4a86-a9a9-7aa225befd02",
    "tags": []
   },
   "source": [
    "# Stance detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3200f066-4d6f-40d2-acbe-7436e3a31465",
   "metadata": {
    "id": "3200f066-4d6f-40d2-acbe-7436e3a31465"
   },
   "source": [
    "Stance detection for comparative questions is the task consisting of determining, given a comparative question $Q$ between two objects $O_1$ and $O_2$ and an answer $A$, the stance of $A$ with respect to $Q$. The answer can either express:\n",
    "- No stance;\n",
    "- Neutrality;\n",
    "- Being in favour of $O_1$;\n",
    "- Being in favour of $O_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13740635-ed1f-4417-ab3c-d367e1d08325",
   "metadata": {
    "id": "13740635-ed1f-4417-ab3c-d367e1d08325"
   },
   "source": [
    "Needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "LDhS9y7VFBQK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDhS9y7VFBQK",
    "outputId": "e18e0b1a-c145-4deb-d445-e414858afbb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\andrea\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: requests in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andrea\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e949c0da-16e8-473e-9697-a509c432b8fd",
   "metadata": {
    "id": "e949c0da-16e8-473e-9697-a509c432b8fd"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from transformers import logging, DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "import utils.manage_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04c2bfe-a62d-4ee8-8395-2b31321a045f",
   "metadata": {
    "id": "f04c2bfe-a62d-4ee8-8395-2b31321a045f"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StanceConfig:\n",
    "    DATASET_PATH = 'touche22-task2-stance-dataset.tsv'\n",
    "    INIT_LR = 1e-5\n",
    "    LR_REDUCTION_FACTOR_PLATEAU = .1\n",
    "    MIN_LR = 1e-8\n",
    "    \n",
    "    MODEL_BATCH_SIZE = 16\n",
    "    MODEL_CHECKPOINT_FOLDER = './model_checkpoint/'\n",
    "    MODEL_EPOCHS = 150\n",
    "    \n",
    "    MODEL_NNO_BATCH_SIZE = 16\n",
    "    MODEL_NNO_CHECKPOINT_FOLDER = './model_nno_checkpoint/'\n",
    "    MODEL_NNO_EPOCHS = 150\n",
    "    \n",
    "    MODEL_NNO_WEIGHTED_BATCH_SIZE = 16\n",
    "    MODEL_NNO_WEIGHTED_CHECKPOINT_FOLDER = './model_nno_checkpoint_weighted/'\n",
    "    MODEL_NNO_WEIGHTED_EPOCHS = 150\n",
    "    \n",
    "    MODEL_OBJ_BATCH_SIZE = 16\n",
    "    MODEL_OBJ_CHECKPOINT_FOLDER = './model_obj_checkpoint/'\n",
    "    MODEL_OBJ_EPOCHS = 150\n",
    "    \n",
    "    MODEL_OBJ_WEIGHTED_BATCH_SIZE = 16\n",
    "    MODEL_OBJ_WEIGHTED_CHECKPOINT_FOLDER = './model_obj_checkpoint_weighted/'\n",
    "    MODEL_OBJ_WEIGHTED_EPOCHS = 150\n",
    "    \n",
    "    SEED = 42\n",
    "    TRAIN_VAL_TEST_RATIOS = {'train': .8, 'val': .1, 'test': .1}\n",
    "\n",
    "stance_config = StanceConfig()\n",
    "logging.set_verbosity(40) #Due to an annoying repeated warning given by the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c0055-145c-4354-9631-79af68d5c058",
   "metadata": {
    "id": "919c0055-145c-4354-9631-79af68d5c058"
   },
   "source": [
    "## Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed53a6c7-8195-404f-bed0-70691c4fd796",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "id": "ed53a6c7-8195-404f-bed0-70691c4fd796",
    "outputId": "6c33ffe6-ebad-44e3-b94a-615c7231eadc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_stance</th>\n",
       "      <th>answer_stance_object</th>\n",
       "      <th>object_count</th>\n",
       "      <th>object_1</th>\n",
       "      <th>mask_pos_1</th>\n",
       "      <th>object_2</th>\n",
       "      <th>mask_pos_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>10373</td>\n",
       "      <td>What's the better way to charge for a cloud pl...</td>\n",
       "      <td>Like all good questions, the answer depends. I...</td>\n",
       "      <td>2</td>\n",
       "      <td>simple but more expensive</td>\n",
       "      <td>2</td>\n",
       "      <td>simple but more expensive</td>\n",
       "      <td>[[231, 252], [389, 393], [605, 619], [753, 757]]</td>\n",
       "      <td>complicated but cheaper</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>18838</td>\n",
       "      <td>Haskell AND Lisp vs. Haskell OR Lisp</td>\n",
       "      <td>I suggest learning both, Haskell first, then C...</td>\n",
       "      <td>2</td>\n",
       "      <td>Haskell AND Lisp</td>\n",
       "      <td>2</td>\n",
       "      <td>Haskell AND Lisp</td>\n",
       "      <td>[[25, 56]]</td>\n",
       "      <td>Haskell OR Lisp</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>19392</td>\n",
       "      <td>When is it better to offload work to the RDBMS...</td>\n",
       "      <td>You want to do all set-based operations in the...</td>\n",
       "      <td>3</td>\n",
       "      <td>do it in code</td>\n",
       "      <td>2</td>\n",
       "      <td>offload work to the RDBMS</td>\n",
       "      <td>[[40, 55], [232, 248]]</td>\n",
       "      <td>do it in code</td>\n",
       "      <td>[[165, 175], [578, 598]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>20653</td>\n",
       "      <td>Is it better to specialize in a single field I...</td>\n",
       "      <td>Specialise if you enjoy it  As you are aware, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>expand into other fields to broaden my horizons</td>\n",
       "      <td>2</td>\n",
       "      <td>to specialize in a single field I like</td>\n",
       "      <td>[[53, 63], [404, 410], [512, 519]]</td>\n",
       "      <td>expand into other fields to broaden my horizons</td>\n",
       "      <td>[[892, 933]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>21186</td>\n",
       "      <td>Microsoft SDE Interview vs Microsoft SDET Inte...</td>\n",
       "      <td>Unfortunately, those are both myths. SDEs and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "      <td>Microsoft SDE Interview</td>\n",
       "      <td>[[37, 41], [283, 295], [950, 962]]</td>\n",
       "      <td>Microsoft SDET Interview</td>\n",
       "      <td>[[46, 51], [341, 354], [832, 836], [938, 945]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ds     id  \\\n",
       "0  softwareengineering.stackexchange  10373   \n",
       "1  softwareengineering.stackexchange  18838   \n",
       "2  softwareengineering.stackexchange  19392   \n",
       "3  softwareengineering.stackexchange  20653   \n",
       "4  softwareengineering.stackexchange  21186   \n",
       "\n",
       "                                            question  \\\n",
       "0  What's the better way to charge for a cloud pl...   \n",
       "1               Haskell AND Lisp vs. Haskell OR Lisp   \n",
       "2  When is it better to offload work to the RDBMS...   \n",
       "3  Is it better to specialize in a single field I...   \n",
       "4  Microsoft SDE Interview vs Microsoft SDET Inte...   \n",
       "\n",
       "                                              answer  answer_stance  \\\n",
       "0  Like all good questions, the answer depends. I...              2   \n",
       "1  I suggest learning both, Haskell first, then C...              2   \n",
       "2  You want to do all set-based operations in the...              3   \n",
       "3  Specialise if you enjoy it  As you are aware, ...              3   \n",
       "4  Unfortunately, those are both myths. SDEs and ...              1   \n",
       "\n",
       "                              answer_stance_object  object_count  \\\n",
       "0                        simple but more expensive             2   \n",
       "1                                 Haskell AND Lisp             2   \n",
       "2                                    do it in code             2   \n",
       "3  expand into other fields to broaden my horizons             2   \n",
       "4                                          Neutral             2   \n",
       "\n",
       "                                 object_1  \\\n",
       "0               simple but more expensive   \n",
       "1                        Haskell AND Lisp   \n",
       "2               offload work to the RDBMS   \n",
       "3  to specialize in a single field I like   \n",
       "4                 Microsoft SDE Interview   \n",
       "\n",
       "                                         mask_pos_1  \\\n",
       "0  [[231, 252], [389, 393], [605, 619], [753, 757]]   \n",
       "1                                        [[25, 56]]   \n",
       "2                            [[40, 55], [232, 248]]   \n",
       "3                [[53, 63], [404, 410], [512, 519]]   \n",
       "4                [[37, 41], [283, 295], [950, 962]]   \n",
       "\n",
       "                                          object_2  \\\n",
       "0                          complicated but cheaper   \n",
       "1                                  Haskell OR Lisp   \n",
       "2                                    do it in code   \n",
       "3  expand into other fields to broaden my horizons   \n",
       "4                         Microsoft SDET Interview   \n",
       "\n",
       "                                       mask_pos_2  \n",
       "0                                              []  \n",
       "1                                              []  \n",
       "2                        [[165, 175], [578, 598]]  \n",
       "3                                    [[892, 933]]  \n",
       "4  [[46, 51], [341, 354], [832, 836], [938, 945]]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(stance_config.DATASET_PATH, sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65233553-cfcd-4976-b1a5-c0169ad23c1e",
   "metadata": {
    "id": "65233553-cfcd-4976-b1a5-c0169ad23c1e"
   },
   "source": [
    "For the current task, only the columns \"question\", \"answer\" and \"answer_stance\" are needed, since the aim of this section is to build a model able to tell the stance of the answer with respect to the asked question, as described. We also include the \"dataset\" column in order to be sure that we're hiding (as per license conditions) the records coming from the dataset kindly provided by Yahoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d9b56ad-34e5-490a-9ae4-612e86777f69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "9d9b56ad-34e5-490a-9ae4-612e86777f69",
    "outputId": "8136acb1-f062-446c-fb17-38c32d1f63a2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>What's the better way to charge for a cloud pl...</td>\n",
       "      <td>Like all good questions, the answer depends. I...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>Haskell AND Lisp vs. Haskell OR Lisp</td>\n",
       "      <td>I suggest learning both, Haskell first, then C...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>When is it better to offload work to the RDBMS...</td>\n",
       "      <td>You want to do all set-based operations in the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>Is it better to specialize in a single field I...</td>\n",
       "      <td>Specialise if you enjoy it  As you are aware, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>Microsoft SDE Interview vs Microsoft SDET Inte...</td>\n",
       "      <td>Unfortunately, those are both myths. SDEs and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ds  \\\n",
       "0  softwareengineering.stackexchange   \n",
       "1  softwareengineering.stackexchange   \n",
       "2  softwareengineering.stackexchange   \n",
       "3  softwareengineering.stackexchange   \n",
       "4  softwareengineering.stackexchange   \n",
       "\n",
       "                                            question  \\\n",
       "0  What's the better way to charge for a cloud pl...   \n",
       "1               Haskell AND Lisp vs. Haskell OR Lisp   \n",
       "2  When is it better to offload work to the RDBMS...   \n",
       "3  Is it better to specialize in a single field I...   \n",
       "4  Microsoft SDE Interview vs Microsoft SDET Inte...   \n",
       "\n",
       "                                              answer  answer_stance  \n",
       "0  Like all good questions, the answer depends. I...              2  \n",
       "1  I suggest learning both, Haskell first, then C...              2  \n",
       "2  You want to do all set-based operations in the...              3  \n",
       "3  Specialise if you enjoy it  As you are aware, ...              3  \n",
       "4  Unfortunately, those are both myths. SDEs and ...              1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[:, ['ds','question','answer','answer_stance']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f6b3d-4667-4839-87f0-edb82c77d145",
   "metadata": {},
   "source": [
    "We take a look at the distribution of the classes in the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a4fc84-92ed-4177-8d3f-73133988e5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.answer_stance.value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5509d2bf-7642-4c97-a219-9bdb4fa18a3e",
   "metadata": {},
   "source": [
    "We will take into account the fact that the distribution of the classes is not uniform, but not immediately."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea9f78d3-174f-412c-96c0-b68caf72162b",
   "metadata": {},
   "source": [
    "## Models implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a165610",
   "metadata": {},
   "source": [
    "### Distilbert with 4 classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "646352cb",
   "metadata": {},
   "source": [
    "For the task of stance detection, we agreed on the fact that a network based on Bidirectional Encoder Representations from Transformers (usually referred to as \"Bert\") was the way to go. In particular, we decided to fine-tune a network built upon the DistilBERT architecture ([typeform/distilbert-base-uncased-mnli · Hugging Face](https://huggingface.co/typeform/distilbert-base-uncased-mnli))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17cd1a-8608-493a-93a1-bbdb5f4b0a50",
   "metadata": {},
   "source": [
    "Retrieving the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3fdcfe2-3abf-44ce-83fd-372de812b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"typeform/distilbert-base-uncased-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e2bb07-0e8a-4c5c-812d-d7925f1448ce",
   "metadata": {},
   "source": [
    "Retrieving the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "221794d0-b6af-4a3c-ac10-e2f17b5fd34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"typeform/distilbert-base-uncased-mnli\", num_labels=4, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577cc067-7595-43f6-986c-f829aa76b590",
   "metadata": {},
   "source": [
    "Quick look at its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57870b02-a2f6-4c9f-811b-f55893a53688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3076      \n",
      "                                                                 \n",
      " dropout_99 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,956,548\n",
      "Trainable params: 66,956,548\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0518c96-c474-4f14-a145-a6e34c7a1063",
   "metadata": {},
   "source": [
    "We want to fine-tune the model, therefore we freeze the part of the network responsible of taking care of the *encoding* part of the process, and then we compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05a27660-3332-4397-aecc-53472db63c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distilbert.trainable = False\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = stance_config.INIT_LR),\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e65c9-3f43-4c9a-9e35-626aea92edde",
   "metadata": {},
   "source": [
    "Before proceeding, for reproducibility, we fix the random seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2605fb0-6865-4087-9e23-dd1b1d276d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_reproducibility(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Fixes a given seed for the pseudo-random number generators.\n",
    "    Args:\n",
    "        seed (int): seed to fix\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    #os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "set_reproducibility(stance_config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd702ca-51a1-402d-b8a5-4be57a52eeef",
   "metadata": {},
   "source": [
    "Defining the train, val, test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1246937-2004-4a0e-b2d3-fdcab19f5e91",
   "metadata": {
    "id": "f71d4cef-69ac-4e9a-85df-f3d0540b3b0e"
   },
   "outputs": [],
   "source": [
    "def train_val_test_splits(df: pd.DataFrame, ratios:dict, blueprint:bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Splits a given DataFrame into train, val and test, according to the provided ratios.\n",
    "    The dataframe is expected to contain the columns \"question\" and \"answer\". Other columns will be ignored.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame that has to be splitted into train, val and test sets.\n",
    "        ratios (dict): The dictionary containing the ratios between the contents of the train, val and test set. Expected keys: 'train', 'val', 'test'. Expected values: positive numbers not exceeding 1.\n",
    "        blueprint (bool): Flag to determine whether the function should just return the dictionary that will contain the required splits.\n",
    "    Returns:\n",
    "        dict: dictionary containing the required splits. Available keys: 'x_train', 'x_val', 'x_test', 'y_train', 'y_val', 'y_test'.\n",
    "    \"\"\"\n",
    "\n",
    "    if blueprint:\n",
    "        return {'x_train': None, 'x_val': None, 'x_test': None, 'y_train': None, 'y_val': None, 'y_test': None}\n",
    "    \n",
    "    #Separating \"train\" from \"val and test\"\n",
    "    x_train, x_val_test, y_train, y_val_test = train_test_split(df.loc[:,['question','answer']],df.loc[:,['answer_stance']],test_size=ratios['test'] + ratios['val'], train_size=ratios['train'])\n",
    "    \n",
    "    #Separating \"val\" from \"test\"\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_val_test,y_val_test,test_size = ratios['test']/(ratios['test'] + ratios['val']), train_size=ratios['val']/(ratios['test'] + ratios['val']))\n",
    "    \n",
    "    return {'x_train': x_train, 'x_val': x_val, 'x_test': x_test, 'y_train': y_train, 'y_val': y_val, 'y_test': y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b4c3fee-ebb1-419d-b30a-c907ca20739b",
   "metadata": {
    "id": "0f54f921-5254-49eb-a925-322bb93a0a39"
   },
   "outputs": [],
   "source": [
    "ratios = stance_config.TRAIN_VAL_TEST_RATIOS\n",
    "list_ratios = list(ratios.values())\n",
    "\n",
    "#Ensuring that the values represent proper proportions (no zeros allowed).\n",
    "assert np.sum(list_ratios) == 1 and np.sum(np.sign(list_ratios)) == 3, 'Please specfy valid ratios for the \"train, val, test\" split.'\n",
    "\n",
    "ds_splits_untok = train_val_test_splits(df, ratios)\n",
    "ds_splits = train_val_test_splits(None, None, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b6e53-0977-4a3f-8782-1f0d23d48709",
   "metadata": {
    "id": "7a936fe6-96d4-4b34-8708-7f93a91b8320"
   },
   "source": [
    "Processing the sets with the selected `DistilBertTokenizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f800a953-19db-4d41-8fa4-093e733c9325",
   "metadata": {
    "id": "661aa56c-b388-4cdd-b5bb-de05604297f5"
   },
   "outputs": [],
   "source": [
    "def tokenize_df(question_answer_df: pd.DataFrame, tokenizer: DistilBertTokenizer) -> dict:\n",
    "    \"\"\"\n",
    "    Given a DistilBertTokenizer, the function returns a dictionary containing the tokenized version of the inputs that will be fed to the networks.\n",
    "    \n",
    "    Args:\n",
    "        question_answer_df (pandas.DataFrame): The DataFrame whose contents have to be tokenized.\n",
    "        tokenizer (DistilBertTokenizer): The DistilBertTokenizer instance loaded from a pre-trained model.\n",
    "    Returns:\n",
    "        dict: dictionary containing the tokenized version of the inputs that will be fed to the networks.\n",
    "    \"\"\"\n",
    "    \n",
    "    #From the dataframe we build the list of lists which contains the values of the dataframe. In particular, each\n",
    "    #element of the outer list is a two-elements list containing the values of a certain row of the dataframe.\n",
    "    LL = question_answer_df.values.tolist()\n",
    "    \n",
    "    #Now, the tokenizer can generate the proper arrays that we'll feed to the networks.    \n",
    "    return dict(tokenizer(LL, padding=True, truncation='longest_first', return_tensors='tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ced8d06d-364b-4930-9427-afd15efcf117",
   "metadata": {
    "id": "e20acbe8-15b0-44fd-876a-049b6be2f7a0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenizing the inputs and preparing the outputs\n",
    "for set_name in ['train','val','test']:\n",
    "    ds_splits[f'x_{set_name}'] = tokenize_df(ds_splits_untok[f'x_{set_name}'],tokenizer)\n",
    "    ds_splits[f'y_{set_name}'] = ds_splits_untok[f'y_{set_name}'].to_numpy().flatten()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3bae258-ec0d-44ab-8edf-1e59981868a1",
   "metadata": {
    "id": "415e181f-1144-4c2c-9293-0a9df8c7822a"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8173f666",
   "metadata": {},
   "source": [
    "We configure two callbacks, one for saving the model weights at the best epoch (***ModelCheckpoint***) and the other to reduce gradually the learning rate (***ReduceLROnPlateau***)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93cab585",
   "metadata": {},
   "source": [
    "We configure 2 callbacks for the training, ***ModelCheckpoint*** to save the weights at the best epoch and ***ReduceLROnPlateau*** to reduce the learning rate during the training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d1b1168-de7f-49d8-a249-3ed3f4e2ea26",
   "metadata": {
    "id": "d6cc39e2-20f7-4e99-b02b-5d2e65b9a95d"
   },
   "source": [
    "Ensuring that the folder needed to store the best weights of the network exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "356953fa-81ec-4263-8ed8-31f9a7e6386c",
   "metadata": {
    "id": "01b35863-029a-4e59-8393-225561b904cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint folder created (if not existent). If the training has been already done, please copy the corresponding weights in the following folder:\n",
      "C:\\Users\\Andrea\\Script Python\\model_checkpoint\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(stance_config.MODEL_CHECKPOINT_FOLDER, exist_ok = True)\n",
    "\n",
    "print('Model checkpoint folder created (if not existent). If the training has been already done, please copy the corresponding weights in the following folder:')\n",
    "print(os.path.abspath(stance_config.MODEL_CHECKPOINT_FOLDER))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75ff2e94-f3c3-47d5-b576-ca963ce07a5f",
   "metadata": {},
   "source": [
    "Starting the training of the networks (or loading the weights if available):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e93827ee-f527-4e25-a905-7788e839e44d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d4ed6fb-0a89-4c18-9ba1-f06206efa109",
    "outputId": "2e9adfd4-362f-4a36-8f68-eefe484744c8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3951 - sparse_categorical_accuracy: 0.2291\n",
      "Epoch 1: val_loss improved from inf to 1.36376, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 25s 386ms/step - loss: 1.3951 - sparse_categorical_accuracy: 0.2291 - val_loss: 1.3638 - val_sparse_categorical_accuracy: 0.2708 - lr: 1.0000e-05\n",
      "Epoch 2/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3419 - sparse_categorical_accuracy: 0.2984\n",
      "Epoch 2: val_loss improved from 1.36376 to 1.33243, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 20s 429ms/step - loss: 1.3419 - sparse_categorical_accuracy: 0.2984 - val_loss: 1.3324 - val_sparse_categorical_accuracy: 0.2917 - lr: 1.0000e-05\n",
      "Epoch 3/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3216 - sparse_categorical_accuracy: 0.3154\n",
      "Epoch 3: val_loss improved from 1.33243 to 1.30938, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 378ms/step - loss: 1.3216 - sparse_categorical_accuracy: 0.3154 - val_loss: 1.3094 - val_sparse_categorical_accuracy: 0.2708 - lr: 1.0000e-05\n",
      "Epoch 4/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3035 - sparse_categorical_accuracy: 0.3272\n",
      "Epoch 4: val_loss improved from 1.30938 to 1.29437, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 366ms/step - loss: 1.3035 - sparse_categorical_accuracy: 0.3272 - val_loss: 1.2944 - val_sparse_categorical_accuracy: 0.2917 - lr: 1.0000e-05\n",
      "Epoch 5/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2868 - sparse_categorical_accuracy: 0.3730\n",
      "Epoch 5: val_loss improved from 1.29437 to 1.28753, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.2868 - sparse_categorical_accuracy: 0.3730 - val_loss: 1.2875 - val_sparse_categorical_accuracy: 0.3021 - lr: 1.0000e-05\n",
      "Epoch 6/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2851 - sparse_categorical_accuracy: 0.3364\n",
      "Epoch 6: val_loss improved from 1.28753 to 1.28189, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 367ms/step - loss: 1.2851 - sparse_categorical_accuracy: 0.3364 - val_loss: 1.2819 - val_sparse_categorical_accuracy: 0.3125 - lr: 1.0000e-05\n",
      "Epoch 7/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2751 - sparse_categorical_accuracy: 0.3547\n",
      "Epoch 7: val_loss improved from 1.28189 to 1.27702, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 368ms/step - loss: 1.2751 - sparse_categorical_accuracy: 0.3547 - val_loss: 1.2770 - val_sparse_categorical_accuracy: 0.3021 - lr: 1.0000e-05\n",
      "Epoch 8/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2748 - sparse_categorical_accuracy: 0.3495\n",
      "Epoch 8: val_loss improved from 1.27702 to 1.27284, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 368ms/step - loss: 1.2748 - sparse_categorical_accuracy: 0.3495 - val_loss: 1.2728 - val_sparse_categorical_accuracy: 0.3021 - lr: 1.0000e-05\n",
      "Epoch 9/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2620 - sparse_categorical_accuracy: 0.3743\n",
      "Epoch 9: val_loss improved from 1.27284 to 1.26736, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.2620 - sparse_categorical_accuracy: 0.3743 - val_loss: 1.2674 - val_sparse_categorical_accuracy: 0.3125 - lr: 1.0000e-05\n",
      "Epoch 10/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2597 - sparse_categorical_accuracy: 0.3704\n",
      "Epoch 10: val_loss improved from 1.26736 to 1.26551, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 368ms/step - loss: 1.2597 - sparse_categorical_accuracy: 0.3704 - val_loss: 1.2655 - val_sparse_categorical_accuracy: 0.2917 - lr: 1.0000e-05\n",
      "Epoch 11/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2666 - sparse_categorical_accuracy: 0.3691\n",
      "Epoch 11: val_loss improved from 1.26551 to 1.26170, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 368ms/step - loss: 1.2666 - sparse_categorical_accuracy: 0.3691 - val_loss: 1.2617 - val_sparse_categorical_accuracy: 0.3125 - lr: 1.0000e-05\n",
      "Epoch 12/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2638 - sparse_categorical_accuracy: 0.3626\n",
      "Epoch 12: val_loss improved from 1.26170 to 1.25938, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.2638 - sparse_categorical_accuracy: 0.3626 - val_loss: 1.2594 - val_sparse_categorical_accuracy: 0.3229 - lr: 1.0000e-05\n",
      "Epoch 13/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2606 - sparse_categorical_accuracy: 0.3848\n",
      "Epoch 13: val_loss improved from 1.25938 to 1.25798, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 368ms/step - loss: 1.2606 - sparse_categorical_accuracy: 0.3848 - val_loss: 1.2580 - val_sparse_categorical_accuracy: 0.3125 - lr: 1.0000e-05\n",
      "Epoch 14/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2546 - sparse_categorical_accuracy: 0.3665\n",
      "Epoch 14: val_loss improved from 1.25798 to 1.25739, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.2546 - sparse_categorical_accuracy: 0.3665 - val_loss: 1.2574 - val_sparse_categorical_accuracy: 0.3125 - lr: 1.0000e-05\n",
      "Epoch 15/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2411 - sparse_categorical_accuracy: 0.4123\n",
      "Epoch 15: val_loss improved from 1.25739 to 1.25561, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.2411 - sparse_categorical_accuracy: 0.4123 - val_loss: 1.2556 - val_sparse_categorical_accuracy: 0.3229 - lr: 1.0000e-05\n",
      "Epoch 16/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2501 - sparse_categorical_accuracy: 0.3809\n",
      "Epoch 16: val_loss improved from 1.25561 to 1.25496, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 370ms/step - loss: 1.2501 - sparse_categorical_accuracy: 0.3809 - val_loss: 1.2550 - val_sparse_categorical_accuracy: 0.3229 - lr: 1.0000e-05\n",
      "Epoch 17/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2451 - sparse_categorical_accuracy: 0.3927\n",
      "Epoch 17: val_loss improved from 1.25496 to 1.25493, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 368ms/step - loss: 1.2451 - sparse_categorical_accuracy: 0.3927 - val_loss: 1.2549 - val_sparse_categorical_accuracy: 0.3125 - lr: 1.0000e-05\n",
      "Epoch 18/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2469 - sparse_categorical_accuracy: 0.3770\n",
      "Epoch 18: val_loss improved from 1.25493 to 1.25027, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 367ms/step - loss: 1.2469 - sparse_categorical_accuracy: 0.3770 - val_loss: 1.2503 - val_sparse_categorical_accuracy: 0.2917 - lr: 1.0000e-05\n",
      "Epoch 19/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2406 - sparse_categorical_accuracy: 0.3914\n",
      "Epoch 19: val_loss did not improve from 1.25027\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.2406 - sparse_categorical_accuracy: 0.3914 - val_loss: 1.2516 - val_sparse_categorical_accuracy: 0.3125 - lr: 1.0000e-05\n",
      "Epoch 20/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2352 - sparse_categorical_accuracy: 0.4031\n",
      "Epoch 20: val_loss did not improve from 1.25027\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.2352 - sparse_categorical_accuracy: 0.4031 - val_loss: 1.2516 - val_sparse_categorical_accuracy: 0.3333 - lr: 1.0000e-05\n",
      "Epoch 21/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2403 - sparse_categorical_accuracy: 0.3953\n",
      "Epoch 21: val_loss improved from 1.25027 to 1.24769, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.2403 - sparse_categorical_accuracy: 0.3953 - val_loss: 1.2477 - val_sparse_categorical_accuracy: 0.3333 - lr: 1.0000e-05\n",
      "Epoch 22/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2317 - sparse_categorical_accuracy: 0.4241\n",
      "Epoch 22: val_loss improved from 1.24769 to 1.24709, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 372ms/step - loss: 1.2317 - sparse_categorical_accuracy: 0.4241 - val_loss: 1.2471 - val_sparse_categorical_accuracy: 0.3125 - lr: 1.0000e-05\n",
      "Epoch 23/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2180 - sparse_categorical_accuracy: 0.4411\n",
      "Epoch 23: val_loss improved from 1.24709 to 1.24472, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.2180 - sparse_categorical_accuracy: 0.4411 - val_loss: 1.2447 - val_sparse_categorical_accuracy: 0.3021 - lr: 1.0000e-05\n",
      "Epoch 24/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2327 - sparse_categorical_accuracy: 0.4123\n",
      "Epoch 24: val_loss improved from 1.24472 to 1.24345, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.2327 - sparse_categorical_accuracy: 0.4123 - val_loss: 1.2435 - val_sparse_categorical_accuracy: 0.3438 - lr: 1.0000e-05\n",
      "Epoch 25/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2278 - sparse_categorical_accuracy: 0.4084\n",
      "Epoch 25: val_loss improved from 1.24345 to 1.24251, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.2278 - sparse_categorical_accuracy: 0.4084 - val_loss: 1.2425 - val_sparse_categorical_accuracy: 0.3542 - lr: 1.0000e-05\n",
      "Epoch 26/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2216 - sparse_categorical_accuracy: 0.4202\n",
      "Epoch 26: val_loss improved from 1.24251 to 1.24246, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 371ms/step - loss: 1.2216 - sparse_categorical_accuracy: 0.4202 - val_loss: 1.2425 - val_sparse_categorical_accuracy: 0.3125 - lr: 1.0000e-05\n",
      "Epoch 27/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2141 - sparse_categorical_accuracy: 0.4280\n",
      "Epoch 27: val_loss improved from 1.24246 to 1.24048, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.2141 - sparse_categorical_accuracy: 0.4280 - val_loss: 1.2405 - val_sparse_categorical_accuracy: 0.3333 - lr: 1.0000e-05\n",
      "Epoch 28/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2109 - sparse_categorical_accuracy: 0.4293\n",
      "Epoch 28: val_loss improved from 1.24048 to 1.23928, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.2109 - sparse_categorical_accuracy: 0.4293 - val_loss: 1.2393 - val_sparse_categorical_accuracy: 0.3333 - lr: 1.0000e-05\n",
      "Epoch 29/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2129 - sparse_categorical_accuracy: 0.4215\n",
      "Epoch 29: val_loss did not improve from 1.23928\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.2129 - sparse_categorical_accuracy: 0.4215 - val_loss: 1.2400 - val_sparse_categorical_accuracy: 0.3125 - lr: 1.0000e-05\n",
      "Epoch 30/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2162 - sparse_categorical_accuracy: 0.4162\n",
      "Epoch 30: val_loss improved from 1.23928 to 1.23859, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 368ms/step - loss: 1.2162 - sparse_categorical_accuracy: 0.4162 - val_loss: 1.2386 - val_sparse_categorical_accuracy: 0.3542 - lr: 1.0000e-05\n",
      "Epoch 31/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2131 - sparse_categorical_accuracy: 0.4332\n",
      "Epoch 31: val_loss did not improve from 1.23859\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.2131 - sparse_categorical_accuracy: 0.4332 - val_loss: 1.2386 - val_sparse_categorical_accuracy: 0.3333 - lr: 1.0000e-05\n",
      "Epoch 32/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2148 - sparse_categorical_accuracy: 0.4450\n",
      "Epoch 32: val_loss improved from 1.23859 to 1.23784, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.2148 - sparse_categorical_accuracy: 0.4450 - val_loss: 1.2378 - val_sparse_categorical_accuracy: 0.3646 - lr: 1.0000e-05\n",
      "Epoch 33/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2067 - sparse_categorical_accuracy: 0.4162\n",
      "Epoch 33: val_loss improved from 1.23784 to 1.23605, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 370ms/step - loss: 1.2067 - sparse_categorical_accuracy: 0.4162 - val_loss: 1.2361 - val_sparse_categorical_accuracy: 0.3438 - lr: 1.0000e-05\n",
      "Epoch 34/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1988 - sparse_categorical_accuracy: 0.4686\n",
      "Epoch 34: val_loss did not improve from 1.23605\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1988 - sparse_categorical_accuracy: 0.4686 - val_loss: 1.2373 - val_sparse_categorical_accuracy: 0.3646 - lr: 1.0000e-05\n",
      "Epoch 35/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1980 - sparse_categorical_accuracy: 0.4647\n",
      "Epoch 35: val_loss improved from 1.23605 to 1.23601, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 370ms/step - loss: 1.1980 - sparse_categorical_accuracy: 0.4647 - val_loss: 1.2360 - val_sparse_categorical_accuracy: 0.4062 - lr: 1.0000e-05\n",
      "Epoch 36/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1989 - sparse_categorical_accuracy: 0.4503\n",
      "Epoch 36: val_loss improved from 1.23601 to 1.23556, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 370ms/step - loss: 1.1989 - sparse_categorical_accuracy: 0.4503 - val_loss: 1.2356 - val_sparse_categorical_accuracy: 0.3958 - lr: 1.0000e-05\n",
      "Epoch 37/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1998 - sparse_categorical_accuracy: 0.4529\n",
      "Epoch 37: val_loss improved from 1.23556 to 1.23443, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.1998 - sparse_categorical_accuracy: 0.4529 - val_loss: 1.2344 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-05\n",
      "Epoch 38/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1939 - sparse_categorical_accuracy: 0.4450\n",
      "Epoch 38: val_loss did not improve from 1.23443\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1939 - sparse_categorical_accuracy: 0.4450 - val_loss: 1.2359 - val_sparse_categorical_accuracy: 0.3854 - lr: 1.0000e-05\n",
      "Epoch 39/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1993 - sparse_categorical_accuracy: 0.4463\n",
      "Epoch 39: val_loss improved from 1.23443 to 1.23411, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.1993 - sparse_categorical_accuracy: 0.4463 - val_loss: 1.2341 - val_sparse_categorical_accuracy: 0.3854 - lr: 1.0000e-05\n",
      "Epoch 40/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2008 - sparse_categorical_accuracy: 0.4490\n",
      "Epoch 40: val_loss improved from 1.23411 to 1.23232, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 371ms/step - loss: 1.2008 - sparse_categorical_accuracy: 0.4490 - val_loss: 1.2323 - val_sparse_categorical_accuracy: 0.4062 - lr: 1.0000e-05\n",
      "Epoch 41/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1824 - sparse_categorical_accuracy: 0.4843\n",
      "Epoch 41: val_loss did not improve from 1.23232\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1824 - sparse_categorical_accuracy: 0.4843 - val_loss: 1.2327 - val_sparse_categorical_accuracy: 0.4062 - lr: 1.0000e-05\n",
      "Epoch 42/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1887 - sparse_categorical_accuracy: 0.4634\n",
      "Epoch 42: val_loss improved from 1.23232 to 1.22895, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 368ms/step - loss: 1.1887 - sparse_categorical_accuracy: 0.4634 - val_loss: 1.2290 - val_sparse_categorical_accuracy: 0.4062 - lr: 1.0000e-05\n",
      "Epoch 43/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1901 - sparse_categorical_accuracy: 0.4555\n",
      "Epoch 43: val_loss did not improve from 1.22895\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1901 - sparse_categorical_accuracy: 0.4555 - val_loss: 1.2292 - val_sparse_categorical_accuracy: 0.4271 - lr: 1.0000e-05\n",
      "Epoch 44/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1876 - sparse_categorical_accuracy: 0.4686\n",
      "Epoch 44: val_loss improved from 1.22895 to 1.22850, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 370ms/step - loss: 1.1876 - sparse_categorical_accuracy: 0.4686 - val_loss: 1.2285 - val_sparse_categorical_accuracy: 0.4062 - lr: 1.0000e-05\n",
      "Epoch 45/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1802 - sparse_categorical_accuracy: 0.4791\n",
      "Epoch 45: val_loss improved from 1.22850 to 1.22758, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.1802 - sparse_categorical_accuracy: 0.4791 - val_loss: 1.2276 - val_sparse_categorical_accuracy: 0.3958 - lr: 1.0000e-05\n",
      "Epoch 46/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1765 - sparse_categorical_accuracy: 0.4764\n",
      "Epoch 46: val_loss improved from 1.22758 to 1.22494, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 370ms/step - loss: 1.1765 - sparse_categorical_accuracy: 0.4764 - val_loss: 1.2249 - val_sparse_categorical_accuracy: 0.3854 - lr: 1.0000e-05\n",
      "Epoch 47/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1796 - sparse_categorical_accuracy: 0.4921\n",
      "Epoch 47: val_loss did not improve from 1.22494\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1796 - sparse_categorical_accuracy: 0.4921 - val_loss: 1.2253 - val_sparse_categorical_accuracy: 0.3854 - lr: 1.0000e-05\n",
      "Epoch 48/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1779 - sparse_categorical_accuracy: 0.4686\n",
      "Epoch 48: val_loss did not improve from 1.22494\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1779 - sparse_categorical_accuracy: 0.4686 - val_loss: 1.2264 - val_sparse_categorical_accuracy: 0.4062 - lr: 1.0000e-05\n",
      "Epoch 49/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1763 - sparse_categorical_accuracy: 0.4830\n",
      "Epoch 49: val_loss did not improve from 1.22494\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1763 - sparse_categorical_accuracy: 0.4830 - val_loss: 1.2251 - val_sparse_categorical_accuracy: 0.3958 - lr: 1.0000e-05\n",
      "Epoch 50/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1848 - sparse_categorical_accuracy: 0.4555\n",
      "Epoch 50: val_loss improved from 1.22494 to 1.22411, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 370ms/step - loss: 1.1848 - sparse_categorical_accuracy: 0.4555 - val_loss: 1.2241 - val_sparse_categorical_accuracy: 0.4062 - lr: 1.0000e-05\n",
      "Epoch 51/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1763 - sparse_categorical_accuracy: 0.4751\n",
      "Epoch 51: val_loss did not improve from 1.22411\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1763 - sparse_categorical_accuracy: 0.4751 - val_loss: 1.2245 - val_sparse_categorical_accuracy: 0.3958 - lr: 1.0000e-05\n",
      "Epoch 52/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1703 - sparse_categorical_accuracy: 0.4791\n",
      "Epoch 52: val_loss did not improve from 1.22411\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1703 - sparse_categorical_accuracy: 0.4791 - val_loss: 1.2268 - val_sparse_categorical_accuracy: 0.4167 - lr: 1.0000e-05\n",
      "Epoch 53/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1730 - sparse_categorical_accuracy: 0.4791\n",
      "Epoch 53: val_loss did not improve from 1.22411\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1730 - sparse_categorical_accuracy: 0.4791 - val_loss: 1.2244 - val_sparse_categorical_accuracy: 0.3958 - lr: 1.0000e-05\n",
      "Epoch 54/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1637 - sparse_categorical_accuracy: 0.4921\n",
      "Epoch 54: val_loss did not improve from 1.22411\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1637 - sparse_categorical_accuracy: 0.4921 - val_loss: 1.2242 - val_sparse_categorical_accuracy: 0.3854 - lr: 1.0000e-05\n",
      "Epoch 55/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1667 - sparse_categorical_accuracy: 0.4804\n",
      "Epoch 55: val_loss improved from 1.22411 to 1.22344, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.1667 - sparse_categorical_accuracy: 0.4804 - val_loss: 1.2234 - val_sparse_categorical_accuracy: 0.3854 - lr: 1.0000e-05\n",
      "Epoch 56/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1676 - sparse_categorical_accuracy: 0.4634\n",
      "Epoch 56: val_loss improved from 1.22344 to 1.22215, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 1.1676 - sparse_categorical_accuracy: 0.4634 - val_loss: 1.2222 - val_sparse_categorical_accuracy: 0.3958 - lr: 1.0000e-05\n",
      "Epoch 57/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1599 - sparse_categorical_accuracy: 0.4948\n",
      "Epoch 57: val_loss improved from 1.22215 to 1.22138, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 371ms/step - loss: 1.1599 - sparse_categorical_accuracy: 0.4948 - val_loss: 1.2214 - val_sparse_categorical_accuracy: 0.3958 - lr: 1.0000e-05\n",
      "Epoch 58/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1570 - sparse_categorical_accuracy: 0.4751\n",
      "Epoch 58: val_loss did not improve from 1.22138\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1570 - sparse_categorical_accuracy: 0.4751 - val_loss: 1.2227 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-05\n",
      "Epoch 59/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1529 - sparse_categorical_accuracy: 0.4804\n",
      "Epoch 59: val_loss did not improve from 1.22138\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1529 - sparse_categorical_accuracy: 0.4804 - val_loss: 1.2225 - val_sparse_categorical_accuracy: 0.3854 - lr: 1.0000e-05\n",
      "Epoch 60/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1468 - sparse_categorical_accuracy: 0.5144\n",
      "Epoch 60: val_loss improved from 1.22138 to 1.21919, saving model to ./model_checkpoint\\\n",
      "48/48 [==============================] - 18s 370ms/step - loss: 1.1468 - sparse_categorical_accuracy: 0.5144 - val_loss: 1.2192 - val_sparse_categorical_accuracy: 0.3958 - lr: 1.0000e-05\n",
      "Epoch 61/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1577 - sparse_categorical_accuracy: 0.4856\n",
      "Epoch 61: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 356ms/step - loss: 1.1577 - sparse_categorical_accuracy: 0.4856 - val_loss: 1.2204 - val_sparse_categorical_accuracy: 0.3958 - lr: 1.0000e-05\n",
      "Epoch 62/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1642 - sparse_categorical_accuracy: 0.4869\n",
      "Epoch 62: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1642 - sparse_categorical_accuracy: 0.4869 - val_loss: 1.2203 - val_sparse_categorical_accuracy: 0.3958 - lr: 1.0000e-05\n",
      "Epoch 63/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1450 - sparse_categorical_accuracy: 0.5262\n",
      "Epoch 63: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1450 - sparse_categorical_accuracy: 0.5262 - val_loss: 1.2223 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-05\n",
      "Epoch 64/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1516 - sparse_categorical_accuracy: 0.4961\n",
      "Epoch 64: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1516 - sparse_categorical_accuracy: 0.4961 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3958 - lr: 1.0000e-05\n",
      "Epoch 65/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1484 - sparse_categorical_accuracy: 0.4843\n",
      "Epoch 65: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1484 - sparse_categorical_accuracy: 0.4843 - val_loss: 1.2210 - val_sparse_categorical_accuracy: 0.3854 - lr: 1.0000e-05\n",
      "Epoch 66/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1494 - sparse_categorical_accuracy: 0.5026\n",
      "Epoch 66: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1494 - sparse_categorical_accuracy: 0.5026 - val_loss: 1.2209 - val_sparse_categorical_accuracy: 0.3646 - lr: 1.0000e-06\n",
      "Epoch 67/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1328 - sparse_categorical_accuracy: 0.5196\n",
      "Epoch 67: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1328 - sparse_categorical_accuracy: 0.5196 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-06\n",
      "Epoch 68/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1358 - sparse_categorical_accuracy: 0.5196\n",
      "Epoch 68: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1358 - sparse_categorical_accuracy: 0.5196 - val_loss: 1.2208 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-06\n",
      "Epoch 69/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1351 - sparse_categorical_accuracy: 0.4777\n",
      "Epoch 69: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1351 - sparse_categorical_accuracy: 0.4777 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-06\n",
      "Epoch 70/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1313 - sparse_categorical_accuracy: 0.5236\n",
      "Epoch 70: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1313 - sparse_categorical_accuracy: 0.5236 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-06\n",
      "Epoch 71/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1461 - sparse_categorical_accuracy: 0.4869\n",
      "Epoch 71: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1461 - sparse_categorical_accuracy: 0.4869 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-07\n",
      "Epoch 72/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1409 - sparse_categorical_accuracy: 0.5118\n",
      "Epoch 72: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1409 - sparse_categorical_accuracy: 0.5118 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-07\n",
      "Epoch 73/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1479 - sparse_categorical_accuracy: 0.5065\n",
      "Epoch 73: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1479 - sparse_categorical_accuracy: 0.5065 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-07\n",
      "Epoch 74/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1348 - sparse_categorical_accuracy: 0.5039\n",
      "Epoch 74: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1348 - sparse_categorical_accuracy: 0.5039 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-07\n",
      "Epoch 75/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1358 - sparse_categorical_accuracy: 0.5275\n",
      "Epoch 75: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 18s 372ms/step - loss: 1.1358 - sparse_categorical_accuracy: 0.5275 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-07\n",
      "Epoch 76/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1458 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 76: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 18s 374ms/step - loss: 1.1458 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 77/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1370 - sparse_categorical_accuracy: 0.4895\n",
      "Epoch 77: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1370 - sparse_categorical_accuracy: 0.4895 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 78/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1305 - sparse_categorical_accuracy: 0.5275\n",
      "Epoch 78: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1305 - sparse_categorical_accuracy: 0.5275 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 79/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1302 - sparse_categorical_accuracy: 0.5013\n",
      "Epoch 79: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1302 - sparse_categorical_accuracy: 0.5013 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 80/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1330 - sparse_categorical_accuracy: 0.5262\n",
      "Epoch 80: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1330 - sparse_categorical_accuracy: 0.5262 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 81/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1444 - sparse_categorical_accuracy: 0.4764\n",
      "Epoch 81: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1444 - sparse_categorical_accuracy: 0.4764 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 82/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1426 - sparse_categorical_accuracy: 0.5052\n",
      "Epoch 82: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1426 - sparse_categorical_accuracy: 0.5052 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 83/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1494 - sparse_categorical_accuracy: 0.4974\n",
      "Epoch 83: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1494 - sparse_categorical_accuracy: 0.4974 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 84/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1388 - sparse_categorical_accuracy: 0.5065\n",
      "Epoch 84: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 356ms/step - loss: 1.1388 - sparse_categorical_accuracy: 0.5065 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 85/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1421 - sparse_categorical_accuracy: 0.4974\n",
      "Epoch 85: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1421 - sparse_categorical_accuracy: 0.4974 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 86/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1384 - sparse_categorical_accuracy: 0.5144\n",
      "Epoch 86: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1384 - sparse_categorical_accuracy: 0.5144 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 87/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1368 - sparse_categorical_accuracy: 0.5052\n",
      "Epoch 87: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1368 - sparse_categorical_accuracy: 0.5052 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 88/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1371 - sparse_categorical_accuracy: 0.5039\n",
      "Epoch 88: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1371 - sparse_categorical_accuracy: 0.5039 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 89/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1435 - sparse_categorical_accuracy: 0.5196\n",
      "Epoch 89: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1435 - sparse_categorical_accuracy: 0.5196 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 90/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1447 - sparse_categorical_accuracy: 0.5079\n",
      "Epoch 90: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 356ms/step - loss: 1.1447 - sparse_categorical_accuracy: 0.5079 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 91/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1414 - sparse_categorical_accuracy: 0.5183\n",
      "Epoch 91: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 356ms/step - loss: 1.1414 - sparse_categorical_accuracy: 0.5183 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 92/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1403 - sparse_categorical_accuracy: 0.5079\n",
      "Epoch 92: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1403 - sparse_categorical_accuracy: 0.5079 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 93/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1401 - sparse_categorical_accuracy: 0.4921\n",
      "Epoch 93: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 356ms/step - loss: 1.1401 - sparse_categorical_accuracy: 0.4921 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 94/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1378 - sparse_categorical_accuracy: 0.5183\n",
      "Epoch 94: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 356ms/step - loss: 1.1378 - sparse_categorical_accuracy: 0.5183 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 95/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1452 - sparse_categorical_accuracy: 0.4921\n",
      "Epoch 95: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1452 - sparse_categorical_accuracy: 0.4921 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 96/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1381 - sparse_categorical_accuracy: 0.5170\n",
      "Epoch 96: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1381 - sparse_categorical_accuracy: 0.5170 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 97/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1399 - sparse_categorical_accuracy: 0.4948\n",
      "Epoch 97: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1399 - sparse_categorical_accuracy: 0.4948 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 98/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1443 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 98: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1443 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 99/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1465 - sparse_categorical_accuracy: 0.4777\n",
      "Epoch 99: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1465 - sparse_categorical_accuracy: 0.4777 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 100/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1458 - sparse_categorical_accuracy: 0.5026\n",
      "Epoch 100: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1458 - sparse_categorical_accuracy: 0.5026 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 101/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1489 - sparse_categorical_accuracy: 0.5013\n",
      "Epoch 101: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1489 - sparse_categorical_accuracy: 0.5013 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 102/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1377 - sparse_categorical_accuracy: 0.4882\n",
      "Epoch 102: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 359ms/step - loss: 1.1377 - sparse_categorical_accuracy: 0.4882 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 103/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1470 - sparse_categorical_accuracy: 0.5065\n",
      "Epoch 103: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1470 - sparse_categorical_accuracy: 0.5065 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 104/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1347 - sparse_categorical_accuracy: 0.5301\n",
      "Epoch 104: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1347 - sparse_categorical_accuracy: 0.5301 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 105/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1379 - sparse_categorical_accuracy: 0.5105\n",
      "Epoch 105: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1379 - sparse_categorical_accuracy: 0.5105 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 106/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1371 - sparse_categorical_accuracy: 0.5209\n",
      "Epoch 106: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1371 - sparse_categorical_accuracy: 0.5209 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 107/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1379 - sparse_categorical_accuracy: 0.5288\n",
      "Epoch 107: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1379 - sparse_categorical_accuracy: 0.5288 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 108/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1445 - sparse_categorical_accuracy: 0.4935\n",
      "Epoch 108: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1445 - sparse_categorical_accuracy: 0.4935 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 109/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1374 - sparse_categorical_accuracy: 0.5079\n",
      "Epoch 109: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1374 - sparse_categorical_accuracy: 0.5079 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 110/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1421 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 110: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1421 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.2206 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 111/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1496 - sparse_categorical_accuracy: 0.4830\n",
      "Epoch 111: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1496 - sparse_categorical_accuracy: 0.4830 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 112/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1377 - sparse_categorical_accuracy: 0.4961\n",
      "Epoch 112: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1377 - sparse_categorical_accuracy: 0.4961 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 113/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1414 - sparse_categorical_accuracy: 0.5131\n",
      "Epoch 113: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1414 - sparse_categorical_accuracy: 0.5131 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 114/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1353 - sparse_categorical_accuracy: 0.4974\n",
      "Epoch 114: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1353 - sparse_categorical_accuracy: 0.4974 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 115/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1357 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 115: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1357 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 116/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1444 - sparse_categorical_accuracy: 0.5039\n",
      "Epoch 116: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1444 - sparse_categorical_accuracy: 0.5039 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 117/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1270 - sparse_categorical_accuracy: 0.5039\n",
      "Epoch 117: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1270 - sparse_categorical_accuracy: 0.5039 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 118/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1497 - sparse_categorical_accuracy: 0.4856\n",
      "Epoch 118: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 356ms/step - loss: 1.1497 - sparse_categorical_accuracy: 0.4856 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 119/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1409 - sparse_categorical_accuracy: 0.5065\n",
      "Epoch 119: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1409 - sparse_categorical_accuracy: 0.5065 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 120/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1420 - sparse_categorical_accuracy: 0.4987\n",
      "Epoch 120: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1420 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 121/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1499 - sparse_categorical_accuracy: 0.4895\n",
      "Epoch 121: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1499 - sparse_categorical_accuracy: 0.4895 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 122/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1269 - sparse_categorical_accuracy: 0.5079\n",
      "Epoch 122: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1269 - sparse_categorical_accuracy: 0.5079 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 123/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1460 - sparse_categorical_accuracy: 0.5144\n",
      "Epoch 123: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1460 - sparse_categorical_accuracy: 0.5144 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 124/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1409 - sparse_categorical_accuracy: 0.4869\n",
      "Epoch 124: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1409 - sparse_categorical_accuracy: 0.4869 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 125/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1362 - sparse_categorical_accuracy: 0.5196\n",
      "Epoch 125: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1362 - sparse_categorical_accuracy: 0.5196 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 126/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1449 - sparse_categorical_accuracy: 0.4895\n",
      "Epoch 126: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 356ms/step - loss: 1.1449 - sparse_categorical_accuracy: 0.4895 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 127/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1521 - sparse_categorical_accuracy: 0.5118\n",
      "Epoch 127: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1521 - sparse_categorical_accuracy: 0.5118 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 128/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1384 - sparse_categorical_accuracy: 0.5065\n",
      "Epoch 128: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 356ms/step - loss: 1.1384 - sparse_categorical_accuracy: 0.5065 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 129/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1375 - sparse_categorical_accuracy: 0.5301\n",
      "Epoch 129: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1375 - sparse_categorical_accuracy: 0.5301 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 130/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1393 - sparse_categorical_accuracy: 0.4895\n",
      "Epoch 130: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1393 - sparse_categorical_accuracy: 0.4895 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 131/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1419 - sparse_categorical_accuracy: 0.5052\n",
      "Epoch 131: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1419 - sparse_categorical_accuracy: 0.5052 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 132/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1310 - sparse_categorical_accuracy: 0.5118\n",
      "Epoch 132: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1310 - sparse_categorical_accuracy: 0.5118 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 133/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1416 - sparse_categorical_accuracy: 0.5118\n",
      "Epoch 133: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1416 - sparse_categorical_accuracy: 0.5118 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 134/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1431 - sparse_categorical_accuracy: 0.5065\n",
      "Epoch 134: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1431 - sparse_categorical_accuracy: 0.5065 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 135/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1435 - sparse_categorical_accuracy: 0.4921\n",
      "Epoch 135: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1435 - sparse_categorical_accuracy: 0.4921 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 136/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1395 - sparse_categorical_accuracy: 0.4895\n",
      "Epoch 136: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1395 - sparse_categorical_accuracy: 0.4895 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 137/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1497 - sparse_categorical_accuracy: 0.5026\n",
      "Epoch 137: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1497 - sparse_categorical_accuracy: 0.5026 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 138/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1378 - sparse_categorical_accuracy: 0.5105\n",
      "Epoch 138: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1378 - sparse_categorical_accuracy: 0.5105 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 139/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1341 - sparse_categorical_accuracy: 0.5157\n",
      "Epoch 139: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1341 - sparse_categorical_accuracy: 0.5157 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 140/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1450 - sparse_categorical_accuracy: 0.5353\n",
      "Epoch 140: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1450 - sparse_categorical_accuracy: 0.5353 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 141/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1379 - sparse_categorical_accuracy: 0.4935\n",
      "Epoch 141: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1379 - sparse_categorical_accuracy: 0.4935 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 142/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1406 - sparse_categorical_accuracy: 0.5092\n",
      "Epoch 142: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1406 - sparse_categorical_accuracy: 0.5092 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 143/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1410 - sparse_categorical_accuracy: 0.4921\n",
      "Epoch 143: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1410 - sparse_categorical_accuracy: 0.4921 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 144/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1442 - sparse_categorical_accuracy: 0.4908\n",
      "Epoch 144: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1442 - sparse_categorical_accuracy: 0.4908 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 145/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1332 - sparse_categorical_accuracy: 0.5380\n",
      "Epoch 145: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1332 - sparse_categorical_accuracy: 0.5380 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 146/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1432 - sparse_categorical_accuracy: 0.4974\n",
      "Epoch 146: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 356ms/step - loss: 1.1432 - sparse_categorical_accuracy: 0.4974 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 147/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1357 - sparse_categorical_accuracy: 0.5079\n",
      "Epoch 147: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1357 - sparse_categorical_accuracy: 0.5079 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 148/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1397 - sparse_categorical_accuracy: 0.5092\n",
      "Epoch 148: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 358ms/step - loss: 1.1397 - sparse_categorical_accuracy: 0.5092 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 149/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1393 - sparse_categorical_accuracy: 0.5092\n",
      "Epoch 149: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1393 - sparse_categorical_accuracy: 0.5092 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n",
      "Epoch 150/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1375 - sparse_categorical_accuracy: 0.5144\n",
      "Epoch 150: val_loss did not improve from 1.21919\n",
      "48/48 [==============================] - 17s 357ms/step - loss: 1.1375 - sparse_categorical_accuracy: 0.5144 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-08\n"
     ]
    }
   ],
   "source": [
    "history = None\n",
    "\n",
    "try:\n",
    "    model.load_weights(stance_config.MODEL_CHECKPOINT_FOLDER)\n",
    "    print('The weights of the model have already been found. Skipping training.')\n",
    "\n",
    "except:\n",
    "    \n",
    "    history = model.fit(\n",
    "        ds_splits['x_train'], \n",
    "        ds_splits['y_train'], \n",
    "        validation_data = (\n",
    "            ds_splits['x_val'],\n",
    "            ds_splits['y_val']\n",
    "        ),\n",
    "        epochs = stance_config.MODEL_EPOCHS,\n",
    "        callbacks = model_callbacks,\n",
    "        batch_size = stance_config.MODEL_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    #Finally, load the best weights obtained during the training:\n",
    "    model.load_weights(stance_config.MODEL_CHECKPOINT_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7564776a-7665-43fc-bb99-dd5619e79974",
   "metadata": {},
   "source": [
    "Let us inspect the history of the training of both networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8b22d55-ffb3-4d76-9c72-848b951b66a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_history(history: keras.callbacks.History) -> None:\n",
    "    \"\"\"Plots the history of the training of a certain model.\n",
    "\n",
    "    Args:\n",
    "        history (keras.callbacks.History): History of the training of a model.\n",
    "\n",
    "    \"\"\"\n",
    "    if history is not None:\n",
    "        train_loss = history.history['loss']\n",
    "\n",
    "        val_loss = history.history['val_loss']\n",
    "\n",
    "        # Visualize the behavior of the loss\n",
    "        plt.plot(train_loss)\n",
    "        plt.plot(val_loss)\n",
    "        plt.grid()\n",
    "        plt.title('Loss during training')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Training', 'Validation'])\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No history available. The weights have been loaded manually.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a40c6c1e-f7d7-4de0-aadb-720155fa746f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_history(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffd9aa45-10bd-4805-a304-ddfdf38b01ff",
   "metadata": {},
   "source": [
    "From the data we can infer the epoch that allowed the model to generalize better by inspecting the loss on data that it had never seen before (data on the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7c5c79c-38fb-45d4-8cf0-ea9b64a9b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model currently uses the weights that have been determined on epoch 60.\n"
     ]
    }
   ],
   "source": [
    "if history is None:\n",
    "    print(f'The data cannot be calculated since the history of the training of the model is not available.')\n",
    "else:\n",
    "    print(f'The model currently uses the weights that have been determined on epoch {np.argmin(history.history[\"val_loss\"]) + 1}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0048ce96-179e-4b08-b114-bc853a1fa69f",
   "metadata": {},
   "source": [
    "The following classification report sums up the performances of the trained networks with respect to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1f56ce1-48c2-4334-b460-674b498d8501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.47      0.76      0.58        25\n",
      "           2       0.45      0.30      0.36        43\n",
      "           3       0.33      0.39      0.36        23\n",
      "\n",
      "    accuracy                           0.43        96\n",
      "   macro avg       0.31      0.36      0.33        96\n",
      "weighted avg       0.40      0.43      0.40        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Classification report for the Model:')    \n",
    "y_true = ds_splits['y_test']\n",
    "y_pred = np.argmax(model.predict(ds_splits['x_test']).logits,axis=-1)\n",
    "print(classification_report(y_true, y_pred, zero_division = 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e98e8128-95bc-4442-b241-bae104ab0b66",
   "metadata": {},
   "source": [
    "Considering we had scarcely any records that could be used to train our data, we don't expect good performances. We just aim to be, at least, better than a random classifier. Furthermore, the distribution of the classes is heavily unbalanced, meaning that it does not resemble an uniform distribution at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54a8509d-e224-48aa-b452-81ad5d564bac",
   "metadata": {},
   "source": [
    "The following section sets up a random classifier, in order to be able to compare the performances of the models with the performances of a random classifier (classes assigned by sampling from an uniform distribution). We want to inspect the macro averages and the weighted averages of precision, recall and f1-scores. Given the randomness of the experiment, we collect the metrics many times and then calculate mean and variance of those measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3276ff0d-dd33-427c-9ea9-bd5d88a9611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_experiment(model_list: list, y_true_list: list, range_list: list, NUM_EXPERIMENTS: int, NUM_DECIMALS: int = 3) -> None:\n",
    "    \"\"\"Evaluates how the models in model_list would perform given the true labels and the range of the labels (thought as numbers).\n",
    "    NUM_EXPERIMENTS runs will be performed and the results will be displayed on screen with a number of decimals equal to NUM_DECIMALS.\n",
    "    Args:\n",
    "        model_list (list): The list of models to test.\n",
    "        y_true_list (list): The list of lists of true labels. The first element of y_true_list must contain the list of true labels for the first model, and so on and so forth.\n",
    "        range_list (list): The list of dictionaries with keys \"min\" and \"max\" describing the minimum and maximum label that each model in model_list can output.\n",
    "        NUM_EXPERIMENTS (int): The number of experiments that will be run.\n",
    "        NUM_DECIMAL (int): The number of decimals that have to be printed to describe the computed statistics.\n",
    "    \"\"\"\n",
    "    assert len(model_list) == len(y_true_list) and NUM_EXPERIMENTS > 0, 'Invalid arguments'\n",
    "    \n",
    "    #The dictionary that will contain the output\n",
    "    measures_dict = {model: {'macro': [], 'weighted':[]} for model in model_list}\n",
    "    \n",
    "    for _ in tqdm.tqdm(range(NUM_EXPERIMENTS)):\n",
    "        for i in range(len(model_list)):\n",
    "            model = model_list[i]\n",
    "            y_true = y_true_list[i]\n",
    "            y_true_min = range_list[i]['min']\n",
    "            y_true_max = range_list[i]['max']\n",
    "            \n",
    "            y_pred = np.random.randint(y_true_min, y_true_max + 1, y_true.shape)\n",
    "            cr = classification_report(y_true, y_pred, zero_division = 0, output_dict = True)\n",
    "            measures_dict[model]['macro'].append(cr['macro avg'])\n",
    "            measures_dict[model]['weighted'].append(cr['weighted avg'])\n",
    "    \n",
    "    precisions = {model: {'macro': [], 'weighted':[]} for model in model_list}\n",
    "    recall = {model: {'macro': [], 'weighted':[]} for model in model_list}\n",
    "    f1_score = {model: {'macro': [], 'weighted':[]} for model in model_list}\n",
    "    \n",
    "    for model in model_list:\n",
    "        for avg_type in measures_dict[model]:\n",
    "            for scores in measures_dict[model][avg_type]:\n",
    "                precisions[model][avg_type].append(scores['precision'])\n",
    "                recall[model][avg_type].append(scores['recall'])\n",
    "                f1_score[model][avg_type].append(scores['f1-score'])\n",
    "\n",
    "    #Computing averages in place\n",
    "    for model in model_list:\n",
    "        for avg_type in measures_dict[model]:\n",
    "            precisions[model][avg_type] = {'mean': np.mean(precisions[model][avg_type]), 'var': np.var(precisions[model][avg_type])}\n",
    "            recall[model][avg_type] = {'mean': np.mean(recall[model][avg_type]), 'var': np.var(recall[model][avg_type])}\n",
    "            f1_score[model][avg_type] = {'mean': np.mean(f1_score[model][avg_type]), 'var': np.var(f1_score[model][avg_type])}\n",
    "            \n",
    "    tab = '\\t'\n",
    "    for i in range(len(model_list)):\n",
    "        print(f'Approximation of the expected performances of a uniform random classifier for the task solved by Model {i+1}')\n",
    "        for avg_type in ['macro', 'weighted']:\n",
    "            print(tab + avg_type[0].upper() + avg_type[1:] + ':')\n",
    "            for metric_name, metric in [('Precision:', precisions), ('Recall', recall), ('F1-score', f1_score)]:\n",
    "                print(2*tab + metric_name)\n",
    "                for stat in ['mean','var']:\n",
    "                    print(3*tab + stat[0].upper() + stat[1:] + ':' + (\"{:.\" + str(NUM_DECIMALS) + \"f}\").format(metric[model_list[i]][avg_type][stat]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e63ce1b3-c11a-41d0-9083-c5833ad70320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:02<00:00, 432.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation of the expected performances of a uniform random classifier for the task solved by Model 1\n",
      "\tMacro:\n",
      "\t\tPrecision:\n",
      "\t\t\tMean:0.250\n",
      "\t\t\tVar:0.002\n",
      "\t\tRecall\n",
      "\t\t\tMean:0.251\n",
      "\t\t\tVar:0.004\n",
      "\t\tF1-score\n",
      "\t\t\tMean:0.225\n",
      "\t\t\tVar:0.002\n",
      "\tWeighted:\n",
      "\t\tPrecision:\n",
      "\t\t\tMean:0.329\n",
      "\t\t\tVar:0.003\n",
      "\t\tRecall\n",
      "\t\t\tMean:0.250\n",
      "\t\t\tVar:0.002\n",
      "\t\tF1-score\n",
      "\t\t\tMean:0.271\n",
      "\t\t\tVar:0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random_experiment([model], [ds_splits['y_test']], [{'min':0, 'max':3}], 1000, 3) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2547c297",
   "metadata": {},
   "source": [
    "### Two models classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0005ee17-8566-43e2-8611-628c5dc10769",
   "metadata": {
    "tags": []
   },
   "source": [
    "The literature, for example through the paper \"[LeviRANK: Limited Query Expansion with Voting Integration for Document Retrieval and Ranking](https://ceur-ws.org/Vol-3180/paper-259.pdf)\", provides some results according to which the performances (in terms of quality of the predictions) can be improved by splitting the task above in the two sub-tasks consisting of initially determining whether the stance is \"No stance\", \"Neutrality\" or \"Being in favour of $O_x$\" ($x \\in \\{1,2\\}$) and then, if the last evaluation yielded \"Being in favour of $O_x$\", determining whether $x$ is $1$ or $2$. We will try this approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75439883-e131-4680-84de-5c5c64944f44",
   "metadata": {
    "id": "75439883-e131-4680-84de-5c5c64944f44"
   },
   "source": [
    "We'll start by splitting the dataset in two different ones:\n",
    "- `df_nno`: used to train a network focused on determining whether the stance is \"No stance\", \"Neutrality\" or \"Being in favour of $O_x$\" ($x \\in \\{1,2\\}$);\n",
    "- `df_obj`: used to train a network focused on determining whether the stance is \"Being in favour of $O_1$\" or \"Being in favour of $O_2$\".\n",
    "\n",
    "In particular, the first dataset will remove the distinction between `answer_stance` being equal to $2$ or $3$, the second dataset will consider ONLY the pairs (`question`, `answer`) whose `answer_stance` is either $2$ or $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "521523d9-4bd7-406c-85a6-fd2dec4084f5",
   "metadata": {
    "id": "521523d9-4bd7-406c-85a6-fd2dec4084f5"
   },
   "outputs": [],
   "source": [
    "df_nno = df.copy()\n",
    "df_obj = df.copy()\n",
    "\n",
    "#The answer stances \"Being in favour of O_1/2\" must become indistinguishable from each other.\n",
    "df_nno.loc[df_nno.answer_stance == 3, 'answer_stance'] = 2\n",
    "\n",
    "#The only answer stances that we're considering are \"Being in favour of O_1/2\".\n",
    "df_obj = df_obj.loc[df_obj.answer_stance.isin([2,3]),:]\n",
    "\n",
    "#Remapping the answer stances \"2\" and \"3\" to, respectively, \"0\" and \"1\"\n",
    "df_obj.answer_stance -= 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf33b5aa-f4f5-4cef-8613-cc98c4d36d04",
   "metadata": {
    "id": "cf33b5aa-f4f5-4cef-8613-cc98c4d36d04"
   },
   "source": [
    "Viewing the resulting DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6332ecb9-4729-49f6-a709-e4e3997ea175",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6332ecb9-4729-49f6-a709-e4e3997ea175",
    "outputId": "e14f03aa-ac61-41b4-bdcf-724f314aea0e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>What's the better way to charge for a cloud pl...</td>\n",
       "      <td>Like all good questions, the answer depends. I...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>Haskell AND Lisp vs. Haskell OR Lisp</td>\n",
       "      <td>I suggest learning both, Haskell first, then C...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>When is it better to offload work to the RDBMS...</td>\n",
       "      <td>You want to do all set-based operations in the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>Is it better to specialize in a single field I...</td>\n",
       "      <td>Specialise if you enjoy it  As you are aware, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>Microsoft SDE Interview vs Microsoft SDET Inte...</td>\n",
       "      <td>Unfortunately, those are both myths. SDEs and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ds  \\\n",
       "0  softwareengineering.stackexchange   \n",
       "1  softwareengineering.stackexchange   \n",
       "2  softwareengineering.stackexchange   \n",
       "3  softwareengineering.stackexchange   \n",
       "4  softwareengineering.stackexchange   \n",
       "\n",
       "                                            question  \\\n",
       "0  What's the better way to charge for a cloud pl...   \n",
       "1               Haskell AND Lisp vs. Haskell OR Lisp   \n",
       "2  When is it better to offload work to the RDBMS...   \n",
       "3  Is it better to specialize in a single field I...   \n",
       "4  Microsoft SDE Interview vs Microsoft SDET Inte...   \n",
       "\n",
       "                                              answer  answer_stance  \n",
       "0  Like all good questions, the answer depends. I...              2  \n",
       "1  I suggest learning both, Haskell first, then C...              2  \n",
       "2  You want to do all set-based operations in the...              2  \n",
       "3  Specialise if you enjoy it  As you are aware, ...              2  \n",
       "4  Unfortunately, those are both myths. SDEs and ...              1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nno.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f1f7e70-c2ce-4169-863f-31ffd1fec261",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9f1f7e70-c2ce-4169-863f-31ffd1fec261",
    "outputId": "bf73c937-fe46-4365-e77f-d6ec6a3d0721"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>What's the better way to charge for a cloud pl...</td>\n",
       "      <td>Like all good questions, the answer depends. I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>Haskell AND Lisp vs. Haskell OR Lisp</td>\n",
       "      <td>I suggest learning both, Haskell first, then C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>When is it better to offload work to the RDBMS...</td>\n",
       "      <td>You want to do all set-based operations in the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>Is it better to specialize in a single field I...</td>\n",
       "      <td>Specialise if you enjoy it  As you are aware, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>How is IntelliJ better than Eclipse?</td>\n",
       "      <td>I work with Intellij (9.0.4 Ultimate) and Ecli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ds  \\\n",
       "0  softwareengineering.stackexchange   \n",
       "1  softwareengineering.stackexchange   \n",
       "2  softwareengineering.stackexchange   \n",
       "3  softwareengineering.stackexchange   \n",
       "5  softwareengineering.stackexchange   \n",
       "\n",
       "                                            question  \\\n",
       "0  What's the better way to charge for a cloud pl...   \n",
       "1               Haskell AND Lisp vs. Haskell OR Lisp   \n",
       "2  When is it better to offload work to the RDBMS...   \n",
       "3  Is it better to specialize in a single field I...   \n",
       "5               How is IntelliJ better than Eclipse?   \n",
       "\n",
       "                                              answer  answer_stance  \n",
       "0  Like all good questions, the answer depends. I...              0  \n",
       "1  I suggest learning both, Haskell first, then C...              0  \n",
       "2  You want to do all set-based operations in the...              1  \n",
       "3  Specialise if you enjoy it  As you are aware, ...              1  \n",
       "5  I work with Intellij (9.0.4 Ultimate) and Ecli...              0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_obj.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3678186-06f9-436d-8865-e2d41ccd9ba4",
   "metadata": {
    "id": "a3678186-06f9-436d-8865-e2d41ccd9ba4"
   },
   "source": [
    "#### Building the networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfda394c-0c78-443f-8dd5-48695b9628ee",
   "metadata": {
    "id": "cfda394c-0c78-443f-8dd5-48695b9628ee"
   },
   "source": [
    "According to the aforementioned paper, a fine-tuned version of the *RoBERTA-Large-MNLI* is appropriate for the task of stance detection. For this reason, we chose to use a lighter model, originally fine-tuned on the same dataset: *distilbert-base-uncased-mnli* ([typeform/distilbert-base-uncased-mnli · Hugging Face](https://huggingface.co/typeform/distilbert-base-uncased-mnli)). Since the tasks are similar, but different, we load two variations of the previously hinted model, whose difference is just the head of the networks (some `Dense` layers that act as \"Classification\" layers), \"re-assembled\" in order to output either $2$ or $3$ values, depending on the number of needed classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a037363-ae24-4500-a72e-0d0227691033",
   "metadata": {
    "id": "2a037363-ae24-4500-a72e-0d0227691033"
   },
   "source": [
    "Retrieving the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2142398f-76a8-4383-9411-b2a661d64418",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2142398f-76a8-4383-9411-b2a661d64418",
    "outputId": "9d683b99-d9a4-4176-ecb3-3d76929fcfde"
   },
   "outputs": [],
   "source": [
    "model_nno = TFDistilBertForSequenceClassification.from_pretrained(\"typeform/distilbert-base-uncased-mnli\", num_labels=3)\n",
    "model_obj = TFDistilBertForSequenceClassification.from_pretrained(\"typeform/distilbert-base-uncased-mnli\", num_labels=2, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62d04419-def4-47a4-ba94-d8e3fd62526f",
   "metadata": {
    "id": "62d04419-def4-47a4-ba94-d8e3fd62526f"
   },
   "source": [
    "Looking at the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18357869-d9c2-4fab-b6fd-79c47a096251",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18357869-d9c2-4fab-b6fd-79c47a096251",
    "outputId": "d8361606-bc55-41a7-f72c-d008d55698cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  2307      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,955,779\n",
      "Trainable params: 66,955,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"tf_distil_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_nno.summary()\n",
    "model_obj.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c8e7c94-6a6d-4522-bfbd-562bd493c424",
   "metadata": {
    "id": "1c8e7c94-6a6d-4522-bfbd-562bd493c424"
   },
   "source": [
    "We want to fine-tune the models, therefore we freeze the part of the networks responsible of taking care of the *encoding* part of the process, and then we compile them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "285e9106-c344-4596-a31c-19c66b368d42",
   "metadata": {
    "id": "285e9106-c344-4596-a31c-19c66b368d42"
   },
   "source": [
    "Compiling the networks to prepare them for training (fine-tuning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2502a93f-3f5d-4d4f-a0c8-6222c2ad168c",
   "metadata": {
    "id": "2502a93f-3f5d-4d4f-a0c8-6222c2ad168c"
   },
   "outputs": [],
   "source": [
    "for model in [model_nno, model_obj]:\n",
    "    model.distilbert.trainable = False\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate = stance_config.INIT_LR),\n",
    "        metrics=keras.metrics.SparseCategoricalAccuracy()\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73f09a3d-6aa5-4c6f-a085-7cfb63f20bb6",
   "metadata": {
    "id": "73f09a3d-6aa5-4c6f-a085-7cfb63f20bb6"
   },
   "source": [
    "Defining the train, val, test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f54f921-5254-49eb-a925-322bb93a0a39",
   "metadata": {
    "id": "0f54f921-5254-49eb-a925-322bb93a0a39"
   },
   "outputs": [],
   "source": [
    "ratios = stance_config.TRAIN_VAL_TEST_RATIOS\n",
    "list_ratios = list(ratios.values())\n",
    "\n",
    "#Ensuring that the values represent proper proportions (no zeros allowed).\n",
    "assert np.sum(list_ratios) == 1 and np.sum(np.sign(list_ratios)) == 3, 'Please specfy valid ratios for the \"train, val, test\" split.'\n",
    "\n",
    "ds_splits_untok = {model_nno: None, model_obj: None}\n",
    "ds_splits = {model_nno: None, model_obj: None}\n",
    "\n",
    "ds_splits_untok[model_nno] = train_val_test_splits(df_nno, ratios)\n",
    "ds_splits_untok[model_obj] = train_val_test_splits(df_obj, ratios)\n",
    "\n",
    "ds_splits[model_nno] = train_val_test_splits(None, None, True)\n",
    "ds_splits[model_obj] = train_val_test_splits(None, None, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a936fe6-96d4-4b34-8708-7f93a91b8320",
   "metadata": {
    "id": "7a936fe6-96d4-4b34-8708-7f93a91b8320"
   },
   "source": [
    "Now processing the sets through the selected `DistilBertTokenizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e20acbe8-15b0-44fd-876a-049b6be2f7a0",
   "metadata": {
    "id": "e20acbe8-15b0-44fd-876a-049b6be2f7a0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenizing the inputs and preparing the outputs\n",
    "for model in [model_nno, model_obj]:\n",
    "    for set_name in ['train','val','test']:\n",
    "        ds_splits[model][f'x_{set_name}'] = tokenize_df(ds_splits_untok[model][f'x_{set_name}'],tokenizer)\n",
    "        ds_splits[model][f'y_{set_name}'] = ds_splits_untok[model][f'y_{set_name}'].to_numpy().flatten()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "415e181f-1144-4c2c-9293-0a9df8c7822a",
   "metadata": {
    "id": "415e181f-1144-4c2c-9293-0a9df8c7822a"
   },
   "source": [
    "Now configuring the callbacks for the training of the networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c2d27c0-6505-4d4c-9088-4422728dd6fc",
   "metadata": {
    "id": "9c2d27c0-6505-4d4c-9088-4422728dd6fc"
   },
   "outputs": [],
   "source": [
    "model_nno_checkpoint = keras.callbacks.ModelCheckpoint(stance_config.MODEL_NNO_CHECKPOINT_FOLDER, save_best_only=True, save_weights_only=True, verbose=1, mode=\"min\", monitor=\"val_loss\")\n",
    "model_obj_checkpoint = keras.callbacks.ModelCheckpoint(stance_config.MODEL_OBJ_CHECKPOINT_FOLDER, save_best_only=True, save_weights_only=True, verbose=1, mode=\"min\", monitor=\"val_loss\")\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', factor=.1, patience=5, min_lr=stance_config.MIN_LR)\n",
    "\n",
    "model_callbacks = {model_nno: [model_nno_checkpoint, reduce_lr], model_obj: [model_obj_checkpoint, reduce_lr]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6cc39e2-20f7-4e99-b02b-5d2e65b9a95d",
   "metadata": {
    "id": "d6cc39e2-20f7-4e99-b02b-5d2e65b9a95d"
   },
   "source": [
    "Ensuring that the folders needed to store the best weights of the networks exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01b35863-029a-4e59-8393-225561b904cb",
   "metadata": {
    "id": "01b35863-029a-4e59-8393-225561b904cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint folders created (if not existent). If the training has been already done, please copy the corresponding weights in the following folders:\n",
      "C:\\Users\\Andrea\\Script Python\\model_nno_checkpoint\n",
      "C:\\Users\\Andrea\\Script Python\\model_obj_checkpoint\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(stance_config.MODEL_NNO_CHECKPOINT_FOLDER, exist_ok = True)\n",
    "os.makedirs(stance_config.MODEL_OBJ_CHECKPOINT_FOLDER, exist_ok = True)\n",
    "\n",
    "print('Model checkpoint folders created (if not existent). If the training has been already done, please copy the corresponding weights in the following folders:')\n",
    "print(os.path.abspath(stance_config.MODEL_NNO_CHECKPOINT_FOLDER))\n",
    "print(os.path.abspath(stance_config.MODEL_OBJ_CHECKPOINT_FOLDER))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69cfa2df",
   "metadata": {},
   "source": [
    "#### Training and evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dfa4db5-cfc9-4d5c-b556-5ee4c5884460",
   "metadata": {},
   "source": [
    "Starting the training of the networks (or loading the weights if available):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d4ed6fb-0a89-4c18-9ba1-f06206efa109",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d4ed6fb-0a89-4c18-9ba1-f06206efa109",
    "outputId": "2e9adfd4-362f-4a36-8f68-eefe484744c8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 3.1413 - sparse_categorical_accuracy: 0.3325\n",
      "Epoch 1: val_loss improved from inf to 3.59115, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 25s 381ms/step - loss: 3.1413 - sparse_categorical_accuracy: 0.3325 - val_loss: 3.5912 - val_sparse_categorical_accuracy: 0.2812 - lr: 1.0000e-05\n",
      "Epoch 2/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 2.6535 - sparse_categorical_accuracy: 0.3573\n",
      "Epoch 2: val_loss improved from 3.59115 to 3.13424, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 18s 370ms/step - loss: 2.6535 - sparse_categorical_accuracy: 0.3573 - val_loss: 3.1342 - val_sparse_categorical_accuracy: 0.2812 - lr: 1.0000e-05\n",
      "Epoch 3/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 2.4056 - sparse_categorical_accuracy: 0.3455\n",
      "Epoch 3: val_loss improved from 3.13424 to 2.69428, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 2.4056 - sparse_categorical_accuracy: 0.3455 - val_loss: 2.6943 - val_sparse_categorical_accuracy: 0.2812 - lr: 1.0000e-05\n",
      "Epoch 4/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 2.0704 - sparse_categorical_accuracy: 0.3325\n",
      "Epoch 4: val_loss improved from 2.69428 to 2.29387, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 18s 367ms/step - loss: 2.0704 - sparse_categorical_accuracy: 0.3325 - val_loss: 2.2939 - val_sparse_categorical_accuracy: 0.3021 - lr: 1.0000e-05\n",
      "Epoch 5/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.7741 - sparse_categorical_accuracy: 0.3560\n",
      "Epoch 5: val_loss improved from 2.29387 to 1.94395, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 18s 368ms/step - loss: 1.7741 - sparse_categorical_accuracy: 0.3560 - val_loss: 1.9439 - val_sparse_categorical_accuracy: 0.3021 - lr: 1.0000e-05\n",
      "Epoch 6/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5370 - sparse_categorical_accuracy: 0.3783\n",
      "Epoch 6: val_loss improved from 1.94395 to 1.65698, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 18s 367ms/step - loss: 1.5370 - sparse_categorical_accuracy: 0.3783 - val_loss: 1.6570 - val_sparse_categorical_accuracy: 0.3125 - lr: 1.0000e-05\n",
      "Epoch 7/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3780 - sparse_categorical_accuracy: 0.3770\n",
      "Epoch 7: val_loss improved from 1.65698 to 1.42656, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 18s 366ms/step - loss: 1.3780 - sparse_categorical_accuracy: 0.3770 - val_loss: 1.4266 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-05\n",
      "Epoch 8/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2086 - sparse_categorical_accuracy: 0.4293\n",
      "Epoch 8: val_loss improved from 1.42656 to 1.25954, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 18s 368ms/step - loss: 1.2086 - sparse_categorical_accuracy: 0.4293 - val_loss: 1.2595 - val_sparse_categorical_accuracy: 0.4167 - lr: 1.0000e-05\n",
      "Epoch 9/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1225 - sparse_categorical_accuracy: 0.4660\n",
      "Epoch 9: val_loss improved from 1.25954 to 1.14395, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 1.1225 - sparse_categorical_accuracy: 0.4660 - val_loss: 1.1440 - val_sparse_categorical_accuracy: 0.4688 - lr: 1.0000e-05\n",
      "Epoch 10/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.0454 - sparse_categorical_accuracy: 0.5105\n",
      "Epoch 10: val_loss improved from 1.14395 to 1.06388, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 1.0454 - sparse_categorical_accuracy: 0.5105 - val_loss: 1.0639 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 11/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.0058 - sparse_categorical_accuracy: 0.5550\n",
      "Epoch 11: val_loss improved from 1.06388 to 1.00743, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 1.0058 - sparse_categorical_accuracy: 0.5550 - val_loss: 1.0074 - val_sparse_categorical_accuracy: 0.5938 - lr: 1.0000e-05\n",
      "Epoch 12/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.9643 - sparse_categorical_accuracy: 0.5654\n",
      "Epoch 12: val_loss improved from 1.00743 to 0.96486, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.9643 - sparse_categorical_accuracy: 0.5654 - val_loss: 0.9649 - val_sparse_categorical_accuracy: 0.5833 - lr: 1.0000e-05\n",
      "Epoch 13/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.9315 - sparse_categorical_accuracy: 0.5942\n",
      "Epoch 13: val_loss improved from 0.96486 to 0.93198, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.9315 - sparse_categorical_accuracy: 0.5942 - val_loss: 0.9320 - val_sparse_categorical_accuracy: 0.5833 - lr: 1.0000e-05\n",
      "Epoch 14/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.9036 - sparse_categorical_accuracy: 0.6047\n",
      "Epoch 14: val_loss improved from 0.93198 to 0.90868, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.9036 - sparse_categorical_accuracy: 0.6047 - val_loss: 0.9087 - val_sparse_categorical_accuracy: 0.6042 - lr: 1.0000e-05\n",
      "Epoch 15/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8963 - sparse_categorical_accuracy: 0.5995\n",
      "Epoch 15: val_loss improved from 0.90868 to 0.89009, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8963 - sparse_categorical_accuracy: 0.5995 - val_loss: 0.8901 - val_sparse_categorical_accuracy: 0.6042 - lr: 1.0000e-05\n",
      "Epoch 16/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8805 - sparse_categorical_accuracy: 0.6021\n",
      "Epoch 16: val_loss improved from 0.89009 to 0.87504, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8805 - sparse_categorical_accuracy: 0.6021 - val_loss: 0.8750 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 17/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8740 - sparse_categorical_accuracy: 0.5982\n",
      "Epoch 17: val_loss improved from 0.87504 to 0.86186, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 18s 366ms/step - loss: 0.8740 - sparse_categorical_accuracy: 0.5982 - val_loss: 0.8619 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 18/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8588 - sparse_categorical_accuracy: 0.6086\n",
      "Epoch 18: val_loss improved from 0.86186 to 0.85084, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8588 - sparse_categorical_accuracy: 0.6086 - val_loss: 0.8508 - val_sparse_categorical_accuracy: 0.6771 - lr: 1.0000e-05\n",
      "Epoch 19/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8560 - sparse_categorical_accuracy: 0.6139\n",
      "Epoch 19: val_loss improved from 0.85084 to 0.84382, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8560 - sparse_categorical_accuracy: 0.6139 - val_loss: 0.8438 - val_sparse_categorical_accuracy: 0.6771 - lr: 1.0000e-05\n",
      "Epoch 20/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8471 - sparse_categorical_accuracy: 0.6086\n",
      "Epoch 20: val_loss improved from 0.84382 to 0.83793, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8471 - sparse_categorical_accuracy: 0.6086 - val_loss: 0.8379 - val_sparse_categorical_accuracy: 0.6771 - lr: 1.0000e-05\n",
      "Epoch 21/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8468 - sparse_categorical_accuracy: 0.6126\n",
      "Epoch 21: val_loss improved from 0.83793 to 0.83055, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8468 - sparse_categorical_accuracy: 0.6126 - val_loss: 0.8306 - val_sparse_categorical_accuracy: 0.6771 - lr: 1.0000e-05\n",
      "Epoch 22/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8387 - sparse_categorical_accuracy: 0.6113\n",
      "Epoch 22: val_loss improved from 0.83055 to 0.82621, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8387 - sparse_categorical_accuracy: 0.6113 - val_loss: 0.8262 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 23/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8369 - sparse_categorical_accuracy: 0.6086\n",
      "Epoch 23: val_loss improved from 0.82621 to 0.82166, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8369 - sparse_categorical_accuracy: 0.6086 - val_loss: 0.8217 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 24/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8405 - sparse_categorical_accuracy: 0.6243\n",
      "Epoch 24: val_loss improved from 0.82166 to 0.81644, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8405 - sparse_categorical_accuracy: 0.6243 - val_loss: 0.8164 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 25/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8244 - sparse_categorical_accuracy: 0.6204\n",
      "Epoch 25: val_loss improved from 0.81644 to 0.81371, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8244 - sparse_categorical_accuracy: 0.6204 - val_loss: 0.8137 - val_sparse_categorical_accuracy: 0.6771 - lr: 1.0000e-05\n",
      "Epoch 26/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8305 - sparse_categorical_accuracy: 0.6113\n",
      "Epoch 26: val_loss improved from 0.81371 to 0.81090, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8305 - sparse_categorical_accuracy: 0.6113 - val_loss: 0.8109 - val_sparse_categorical_accuracy: 0.6771 - lr: 1.0000e-05\n",
      "Epoch 27/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8275 - sparse_categorical_accuracy: 0.6126\n",
      "Epoch 27: val_loss improved from 0.81090 to 0.80710, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8275 - sparse_categorical_accuracy: 0.6126 - val_loss: 0.8071 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 28/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8174 - sparse_categorical_accuracy: 0.6270\n",
      "Epoch 28: val_loss improved from 0.80710 to 0.80431, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8174 - sparse_categorical_accuracy: 0.6270 - val_loss: 0.8043 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 29/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8248 - sparse_categorical_accuracy: 0.6139\n",
      "Epoch 29: val_loss improved from 0.80431 to 0.80219, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8248 - sparse_categorical_accuracy: 0.6139 - val_loss: 0.8022 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 30/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8169 - sparse_categorical_accuracy: 0.6191\n",
      "Epoch 30: val_loss improved from 0.80219 to 0.80001, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8169 - sparse_categorical_accuracy: 0.6191 - val_loss: 0.8000 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 31/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8191 - sparse_categorical_accuracy: 0.6099\n",
      "Epoch 31: val_loss improved from 0.80001 to 0.79916, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8191 - sparse_categorical_accuracy: 0.6099 - val_loss: 0.7992 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 32/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8130 - sparse_categorical_accuracy: 0.6270\n",
      "Epoch 32: val_loss improved from 0.79916 to 0.79677, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8130 - sparse_categorical_accuracy: 0.6270 - val_loss: 0.7968 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 33/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8131 - sparse_categorical_accuracy: 0.6270\n",
      "Epoch 33: val_loss improved from 0.79677 to 0.79464, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8131 - sparse_categorical_accuracy: 0.6270 - val_loss: 0.7946 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 34/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8072 - sparse_categorical_accuracy: 0.6387\n",
      "Epoch 34: val_loss improved from 0.79464 to 0.79342, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8072 - sparse_categorical_accuracy: 0.6387 - val_loss: 0.7934 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 35/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8071 - sparse_categorical_accuracy: 0.6230\n",
      "Epoch 35: val_loss improved from 0.79342 to 0.79112, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8071 - sparse_categorical_accuracy: 0.6230 - val_loss: 0.7911 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 36/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8066 - sparse_categorical_accuracy: 0.6257\n",
      "Epoch 36: val_loss improved from 0.79112 to 0.78957, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8066 - sparse_categorical_accuracy: 0.6257 - val_loss: 0.7896 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 37/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8023 - sparse_categorical_accuracy: 0.6283\n",
      "Epoch 37: val_loss improved from 0.78957 to 0.78837, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8023 - sparse_categorical_accuracy: 0.6283 - val_loss: 0.7884 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 38/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8047 - sparse_categorical_accuracy: 0.6322\n",
      "Epoch 38: val_loss improved from 0.78837 to 0.78767, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8047 - sparse_categorical_accuracy: 0.6322 - val_loss: 0.7877 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 39/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8009 - sparse_categorical_accuracy: 0.6374\n",
      "Epoch 39: val_loss improved from 0.78767 to 0.78586, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8009 - sparse_categorical_accuracy: 0.6374 - val_loss: 0.7859 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 40/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.8067 - sparse_categorical_accuracy: 0.6243\n",
      "Epoch 40: val_loss improved from 0.78586 to 0.78375, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.8067 - sparse_categorical_accuracy: 0.6243 - val_loss: 0.7838 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 41/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7940 - sparse_categorical_accuracy: 0.6348\n",
      "Epoch 41: val_loss improved from 0.78375 to 0.78310, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7940 - sparse_categorical_accuracy: 0.6348 - val_loss: 0.7831 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 42/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7946 - sparse_categorical_accuracy: 0.6387\n",
      "Epoch 42: val_loss improved from 0.78310 to 0.78139, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7946 - sparse_categorical_accuracy: 0.6387 - val_loss: 0.7814 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 43/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7986 - sparse_categorical_accuracy: 0.6374\n",
      "Epoch 43: val_loss improved from 0.78139 to 0.78026, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7986 - sparse_categorical_accuracy: 0.6374 - val_loss: 0.7803 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 44/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7956 - sparse_categorical_accuracy: 0.6296\n",
      "Epoch 44: val_loss improved from 0.78026 to 0.77943, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7956 - sparse_categorical_accuracy: 0.6296 - val_loss: 0.7794 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 45/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7972 - sparse_categorical_accuracy: 0.6361\n",
      "Epoch 45: val_loss improved from 0.77943 to 0.77807, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7972 - sparse_categorical_accuracy: 0.6361 - val_loss: 0.7781 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 46/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7859 - sparse_categorical_accuracy: 0.6361\n",
      "Epoch 46: val_loss improved from 0.77807 to 0.77674, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7859 - sparse_categorical_accuracy: 0.6361 - val_loss: 0.7767 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 47/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7919 - sparse_categorical_accuracy: 0.6335\n",
      "Epoch 47: val_loss improved from 0.77674 to 0.77599, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7919 - sparse_categorical_accuracy: 0.6335 - val_loss: 0.7760 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 48/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7891 - sparse_categorical_accuracy: 0.6440\n",
      "Epoch 48: val_loss improved from 0.77599 to 0.77539, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7891 - sparse_categorical_accuracy: 0.6440 - val_loss: 0.7754 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 49/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7817 - sparse_categorical_accuracy: 0.6479\n",
      "Epoch 49: val_loss improved from 0.77539 to 0.77532, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7817 - sparse_categorical_accuracy: 0.6479 - val_loss: 0.7753 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 50/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7831 - sparse_categorical_accuracy: 0.6387\n",
      "Epoch 50: val_loss improved from 0.77532 to 0.77391, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7831 - sparse_categorical_accuracy: 0.6387 - val_loss: 0.7739 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 51/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7786 - sparse_categorical_accuracy: 0.6427\n",
      "Epoch 51: val_loss improved from 0.77391 to 0.77327, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7786 - sparse_categorical_accuracy: 0.6427 - val_loss: 0.7733 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 52/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7771 - sparse_categorical_accuracy: 0.6401\n",
      "Epoch 52: val_loss did not improve from 0.77327\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7771 - sparse_categorical_accuracy: 0.6401 - val_loss: 0.7737 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 53/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7774 - sparse_categorical_accuracy: 0.6571\n",
      "Epoch 53: val_loss improved from 0.77327 to 0.77172, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7774 - sparse_categorical_accuracy: 0.6571 - val_loss: 0.7717 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 54/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7698 - sparse_categorical_accuracy: 0.6623\n",
      "Epoch 54: val_loss improved from 0.77172 to 0.77058, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7698 - sparse_categorical_accuracy: 0.6623 - val_loss: 0.7706 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 55/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7697 - sparse_categorical_accuracy: 0.6479\n",
      "Epoch 55: val_loss improved from 0.77058 to 0.76993, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7697 - sparse_categorical_accuracy: 0.6479 - val_loss: 0.7699 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 56/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7754 - sparse_categorical_accuracy: 0.6479\n",
      "Epoch 56: val_loss improved from 0.76993 to 0.76911, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7754 - sparse_categorical_accuracy: 0.6479 - val_loss: 0.7691 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 57/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7704 - sparse_categorical_accuracy: 0.6636\n",
      "Epoch 57: val_loss improved from 0.76911 to 0.76909, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7704 - sparse_categorical_accuracy: 0.6636 - val_loss: 0.7691 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 58/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7688 - sparse_categorical_accuracy: 0.6531\n",
      "Epoch 58: val_loss improved from 0.76909 to 0.76871, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7688 - sparse_categorical_accuracy: 0.6531 - val_loss: 0.7687 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 59/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7622 - sparse_categorical_accuracy: 0.6531\n",
      "Epoch 59: val_loss did not improve from 0.76871\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7622 - sparse_categorical_accuracy: 0.6531 - val_loss: 0.7687 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 60/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7638 - sparse_categorical_accuracy: 0.6597\n",
      "Epoch 60: val_loss improved from 0.76871 to 0.76745, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7638 - sparse_categorical_accuracy: 0.6597 - val_loss: 0.7675 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 61/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7641 - sparse_categorical_accuracy: 0.6584\n",
      "Epoch 61: val_loss did not improve from 0.76745\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7641 - sparse_categorical_accuracy: 0.6584 - val_loss: 0.7679 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 62/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7652 - sparse_categorical_accuracy: 0.6597\n",
      "Epoch 62: val_loss did not improve from 0.76745\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7652 - sparse_categorical_accuracy: 0.6597 - val_loss: 0.7677 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 63/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7596 - sparse_categorical_accuracy: 0.6545\n",
      "Epoch 63: val_loss improved from 0.76745 to 0.76677, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 18s 366ms/step - loss: 0.7596 - sparse_categorical_accuracy: 0.6545 - val_loss: 0.7668 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 64/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7567 - sparse_categorical_accuracy: 0.6584\n",
      "Epoch 64: val_loss improved from 0.76677 to 0.76575, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7567 - sparse_categorical_accuracy: 0.6584 - val_loss: 0.7658 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 65/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7574 - sparse_categorical_accuracy: 0.6793\n",
      "Epoch 65: val_loss improved from 0.76575 to 0.76489, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7574 - sparse_categorical_accuracy: 0.6793 - val_loss: 0.7649 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 66/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7562 - sparse_categorical_accuracy: 0.6597\n",
      "Epoch 66: val_loss improved from 0.76489 to 0.76356, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7562 - sparse_categorical_accuracy: 0.6597 - val_loss: 0.7636 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 67/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7477 - sparse_categorical_accuracy: 0.6728\n",
      "Epoch 67: val_loss improved from 0.76356 to 0.76293, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7477 - sparse_categorical_accuracy: 0.6728 - val_loss: 0.7629 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 68/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7532 - sparse_categorical_accuracy: 0.6610\n",
      "Epoch 68: val_loss did not improve from 0.76293\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7532 - sparse_categorical_accuracy: 0.6610 - val_loss: 0.7633 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 69/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7448 - sparse_categorical_accuracy: 0.6649\n",
      "Epoch 69: val_loss improved from 0.76293 to 0.76211, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7448 - sparse_categorical_accuracy: 0.6649 - val_loss: 0.7621 - val_sparse_categorical_accuracy: 0.6354 - lr: 1.0000e-05\n",
      "Epoch 70/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7442 - sparse_categorical_accuracy: 0.6610\n",
      "Epoch 70: val_loss improved from 0.76211 to 0.76188, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7442 - sparse_categorical_accuracy: 0.6610 - val_loss: 0.7619 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 71/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7524 - sparse_categorical_accuracy: 0.6571\n",
      "Epoch 71: val_loss did not improve from 0.76188\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7524 - sparse_categorical_accuracy: 0.6571 - val_loss: 0.7620 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 72/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7351 - sparse_categorical_accuracy: 0.6702\n",
      "Epoch 72: val_loss did not improve from 0.76188\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7351 - sparse_categorical_accuracy: 0.6702 - val_loss: 0.7622 - val_sparse_categorical_accuracy: 0.6354 - lr: 1.0000e-05\n",
      "Epoch 73/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7433 - sparse_categorical_accuracy: 0.6610\n",
      "Epoch 73: val_loss did not improve from 0.76188\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7433 - sparse_categorical_accuracy: 0.6610 - val_loss: 0.7619 - val_sparse_categorical_accuracy: 0.6354 - lr: 1.0000e-05\n",
      "Epoch 74/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7395 - sparse_categorical_accuracy: 0.6715\n",
      "Epoch 74: val_loss improved from 0.76188 to 0.75989, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7395 - sparse_categorical_accuracy: 0.6715 - val_loss: 0.7599 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 75/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7345 - sparse_categorical_accuracy: 0.6610\n",
      "Epoch 75: val_loss improved from 0.75989 to 0.75841, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 18s 366ms/step - loss: 0.7345 - sparse_categorical_accuracy: 0.6610 - val_loss: 0.7584 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 76/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7375 - sparse_categorical_accuracy: 0.6793\n",
      "Epoch 76: val_loss did not improve from 0.75841\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7375 - sparse_categorical_accuracy: 0.6793 - val_loss: 0.7594 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 77/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7303 - sparse_categorical_accuracy: 0.6846\n",
      "Epoch 77: val_loss did not improve from 0.75841\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7303 - sparse_categorical_accuracy: 0.6846 - val_loss: 0.7598 - val_sparse_categorical_accuracy: 0.6354 - lr: 1.0000e-05\n",
      "Epoch 78/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7342 - sparse_categorical_accuracy: 0.6662\n",
      "Epoch 78: val_loss did not improve from 0.75841\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7342 - sparse_categorical_accuracy: 0.6662 - val_loss: 0.7594 - val_sparse_categorical_accuracy: 0.6354 - lr: 1.0000e-05\n",
      "Epoch 79/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7294 - sparse_categorical_accuracy: 0.6754\n",
      "Epoch 79: val_loss improved from 0.75841 to 0.75754, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 18s 366ms/step - loss: 0.7294 - sparse_categorical_accuracy: 0.6754 - val_loss: 0.7575 - val_sparse_categorical_accuracy: 0.6250 - lr: 1.0000e-05\n",
      "Epoch 80/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7246 - sparse_categorical_accuracy: 0.6832\n",
      "Epoch 80: val_loss did not improve from 0.75754\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7246 - sparse_categorical_accuracy: 0.6832 - val_loss: 0.7582 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 81/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7329 - sparse_categorical_accuracy: 0.6715\n",
      "Epoch 81: val_loss did not improve from 0.75754\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7329 - sparse_categorical_accuracy: 0.6715 - val_loss: 0.7585 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 82/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7365 - sparse_categorical_accuracy: 0.6675\n",
      "Epoch 82: val_loss did not improve from 0.75754\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7365 - sparse_categorical_accuracy: 0.6675 - val_loss: 0.7579 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 83/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7270 - sparse_categorical_accuracy: 0.6859\n",
      "Epoch 83: val_loss did not improve from 0.75754\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7270 - sparse_categorical_accuracy: 0.6859 - val_loss: 0.7577 - val_sparse_categorical_accuracy: 0.6354 - lr: 1.0000e-05\n",
      "Epoch 84/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7228 - sparse_categorical_accuracy: 0.6793\n",
      "Epoch 84: val_loss improved from 0.75754 to 0.75602, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 18s 366ms/step - loss: 0.7228 - sparse_categorical_accuracy: 0.6793 - val_loss: 0.7560 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-05\n",
      "Epoch 85/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7199 - sparse_categorical_accuracy: 0.6754\n",
      "Epoch 85: val_loss improved from 0.75602 to 0.75572, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7199 - sparse_categorical_accuracy: 0.6754 - val_loss: 0.7557 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 86/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7206 - sparse_categorical_accuracy: 0.6793\n",
      "Epoch 86: val_loss improved from 0.75572 to 0.75464, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 18s 366ms/step - loss: 0.7206 - sparse_categorical_accuracy: 0.6793 - val_loss: 0.7546 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 87/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7097 - sparse_categorical_accuracy: 0.6950\n",
      "Epoch 87: val_loss did not improve from 0.75464\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7097 - sparse_categorical_accuracy: 0.6950 - val_loss: 0.7547 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 88/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7207 - sparse_categorical_accuracy: 0.6950\n",
      "Epoch 88: val_loss improved from 0.75464 to 0.75344, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7207 - sparse_categorical_accuracy: 0.6950 - val_loss: 0.7534 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 89/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7160 - sparse_categorical_accuracy: 0.6898\n",
      "Epoch 89: val_loss improved from 0.75344 to 0.75180, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 18s 366ms/step - loss: 0.7160 - sparse_categorical_accuracy: 0.6898 - val_loss: 0.7518 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 90/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7165 - sparse_categorical_accuracy: 0.6793\n",
      "Epoch 90: val_loss improved from 0.75180 to 0.75164, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7165 - sparse_categorical_accuracy: 0.6793 - val_loss: 0.7516 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 91/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7159 - sparse_categorical_accuracy: 0.6728\n",
      "Epoch 91: val_loss did not improve from 0.75164\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7159 - sparse_categorical_accuracy: 0.6728 - val_loss: 0.7523 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 92/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7145 - sparse_categorical_accuracy: 0.6872\n",
      "Epoch 92: val_loss improved from 0.75164 to 0.75102, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7145 - sparse_categorical_accuracy: 0.6872 - val_loss: 0.7510 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 93/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7108 - sparse_categorical_accuracy: 0.6963\n",
      "Epoch 93: val_loss improved from 0.75102 to 0.75091, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7108 - sparse_categorical_accuracy: 0.6963 - val_loss: 0.7509 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 94/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7034 - sparse_categorical_accuracy: 0.6885\n",
      "Epoch 94: val_loss improved from 0.75091 to 0.75070, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7034 - sparse_categorical_accuracy: 0.6885 - val_loss: 0.7507 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 95/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7073 - sparse_categorical_accuracy: 0.6846\n",
      "Epoch 95: val_loss improved from 0.75070 to 0.74891, saving model to ./model_nno_checkpoint\\\n",
      "48/48 [==============================] - 17s 365ms/step - loss: 0.7073 - sparse_categorical_accuracy: 0.6846 - val_loss: 0.7489 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 96/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7064 - sparse_categorical_accuracy: 0.6963\n",
      "Epoch 96: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7064 - sparse_categorical_accuracy: 0.6963 - val_loss: 0.7499 - val_sparse_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 97/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7038 - sparse_categorical_accuracy: 0.6924\n",
      "Epoch 97: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7038 - sparse_categorical_accuracy: 0.6924 - val_loss: 0.7489 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 98/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7044 - sparse_categorical_accuracy: 0.6911\n",
      "Epoch 98: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7044 - sparse_categorical_accuracy: 0.6911 - val_loss: 0.7508 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 99/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6996 - sparse_categorical_accuracy: 0.6872\n",
      "Epoch 99: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.6996 - sparse_categorical_accuracy: 0.6872 - val_loss: 0.7501 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 100/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6981 - sparse_categorical_accuracy: 0.6819\n",
      "Epoch 100: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.6981 - sparse_categorical_accuracy: 0.6819 - val_loss: 0.7498 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-05\n",
      "Epoch 101/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6904 - sparse_categorical_accuracy: 0.6976\n",
      "Epoch 101: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.6904 - sparse_categorical_accuracy: 0.6976 - val_loss: 0.7497 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-06\n",
      "Epoch 102/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6990 - sparse_categorical_accuracy: 0.6872\n",
      "Epoch 102: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.6990 - sparse_categorical_accuracy: 0.6872 - val_loss: 0.7498 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-06\n",
      "Epoch 103/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7049 - sparse_categorical_accuracy: 0.6950\n",
      "Epoch 103: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7049 - sparse_categorical_accuracy: 0.6950 - val_loss: 0.7497 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-06\n",
      "Epoch 104/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6904 - sparse_categorical_accuracy: 0.6990\n",
      "Epoch 104: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.6904 - sparse_categorical_accuracy: 0.6990 - val_loss: 0.7497 - val_sparse_categorical_accuracy: 0.6562 - lr: 1.0000e-06\n",
      "Epoch 105/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6998 - sparse_categorical_accuracy: 0.6976\n",
      "Epoch 105: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6998 - sparse_categorical_accuracy: 0.6976 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-06\n",
      "Epoch 106/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7034 - sparse_categorical_accuracy: 0.7055\n",
      "Epoch 106: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7034 - sparse_categorical_accuracy: 0.7055 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-07\n",
      "Epoch 107/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6989 - sparse_categorical_accuracy: 0.6924\n",
      "Epoch 107: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.6989 - sparse_categorical_accuracy: 0.6924 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-07\n",
      "Epoch 108/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6983 - sparse_categorical_accuracy: 0.7003\n",
      "Epoch 108: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6983 - sparse_categorical_accuracy: 0.7003 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-07\n",
      "Epoch 109/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7038 - sparse_categorical_accuracy: 0.6859\n",
      "Epoch 109: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7038 - sparse_categorical_accuracy: 0.6859 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-07\n",
      "Epoch 110/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7008 - sparse_categorical_accuracy: 0.6898\n",
      "Epoch 110: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7008 - sparse_categorical_accuracy: 0.6898 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-07\n",
      "Epoch 111/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7046 - sparse_categorical_accuracy: 0.6990\n",
      "Epoch 111: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7046 - sparse_categorical_accuracy: 0.6990 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 112/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6974 - sparse_categorical_accuracy: 0.6976\n",
      "Epoch 112: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.6974 - sparse_categorical_accuracy: 0.6976 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 113/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7047 - sparse_categorical_accuracy: 0.7029\n",
      "Epoch 113: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.7047 - sparse_categorical_accuracy: 0.7029 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 114/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6928 - sparse_categorical_accuracy: 0.6898\n",
      "Epoch 114: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6928 - sparse_categorical_accuracy: 0.6898 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 115/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6962 - sparse_categorical_accuracy: 0.6806\n",
      "Epoch 115: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6962 - sparse_categorical_accuracy: 0.6806 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 116/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6911 - sparse_categorical_accuracy: 0.7081\n",
      "Epoch 116: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6911 - sparse_categorical_accuracy: 0.7081 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 117/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6883 - sparse_categorical_accuracy: 0.7003\n",
      "Epoch 117: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6883 - sparse_categorical_accuracy: 0.7003 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 118/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7055 - sparse_categorical_accuracy: 0.6898\n",
      "Epoch 118: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7055 - sparse_categorical_accuracy: 0.6898 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 119/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6892 - sparse_categorical_accuracy: 0.6950\n",
      "Epoch 119: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6892 - sparse_categorical_accuracy: 0.6950 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 120/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6930 - sparse_categorical_accuracy: 0.6950\n",
      "Epoch 120: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6930 - sparse_categorical_accuracy: 0.6950 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 121/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6952 - sparse_categorical_accuracy: 0.6911\n",
      "Epoch 121: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6952 - sparse_categorical_accuracy: 0.6911 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 122/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6928 - sparse_categorical_accuracy: 0.6976\n",
      "Epoch 122: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6928 - sparse_categorical_accuracy: 0.6976 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 123/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7036 - sparse_categorical_accuracy: 0.6911\n",
      "Epoch 123: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7036 - sparse_categorical_accuracy: 0.6911 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 124/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6938 - sparse_categorical_accuracy: 0.6950\n",
      "Epoch 124: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6938 - sparse_categorical_accuracy: 0.6950 - val_loss: 0.7495 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 125/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6968 - sparse_categorical_accuracy: 0.6976\n",
      "Epoch 125: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6968 - sparse_categorical_accuracy: 0.6976 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 126/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6963 - sparse_categorical_accuracy: 0.6950\n",
      "Epoch 126: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6963 - sparse_categorical_accuracy: 0.6950 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 127/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7028 - sparse_categorical_accuracy: 0.6898\n",
      "Epoch 127: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7028 - sparse_categorical_accuracy: 0.6898 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 128/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7001 - sparse_categorical_accuracy: 0.6859\n",
      "Epoch 128: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7001 - sparse_categorical_accuracy: 0.6859 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 129/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6959 - sparse_categorical_accuracy: 0.6976\n",
      "Epoch 129: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6959 - sparse_categorical_accuracy: 0.6976 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 130/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6949 - sparse_categorical_accuracy: 0.6990\n",
      "Epoch 130: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6949 - sparse_categorical_accuracy: 0.6990 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 131/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6957 - sparse_categorical_accuracy: 0.6950\n",
      "Epoch 131: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6957 - sparse_categorical_accuracy: 0.6950 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 132/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6960 - sparse_categorical_accuracy: 0.6911\n",
      "Epoch 132: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6960 - sparse_categorical_accuracy: 0.6911 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 133/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6936 - sparse_categorical_accuracy: 0.7068\n",
      "Epoch 133: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6936 - sparse_categorical_accuracy: 0.7068 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 134/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7012 - sparse_categorical_accuracy: 0.7016\n",
      "Epoch 134: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7012 - sparse_categorical_accuracy: 0.7016 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 135/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6932 - sparse_categorical_accuracy: 0.6924\n",
      "Epoch 135: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6932 - sparse_categorical_accuracy: 0.6924 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 136/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6969 - sparse_categorical_accuracy: 0.7055\n",
      "Epoch 136: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.6969 - sparse_categorical_accuracy: 0.7055 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 137/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6985 - sparse_categorical_accuracy: 0.6963\n",
      "Epoch 137: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.6985 - sparse_categorical_accuracy: 0.6963 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 138/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6912 - sparse_categorical_accuracy: 0.6976\n",
      "Epoch 138: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6912 - sparse_categorical_accuracy: 0.6976 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 139/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6921 - sparse_categorical_accuracy: 0.6963\n",
      "Epoch 139: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6921 - sparse_categorical_accuracy: 0.6963 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 140/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7032 - sparse_categorical_accuracy: 0.6924\n",
      "Epoch 140: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7032 - sparse_categorical_accuracy: 0.6924 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 141/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6948 - sparse_categorical_accuracy: 0.6990\n",
      "Epoch 141: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6948 - sparse_categorical_accuracy: 0.6990 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 142/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6993 - sparse_categorical_accuracy: 0.6872\n",
      "Epoch 142: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6993 - sparse_categorical_accuracy: 0.6872 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 143/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7015 - sparse_categorical_accuracy: 0.7003\n",
      "Epoch 143: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7015 - sparse_categorical_accuracy: 0.7003 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 144/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6943 - sparse_categorical_accuracy: 0.6872\n",
      "Epoch 144: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.6943 - sparse_categorical_accuracy: 0.6872 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 145/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6890 - sparse_categorical_accuracy: 0.6924\n",
      "Epoch 145: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.6890 - sparse_categorical_accuracy: 0.6924 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 146/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6999 - sparse_categorical_accuracy: 0.6832\n",
      "Epoch 146: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.6999 - sparse_categorical_accuracy: 0.6832 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 147/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6969 - sparse_categorical_accuracy: 0.6885\n",
      "Epoch 147: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 353ms/step - loss: 0.6969 - sparse_categorical_accuracy: 0.6885 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 148/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6971 - sparse_categorical_accuracy: 0.7042\n",
      "Epoch 148: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6971 - sparse_categorical_accuracy: 0.7042 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 149/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.7027 - sparse_categorical_accuracy: 0.6819\n",
      "Epoch 149: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.7027 - sparse_categorical_accuracy: 0.6819 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 150/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6932 - sparse_categorical_accuracy: 0.6898\n",
      "Epoch 150: val_loss did not improve from 0.74891\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.6932 - sparse_categorical_accuracy: 0.6898 - val_loss: 0.7494 - val_sparse_categorical_accuracy: 0.6458 - lr: 1.0000e-08\n",
      "Epoch 1/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.7181 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 1: val_loss improved from inf to 0.71249, saving model to ./model_obj_checkpoint\\\n",
      "30/30 [==============================] - 14s 397ms/step - loss: 0.7181 - sparse_categorical_accuracy: 0.5000 - val_loss: 0.7125 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-05\n",
      "Epoch 2/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.7034 - sparse_categorical_accuracy: 0.4938\n",
      "Epoch 2: val_loss improved from 0.71249 to 0.70833, saving model to ./model_obj_checkpoint\\\n",
      "30/30 [==============================] - 11s 364ms/step - loss: 0.7034 - sparse_categorical_accuracy: 0.4938 - val_loss: 0.7083 - val_sparse_categorical_accuracy: 0.4500 - lr: 1.0000e-05\n",
      "Epoch 3/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6967 - sparse_categorical_accuracy: 0.5083\n",
      "Epoch 3: val_loss improved from 0.70833 to 0.70587, saving model to ./model_obj_checkpoint\\\n",
      "30/30 [==============================] - 11s 364ms/step - loss: 0.6967 - sparse_categorical_accuracy: 0.5083 - val_loss: 0.7059 - val_sparse_categorical_accuracy: 0.4333 - lr: 1.0000e-05\n",
      "Epoch 4/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6936 - sparse_categorical_accuracy: 0.5146\n",
      "Epoch 4: val_loss improved from 0.70587 to 0.70387, saving model to ./model_obj_checkpoint\\\n",
      "30/30 [==============================] - 11s 364ms/step - loss: 0.6936 - sparse_categorical_accuracy: 0.5146 - val_loss: 0.7039 - val_sparse_categorical_accuracy: 0.4333 - lr: 1.0000e-05\n",
      "Epoch 5/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6943 - sparse_categorical_accuracy: 0.5188\n",
      "Epoch 5: val_loss improved from 0.70387 to 0.69978, saving model to ./model_obj_checkpoint\\\n",
      "30/30 [==============================] - 11s 364ms/step - loss: 0.6943 - sparse_categorical_accuracy: 0.5188 - val_loss: 0.6998 - val_sparse_categorical_accuracy: 0.4333 - lr: 1.0000e-05\n",
      "Epoch 6/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6864 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 6: val_loss improved from 0.69978 to 0.69926, saving model to ./model_obj_checkpoint\\\n",
      "30/30 [==============================] - 11s 364ms/step - loss: 0.6864 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6993 - val_sparse_categorical_accuracy: 0.4167 - lr: 1.0000e-05\n",
      "Epoch 7/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6828 - sparse_categorical_accuracy: 0.5437\n",
      "Epoch 7: val_loss improved from 0.69926 to 0.69811, saving model to ./model_obj_checkpoint\\\n",
      "30/30 [==============================] - 11s 364ms/step - loss: 0.6828 - sparse_categorical_accuracy: 0.5437 - val_loss: 0.6981 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 8/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6894 - sparse_categorical_accuracy: 0.5250\n",
      "Epoch 8: val_loss improved from 0.69811 to 0.69483, saving model to ./model_obj_checkpoint\\\n",
      "30/30 [==============================] - 11s 364ms/step - loss: 0.6894 - sparse_categorical_accuracy: 0.5250 - val_loss: 0.6948 - val_sparse_categorical_accuracy: 0.5167 - lr: 1.0000e-05\n",
      "Epoch 9/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6863 - sparse_categorical_accuracy: 0.5500\n",
      "Epoch 9: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6863 - sparse_categorical_accuracy: 0.5500 - val_loss: 0.6953 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 10/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6808 - sparse_categorical_accuracy: 0.5750\n",
      "Epoch 10: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6808 - sparse_categorical_accuracy: 0.5750 - val_loss: 0.6955 - val_sparse_categorical_accuracy: 0.5167 - lr: 1.0000e-05\n",
      "Epoch 11/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6822 - sparse_categorical_accuracy: 0.5708\n",
      "Epoch 11: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6822 - sparse_categorical_accuracy: 0.5708 - val_loss: 0.6964 - val_sparse_categorical_accuracy: 0.5167 - lr: 1.0000e-05\n",
      "Epoch 12/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6704 - sparse_categorical_accuracy: 0.5979\n",
      "Epoch 12: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6704 - sparse_categorical_accuracy: 0.5979 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 13/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6719 - sparse_categorical_accuracy: 0.5583\n",
      "Epoch 13: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6719 - sparse_categorical_accuracy: 0.5583 - val_loss: 0.6956 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-05\n",
      "Epoch 14/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6717 - sparse_categorical_accuracy: 0.5771\n",
      "Epoch 14: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6717 - sparse_categorical_accuracy: 0.5771 - val_loss: 0.6955 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-06\n",
      "Epoch 15/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6723 - sparse_categorical_accuracy: 0.6021\n",
      "Epoch 15: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6723 - sparse_categorical_accuracy: 0.6021 - val_loss: 0.6955 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-06\n",
      "Epoch 16/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6743 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 16: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6743 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6955 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-06\n",
      "Epoch 17/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6735 - sparse_categorical_accuracy: 0.5750\n",
      "Epoch 17: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6735 - sparse_categorical_accuracy: 0.5750 - val_loss: 0.6958 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-06\n",
      "Epoch 18/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6682 - sparse_categorical_accuracy: 0.5750\n",
      "Epoch 18: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6682 - sparse_categorical_accuracy: 0.5750 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-06\n",
      "Epoch 19/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6686 - sparse_categorical_accuracy: 0.5875\n",
      "Epoch 19: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6686 - sparse_categorical_accuracy: 0.5875 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-07\n",
      "Epoch 20/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6665 - sparse_categorical_accuracy: 0.5938\n",
      "Epoch 20: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6665 - sparse_categorical_accuracy: 0.5938 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-07\n",
      "Epoch 21/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6732 - sparse_categorical_accuracy: 0.5896\n",
      "Epoch 21: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6732 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-07\n",
      "Epoch 22/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6682 - sparse_categorical_accuracy: 0.6146\n",
      "Epoch 22: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6682 - sparse_categorical_accuracy: 0.6146 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-07\n",
      "Epoch 23/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6685 - sparse_categorical_accuracy: 0.6125\n",
      "Epoch 23: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6685 - sparse_categorical_accuracy: 0.6125 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-07\n",
      "Epoch 24/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6778 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 24: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6778 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 25/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6753 - sparse_categorical_accuracy: 0.5833\n",
      "Epoch 25: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6753 - sparse_categorical_accuracy: 0.5833 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 26/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6798 - sparse_categorical_accuracy: 0.5625\n",
      "Epoch 26: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6798 - sparse_categorical_accuracy: 0.5625 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 27/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6733 - sparse_categorical_accuracy: 0.5833\n",
      "Epoch 27: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6733 - sparse_categorical_accuracy: 0.5833 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 28/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6755 - sparse_categorical_accuracy: 0.5688\n",
      "Epoch 28: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6755 - sparse_categorical_accuracy: 0.5688 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 29/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6779 - sparse_categorical_accuracy: 0.5646\n",
      "Epoch 29: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6779 - sparse_categorical_accuracy: 0.5646 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 30/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6728 - sparse_categorical_accuracy: 0.5688\n",
      "Epoch 30: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6728 - sparse_categorical_accuracy: 0.5688 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 31/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6724 - sparse_categorical_accuracy: 0.5792\n",
      "Epoch 31: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6724 - sparse_categorical_accuracy: 0.5792 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 32/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6713 - sparse_categorical_accuracy: 0.5750\n",
      "Epoch 32: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6713 - sparse_categorical_accuracy: 0.5750 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 33/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6663 - sparse_categorical_accuracy: 0.5917\n",
      "Epoch 33: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6663 - sparse_categorical_accuracy: 0.5917 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 34/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6688 - sparse_categorical_accuracy: 0.6000\n",
      "Epoch 34: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6688 - sparse_categorical_accuracy: 0.6000 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 35/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6698 - sparse_categorical_accuracy: 0.6083\n",
      "Epoch 35: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6698 - sparse_categorical_accuracy: 0.6083 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 36/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6809 - sparse_categorical_accuracy: 0.5729\n",
      "Epoch 36: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6809 - sparse_categorical_accuracy: 0.5729 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 37/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6628 - sparse_categorical_accuracy: 0.6021\n",
      "Epoch 37: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6628 - sparse_categorical_accuracy: 0.6021 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 38/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6707 - sparse_categorical_accuracy: 0.5917\n",
      "Epoch 38: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6707 - sparse_categorical_accuracy: 0.5917 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 39/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6659 - sparse_categorical_accuracy: 0.5792\n",
      "Epoch 39: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6659 - sparse_categorical_accuracy: 0.5792 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 40/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6708 - sparse_categorical_accuracy: 0.5833\n",
      "Epoch 40: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6708 - sparse_categorical_accuracy: 0.5833 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 41/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6726 - sparse_categorical_accuracy: 0.5562\n",
      "Epoch 41: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6726 - sparse_categorical_accuracy: 0.5562 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 42/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6827 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 42: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6827 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 43/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6769 - sparse_categorical_accuracy: 0.5771\n",
      "Epoch 43: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6769 - sparse_categorical_accuracy: 0.5771 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 44/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6785 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 44: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6785 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 45/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6742 - sparse_categorical_accuracy: 0.5688\n",
      "Epoch 45: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6742 - sparse_categorical_accuracy: 0.5688 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 46/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6726 - sparse_categorical_accuracy: 0.5771\n",
      "Epoch 46: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6726 - sparse_categorical_accuracy: 0.5771 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 47/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6692 - sparse_categorical_accuracy: 0.5437\n",
      "Epoch 47: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6692 - sparse_categorical_accuracy: 0.5437 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 48/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6715 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 48: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6715 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 49/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6663 - sparse_categorical_accuracy: 0.5958\n",
      "Epoch 49: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6663 - sparse_categorical_accuracy: 0.5958 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 50/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6710 - sparse_categorical_accuracy: 0.5875\n",
      "Epoch 50: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6710 - sparse_categorical_accuracy: 0.5875 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 51/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6781 - sparse_categorical_accuracy: 0.5583\n",
      "Epoch 51: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6781 - sparse_categorical_accuracy: 0.5583 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 52/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6722 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 52: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6722 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 53/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6765 - sparse_categorical_accuracy: 0.5500\n",
      "Epoch 53: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6765 - sparse_categorical_accuracy: 0.5500 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 54/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6708 - sparse_categorical_accuracy: 0.5708\n",
      "Epoch 54: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6708 - sparse_categorical_accuracy: 0.5708 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 55/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6739 - sparse_categorical_accuracy: 0.5604\n",
      "Epoch 55: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6739 - sparse_categorical_accuracy: 0.5604 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 56/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6746 - sparse_categorical_accuracy: 0.5625\n",
      "Epoch 56: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6746 - sparse_categorical_accuracy: 0.5625 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 57/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6688 - sparse_categorical_accuracy: 0.5979\n",
      "Epoch 57: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6688 - sparse_categorical_accuracy: 0.5979 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 58/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6713 - sparse_categorical_accuracy: 0.6062\n",
      "Epoch 58: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6713 - sparse_categorical_accuracy: 0.6062 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 59/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6749 - sparse_categorical_accuracy: 0.5604\n",
      "Epoch 59: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6749 - sparse_categorical_accuracy: 0.5604 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 60/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6691 - sparse_categorical_accuracy: 0.5729\n",
      "Epoch 60: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6691 - sparse_categorical_accuracy: 0.5729 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 61/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6756 - sparse_categorical_accuracy: 0.5562\n",
      "Epoch 61: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6756 - sparse_categorical_accuracy: 0.5562 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 62/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6632 - sparse_categorical_accuracy: 0.5979\n",
      "Epoch 62: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6632 - sparse_categorical_accuracy: 0.5979 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 63/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6777 - sparse_categorical_accuracy: 0.5437\n",
      "Epoch 63: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6777 - sparse_categorical_accuracy: 0.5437 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 64/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6732 - sparse_categorical_accuracy: 0.6042\n",
      "Epoch 64: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6732 - sparse_categorical_accuracy: 0.6042 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 65/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6811 - sparse_categorical_accuracy: 0.5521\n",
      "Epoch 65: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6811 - sparse_categorical_accuracy: 0.5521 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 66/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6714 - sparse_categorical_accuracy: 0.5771\n",
      "Epoch 66: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6714 - sparse_categorical_accuracy: 0.5771 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 67/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6740 - sparse_categorical_accuracy: 0.5750\n",
      "Epoch 67: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6740 - sparse_categorical_accuracy: 0.5750 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 68/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6641 - sparse_categorical_accuracy: 0.6333\n",
      "Epoch 68: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6641 - sparse_categorical_accuracy: 0.6333 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 69/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6786 - sparse_categorical_accuracy: 0.5833\n",
      "Epoch 69: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6786 - sparse_categorical_accuracy: 0.5833 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 70/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6658 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 70: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6658 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 71/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6699 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 71: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6699 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 72/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6767 - sparse_categorical_accuracy: 0.5479\n",
      "Epoch 72: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6767 - sparse_categorical_accuracy: 0.5479 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 73/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6770 - sparse_categorical_accuracy: 0.5729\n",
      "Epoch 73: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6770 - sparse_categorical_accuracy: 0.5729 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 74/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6769 - sparse_categorical_accuracy: 0.5792\n",
      "Epoch 74: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6769 - sparse_categorical_accuracy: 0.5792 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 75/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6804 - sparse_categorical_accuracy: 0.5542\n",
      "Epoch 75: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6804 - sparse_categorical_accuracy: 0.5542 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 76/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6680 - sparse_categorical_accuracy: 0.5896\n",
      "Epoch 76: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6680 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 77/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6758 - sparse_categorical_accuracy: 0.5729\n",
      "Epoch 77: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6758 - sparse_categorical_accuracy: 0.5729 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 78/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6742 - sparse_categorical_accuracy: 0.5688\n",
      "Epoch 78: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6742 - sparse_categorical_accuracy: 0.5688 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 79/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6800 - sparse_categorical_accuracy: 0.5750\n",
      "Epoch 79: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6800 - sparse_categorical_accuracy: 0.5750 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 80/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6729 - sparse_categorical_accuracy: 0.6021\n",
      "Epoch 80: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6729 - sparse_categorical_accuracy: 0.6021 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 81/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6749 - sparse_categorical_accuracy: 0.5896\n",
      "Epoch 81: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6749 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 82/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6770 - sparse_categorical_accuracy: 0.5729\n",
      "Epoch 82: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6770 - sparse_categorical_accuracy: 0.5729 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 83/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6634 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 83: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 349ms/step - loss: 0.6634 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 84/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6738 - sparse_categorical_accuracy: 0.5917\n",
      "Epoch 84: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 12s 389ms/step - loss: 0.6738 - sparse_categorical_accuracy: 0.5917 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 85/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6737 - sparse_categorical_accuracy: 0.5833\n",
      "Epoch 85: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 11s 374ms/step - loss: 0.6737 - sparse_categorical_accuracy: 0.5833 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 86/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6778 - sparse_categorical_accuracy: 0.5833\n",
      "Epoch 86: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 11s 352ms/step - loss: 0.6778 - sparse_categorical_accuracy: 0.5833 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 87/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6655 - sparse_categorical_accuracy: 0.5771\n",
      "Epoch 87: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6655 - sparse_categorical_accuracy: 0.5771 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 88/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6778 - sparse_categorical_accuracy: 0.5500\n",
      "Epoch 88: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6778 - sparse_categorical_accuracy: 0.5500 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 89/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6784 - sparse_categorical_accuracy: 0.5646\n",
      "Epoch 89: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6784 - sparse_categorical_accuracy: 0.5646 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 90/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6707 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 90: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6707 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 91/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6732 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 91: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6732 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 92/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6660 - sparse_categorical_accuracy: 0.5979\n",
      "Epoch 92: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6660 - sparse_categorical_accuracy: 0.5979 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 93/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6735 - sparse_categorical_accuracy: 0.5625\n",
      "Epoch 93: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6735 - sparse_categorical_accuracy: 0.5625 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 94/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6837 - sparse_categorical_accuracy: 0.5458\n",
      "Epoch 94: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6837 - sparse_categorical_accuracy: 0.5458 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 95/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6707 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 95: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6707 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 96/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6722 - sparse_categorical_accuracy: 0.5896\n",
      "Epoch 96: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6722 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 97/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6800 - sparse_categorical_accuracy: 0.5646\n",
      "Epoch 97: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6800 - sparse_categorical_accuracy: 0.5646 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 98/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6867 - sparse_categorical_accuracy: 0.5542\n",
      "Epoch 98: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6867 - sparse_categorical_accuracy: 0.5542 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 99/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6761 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 99: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6761 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 100/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6726 - sparse_categorical_accuracy: 0.5792\n",
      "Epoch 100: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6726 - sparse_categorical_accuracy: 0.5792 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 101/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6705 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 101: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6705 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 102/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6769 - sparse_categorical_accuracy: 0.5500\n",
      "Epoch 102: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6769 - sparse_categorical_accuracy: 0.5500 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 103/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6726 - sparse_categorical_accuracy: 0.5646\n",
      "Epoch 103: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6726 - sparse_categorical_accuracy: 0.5646 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 104/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6741 - sparse_categorical_accuracy: 0.5708\n",
      "Epoch 104: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6741 - sparse_categorical_accuracy: 0.5708 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 105/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6665 - sparse_categorical_accuracy: 0.5938\n",
      "Epoch 105: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6665 - sparse_categorical_accuracy: 0.5938 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 106/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6737 - sparse_categorical_accuracy: 0.5708\n",
      "Epoch 106: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6737 - sparse_categorical_accuracy: 0.5708 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 107/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6759 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 107: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6759 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 108/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6716 - sparse_categorical_accuracy: 0.5938\n",
      "Epoch 108: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6716 - sparse_categorical_accuracy: 0.5938 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 109/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6792 - sparse_categorical_accuracy: 0.5625\n",
      "Epoch 109: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 348ms/step - loss: 0.6792 - sparse_categorical_accuracy: 0.5625 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 110/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6754 - sparse_categorical_accuracy: 0.5792\n",
      "Epoch 110: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6754 - sparse_categorical_accuracy: 0.5792 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 111/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6738 - sparse_categorical_accuracy: 0.5542\n",
      "Epoch 111: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6738 - sparse_categorical_accuracy: 0.5542 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 112/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6789 - sparse_categorical_accuracy: 0.5688\n",
      "Epoch 112: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6789 - sparse_categorical_accuracy: 0.5688 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 113/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6667 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 113: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6667 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 114/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6906 - sparse_categorical_accuracy: 0.5125\n",
      "Epoch 114: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 348ms/step - loss: 0.6906 - sparse_categorical_accuracy: 0.5125 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 115/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6780 - sparse_categorical_accuracy: 0.5625\n",
      "Epoch 115: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6780 - sparse_categorical_accuracy: 0.5625 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 116/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6786 - sparse_categorical_accuracy: 0.5708\n",
      "Epoch 116: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6786 - sparse_categorical_accuracy: 0.5708 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 117/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6676 - sparse_categorical_accuracy: 0.5875\n",
      "Epoch 117: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6676 - sparse_categorical_accuracy: 0.5875 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 118/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6644 - sparse_categorical_accuracy: 0.6104\n",
      "Epoch 118: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6644 - sparse_categorical_accuracy: 0.6104 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 119/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6725 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 119: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6725 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 120/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6657 - sparse_categorical_accuracy: 0.5958\n",
      "Epoch 120: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6657 - sparse_categorical_accuracy: 0.5958 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 121/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6655 - sparse_categorical_accuracy: 0.6125\n",
      "Epoch 121: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 348ms/step - loss: 0.6655 - sparse_categorical_accuracy: 0.6125 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 122/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6772 - sparse_categorical_accuracy: 0.5771\n",
      "Epoch 122: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6772 - sparse_categorical_accuracy: 0.5771 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 123/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6740 - sparse_categorical_accuracy: 0.5896\n",
      "Epoch 123: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 348ms/step - loss: 0.6740 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 124/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6739 - sparse_categorical_accuracy: 0.5688\n",
      "Epoch 124: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6739 - sparse_categorical_accuracy: 0.5688 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 125/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6779 - sparse_categorical_accuracy: 0.5604\n",
      "Epoch 125: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6779 - sparse_categorical_accuracy: 0.5604 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 126/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6738 - sparse_categorical_accuracy: 0.5917\n",
      "Epoch 126: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6738 - sparse_categorical_accuracy: 0.5917 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 127/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6695 - sparse_categorical_accuracy: 0.5729\n",
      "Epoch 127: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6695 - sparse_categorical_accuracy: 0.5729 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 128/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6765 - sparse_categorical_accuracy: 0.5792\n",
      "Epoch 128: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6765 - sparse_categorical_accuracy: 0.5792 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 129/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6642 - sparse_categorical_accuracy: 0.5875\n",
      "Epoch 129: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6642 - sparse_categorical_accuracy: 0.5875 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 130/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6729 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 130: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6729 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 131/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6749 - sparse_categorical_accuracy: 0.5604\n",
      "Epoch 131: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6749 - sparse_categorical_accuracy: 0.5604 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 132/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6733 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 132: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6733 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 133/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6735 - sparse_categorical_accuracy: 0.5938\n",
      "Epoch 133: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6735 - sparse_categorical_accuracy: 0.5938 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 134/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6723 - sparse_categorical_accuracy: 0.5938\n",
      "Epoch 134: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6723 - sparse_categorical_accuracy: 0.5938 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 135/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6738 - sparse_categorical_accuracy: 0.5750\n",
      "Epoch 135: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6738 - sparse_categorical_accuracy: 0.5750 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 136/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6711 - sparse_categorical_accuracy: 0.5875\n",
      "Epoch 136: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6711 - sparse_categorical_accuracy: 0.5875 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 137/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6759 - sparse_categorical_accuracy: 0.5729\n",
      "Epoch 137: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 348ms/step - loss: 0.6759 - sparse_categorical_accuracy: 0.5729 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 138/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6671 - sparse_categorical_accuracy: 0.6021\n",
      "Epoch 138: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6671 - sparse_categorical_accuracy: 0.6021 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 139/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6716 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 139: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6716 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 140/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6771 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 140: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 348ms/step - loss: 0.6771 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 141/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6777 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 141: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6777 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 142/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6736 - sparse_categorical_accuracy: 0.5583\n",
      "Epoch 142: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6736 - sparse_categorical_accuracy: 0.5583 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 143/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6806 - sparse_categorical_accuracy: 0.5396\n",
      "Epoch 143: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6806 - sparse_categorical_accuracy: 0.5396 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 144/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6806 - sparse_categorical_accuracy: 0.5333\n",
      "Epoch 144: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6806 - sparse_categorical_accuracy: 0.5333 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 145/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6640 - sparse_categorical_accuracy: 0.6042\n",
      "Epoch 145: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6640 - sparse_categorical_accuracy: 0.6042 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 146/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6777 - sparse_categorical_accuracy: 0.5604\n",
      "Epoch 146: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6777 - sparse_categorical_accuracy: 0.5604 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 147/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6698 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 147: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6698 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 148/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6710 - sparse_categorical_accuracy: 0.6083\n",
      "Epoch 148: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 348ms/step - loss: 0.6710 - sparse_categorical_accuracy: 0.6083 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 149/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6706 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 149: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 348ms/step - loss: 0.6706 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n",
      "Epoch 150/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6796 - sparse_categorical_accuracy: 0.5521\n",
      "Epoch 150: val_loss did not improve from 0.69483\n",
      "30/30 [==============================] - 10s 348ms/step - loss: 0.6796 - sparse_categorical_accuracy: 0.5521 - val_loss: 0.6961 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x252e54b5fd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histories = {model_nno: None, model_obj: None}\n",
    "\n",
    "try:    \n",
    "    model_nno.load_weights(stance_config.MODEL_NNO_CHECKPOINT_FOLDER)\n",
    "    model_obj.load_weights(stance_config.MODEL_OBJ_CHECKPOINT_FOLDER)\n",
    "    print('The weights of the models have already been found. Skipping training.')\n",
    "\n",
    "except:\n",
    "    \n",
    "    for model_type, epochs, batch_size in [(model_nno, stance_config.MODEL_NNO_EPOCHS, stance_config.MODEL_NNO_BATCH_SIZE), (model_obj, stance_config.MODEL_OBJ_EPOCHS, stance_config.MODEL_OBJ_BATCH_SIZE)]:\n",
    "\n",
    "        histories[model_type] = model.fit(\n",
    "            ds_splits[model_type]['x_train'], \n",
    "            ds_splits[model_type]['y_train'], \n",
    "            validation_data = (\n",
    "                ds_splits[model_type]['x_val'],\n",
    "                ds_splits[model_type]['y_val']\n",
    "            ),\n",
    "            epochs = epochs,\n",
    "            callbacks = model_callbacks[model_type],\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "    #Finally, load the best weights obtained during the training:\n",
    "    model_nno.load_weights(stance_config.MODEL_NNO_CHECKPOINT_FOLDER)\n",
    "    model_obj.load_weights(stance_config.MODEL_OBJ_CHECKPOINT_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05685a98-62a8-4c63-ab80-af0cd468cb75",
   "metadata": {},
   "source": [
    "Let us inspect the history of the training of both networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09f9ef8a-468a-4493-a77b-6c439165b9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyEklEQVR4nO3dd3xcZ5n3/881XdKMiiVbtqXYstNMnOIWp0LskH2AbDahhB8xLSYsIZQNEGCBwCah7cPuk90HsoFA6CXgHwsJhGwCIcYiBZLgdLckLootF1m2ujQzmnI9f5wjRZYlq3ikadf79TovzTnnPme+M7auuXWfMqKqGGOMyX+ebAcwxhiTGVbQjTGmQFhBN8aYAmEF3RhjCoQVdGOMKRBW0I0xpkBYQTd5R0TWisijx7H9u0TkwUxmyiQR+baI/Eum25rCJ3YeuhkvEWkC/lFVH8pyjrVujguzmWMkufIemeJkPXRTVETEV8zPbwqbFXRz3EQkKCJfF5F97vR1EQm662pE5D4R6RCRNhF5REQ87rrPiMheEekWkRdF5PWj7L9aRO4VkS4ReRI4cci6BhHRoYVSRBpF5B/dx2tF5DER+b8i0gbcMnzIxt3+OhF5WUTaReSbIiLuOq+I/IeIHBKRXSLy0eHPN2Q/PwXmAb8TkR4R+ech+d4vIruBP7lt/1tEDohIp4g8LCKLh+znRyLyFffxKhFpFpFPishBEdkvIu+bZNtqEfmd+z7+TUS+cjxDVyb3WEE3mfB54FxgCXAWsBL4grvuk0AzMBOoBW4EVEROBT4KnK2qEeANQNMo+/8mEAPmANe400ScA+wEZgFfHaXNZcDZbv7/z80D8AHgTe5rWwa8ebQnUdX3ALuBf1DVsKr++5DVFwGvGbLfB4CT3UxPA3cdI/9soAKoA94PfFNEqibR9ptAr9vmancyBcQKusmEdwFfUtWDqtoKfBF4j7sugVOI56tqQlUfUefATQoIAqeJiF9Vm1R1x/Adi4gXeBtwk6r2quom4McTzLdPVf9LVZOqGh2lzddUtUNVdwMbcAo4OMX9G6rarKrtwNcm+NwDbnHzRwFU9Qeq2q2qceAW4CwRqRhl2wTO+5tQ1fuBHuDUibQd8j7erKp9qrqFib+PJsdZQTeZMBd4Zcj8K+4ygP8DbAceFJGdIvJZAFXdDnwcp5gdFJF1IjKXo80EfMCeYfufiD1jN+HAkMd9QNh9PHfY9uPZ1zEzuMM4XxORHSLSxat/mdSMsu1hVU2Okm+8bUd6Hyf7WkyOsoJuMmEfMH/I/Dx3GW4v9JOquhD4B+CGgbFyVf25e6bKfECBfxth361AEjhh2P4H9Lo/S4csmz1sH8dzKtd+oH7I/AmjNRzjuYYufydwBXAJzvBIg7tcJpFvvAbex4m8FpNnrKCbifKLSGjI5AN+AXxBRGaKSA1wE/AzABG5TEROcg8yduEMtaRE5FQRudg9eBoDou66I6hqCrgb52BmqYicxpCxX3eIZy/wbrfnew1DDppmwC+Bj4lInYhUAp8Zo30LsHCMNhEgDhzG+SD61+MNOZYR3sdFwHun+nnN9LKCbibqfpziOzDdAnwF2Ag8D7yAc5DvK277k4GHcMZy/wp8S1UbccbPvwYcwhnumIVzwHQkH8UZNjgA/Aj44bD1HwA+jVMgFwN/OZ4XOMx3gQdxXtszOK8/yQgfPq7/jfPh1iEinxqlzU9who32AluAxzOY91g+ivMXwQHgpzgfxPFpem4zDezCImMmQETeBHxbVeeP2TjHici/AbNV1c52KRDWQzfmGESkREQuFRGfiNQBNwP3ZDvXZIjIIhE5UxwrcU5rzMvXYkZmBd2YYxOc0zDbcYZctuIcI8hHEZxx9F6cYwP/Afw2q4lMRtmQizHGFAjroRtjTIHI2o2CampqtKGhYVLb9vb2UlZWltlAGWYZM8MyZoZlPH65ku+pp546pKozR1ypqlmZli9frpO1YcOGSW87XSxjZljGzLCMxy9X8gEbdZS6akMuxhhTIKygG2NMgbCCbowxBcK+PcUYc9wSiQTNzc3EYrFJ76OiooKtW7dmMFVmTXe+UChEfX09fr9/3NtYQTfGHLfm5mYikQgNDQ24X/Y0Yd3d3UQikQwny5zpzKeqHD58mObmZhYsWDDu7WzIxRhz3GKxGNXV1ZMu5uZIIkJ1dfWE/+Kxgm6MyQgr5pk1mfcz/wp6yxYW7PwZ9B7OdhJjjMkp+VfQD29n/u7/hq692U5ijMkRhw8fZsmSJSxZsoTZs2dTV1c3ON/f33/MbTdu3Mj1118/5nNccsklmYo7ZfLvoGhJpfMz1pHNFMaYHFJdXc2zzz4LwC233EI4HOZTn3r1+0WSySQ+38jlbsWKFaxYsWLM53jooYcyknUq5V8PvaTK+RntyGoMY0xuW7t2LTfccAOrV6/mM5/5DE8++STnn38+S5cu5fzzz+fFF18EoLGxkcsuuwxwPgyuueYaVq1axcKFC7ntttsG9zdnzpzB9qtWreLKK69k0aJFvOtd70Ldu9bef//9LFq0iAsvvJDrr79+cL/TZcweuoiEgIdxvjLMB/xKVW8e1mYVzn2Vd7mL7lbVL2U06YBQpfPTeujG5KQv/m4zW/Z1TXi7VCqF1+sdcd1pc8u5+R8WT3ifL730Eg899BBer5euri4efvhhfD4fDz30EDfeeCO//vWvj9pm27ZtbNiwge7ubk499VQ+9KEPHXUu+DPPPMPmzZuZO3cuF1xwAY899hgrVqzggx/8IA8//DALFixgzZo1E857vMYz5BIHLlbVHhHxA4+KyAOqOvx7EB9R1an/OBoYcrEeujFmDG9/+9sHPyQ6Ozu5+uqrefnllxEREonEiNv8/d//PcFgkGAwyKxZs2hpaaG+vv6INitXrhxctmTJEpqamgiHwyxcuHDwvPE1a9Zw5513TuGrO9qYBd29u1ePO+t3p+x9K0YgjOJBou1Zi2CMGd1ketIwNRfuDL3d7b/8y7+wevVq7rnnHpqamli1atWI2wSDwcHHXq+XZDI5rjYDwy7ZNK6DoiLiBZ4CTgK+qapPjNDsPBF5DtgHfEpVN4+wn2uBawFqa2tpbGycVOjzfGUc2rmFlye5/XTo6emZ9OubLpYxMyyjc1l8d3f3ce0jlUod9z4A4vE4fr+fRCJBNBod3Ofhw4eZMWMG3d3dfOc730FV6e7upq+vj2QySXd39+C2A9uk02l6enoG54e3B+jv7ycWi1FXV8eOHTvYtGkT8+fP52c/+9kR7SYjFotN6N9tXAVdVVPAEhGpBO4RkdNVddOQJk8D891hmUuB3wAnj7CfO4E7AVasWKGjfUKOpe+JCHVVpdRNcvvpMHDgJJdZxsywjLB169bj7l1nqoc+MFzi9/spKSkZ3OeNN97I1VdfzR133MHFF1+MiBCJRCgtLcXn8xGJRAa3HdjG4/EQDocH54e3BwgEAoRCIWbNmsUdd9zBlVdeSU1NDStXrqSlpeW4XlMoFGLp0qXjbj+h0xZVtUNEGoE3ApuGLO8a8vh+EfmWiNSo6qGJ7H+8kr4yOyhqjBnRLbfcMuLy8847j5deemlw/stf/jIAq1atGvywG77tpk2v9lv3799/VHuA22+/ffDx6tWr2bZtG6rKRz7ykXGdDplJY562KCIz3Z45IlICXAJsG9ZmtrjXqYrISne/U3YpZ9IXtoOixpic893vfpclS5awePFiOjs7+eAHPzitzz+eHvoc4MfuOLoH+KWq3ici1wGo6reBK4EPiUgSiAJX6RQeIUj4wxCzK0WNMbnlE5/4BJ/4xCey9vzjOcvleeCoQRy3kA88vh24fXibqZL0haG7Y7qezhhj8kL+XSmKW9BjnZADpwkZY0yuyMuCnvCHQVMQP/5TnIwxplDkZUFP+sLOA7u4yBhjBuVpQXev/rJTF40xOKcS/uEPfzhi2de//nU+/OEPj9p+48aNAFx66aV0dHQc1eaWW27h1ltvPebz/uY3v2HLli2D8zfddFNW78qYpwV9oIfekdUcxpjcsGbNGtatW3fEsnXr1o3rBln3338/lZWVk3re4QX9S1/6Ulbvm56XBT3hdwu69dCNMcCVV17JfffdRzweB6CpqYl9+/bx85//nBUrVrB48WJuvvnmEbdtaGjg0CHnGsivfvWrnHrqqVxyySWDt9cF5/zyiy66iLPOOou3ve1t9PX18Ze//IV7772XT3/60yxZsoQdO3awdu1afvWrXwGwfv16li5dyhlnnME111wzmK2hoYGbb76ZZcuWccYZZ7Bt27ajQ01S/n3BBdZDNyanPfBZOPDChDcrSSXBO0pJmn0GvOlro25bXV3NypUr+f3vf88VV1zBunXreMc73sHnPvc5ZsyYQSqV4vWvfz3PP/88Z5555oj7eOqpp1i3bh3PPPMMyWSSZcuWsXz5cgDe+ta3ctVVVxGJRPjCF77A97//ff7pn/6Jyy+/nMsuu4wrr7zyiH3FYjHWrl3L+vXrOeWUU3jve9/LHXfcwcc//nEAampqePrpp/nWt77Frbfeyve+970Jv18jycse+mBBtx66McY1dNhlYLjll7/8JcuWLWPp0qVs3rz5iOGR4R555BHe8pa3UFpaSnl5OZdffvnguk2bNvGGN7yBM844g7vuuovNm4+69+ARXnzxRRYsWMApp5wCwNVXX83DDz88uP6tb30rAMuXL6epqWmyL/koedlDT3lD4PFZD92YXHSMnvSxRI/z5lxvfvObueGGG3j66aeJRqNUVVVx66238re//Y2qqirWrl1LLBY75j7cO5gcZe3atdx1112cf/75/OhHPxrzDohjXSg/cPvd0W7PO1l52UNHxPnmIuuhG2Nc4XCYVatWcc0117BmzRq6urooKyujoqKClpYWHnjggWNu/7rXvY577rln8Ja7v/vd7wbXdXd3M3v2bBKJBHfdddfg8kgkMuLtcRctWkRTUxPbt28H4Kc//SkXXXRRhl7p6PKyhw4431xk56EbY4ZYs2YNb33rW1m3bh2LFi1i6dKlLF68mIULF3LBBRccc9tly5bxjne8gyVLljB//nxe+9rXDq778pe/zMUXX0xDQwNnnHHGYBG/6qqr+MAHPsBtt902eDAUnNve/vCHP+Ttb387yWSSs88+m+uuu25qXvRQqpqVafny5TpZGzZsUL3zYtUfXzHpfUy1DRs2ZDvCmCxjZlhG1S1bthz3Prq6ujKQZOpkI99I7yuwUUepq/k55AJOD92GXIwxZlD+FvRQpR0UNcaYIfK3oFsP3Ziconb304yazPuZvwU9VOncQjedznYSY4peKBTi8OHDVtQzRFU5fPgwoVBoQtvl8VkuVaBp6O+GUEW20xhT1Orr62lubqa1tXXS+4jFYhMuYNNpuvOFQiHq6+sntE0eF/RK52e0wwq6MVnm9/tZsGDBce2jsbFxQt9wP91yPR/k4ZDL+q0t3NDYx8GE+0lp4+jGGAPkYUEHaIspXdiXXBhjzFB5V9DLS/wAdOJ+yYWdumiMMUA+FvSQW9DVvrXIGGOGyruCHgk5x3HbUqXOAuuhG2MMkIcFfWDIpS3hc26haz10Y4wB8rCglwW8CNAVS9nl/8YYM8SYBV1EQiLypIg8JyKbReSLI7QREblNRLaLyPMismxq4jo3oC/1Q3cs4VxcZD10Y4wBxndhURy4WFV7RMQPPCoiD6jq40PavAk42Z3OAe5wf06JUp/QFUu690TvmKqnMcaYvDJmD929BW+PO+t3p+E3bLgC+Inb9nGgUkTmZDbqq0r9Qlc0Yd9aZIwxQ4zr0n8R8QJPAScB31TVJ4Y1qQP2DJlvdpftH7afa4FrAWpra8f8Xr7RBCXFnpZDtETilHft54lJ7mcq9fT0TPr1TRfLmBmWMTNyPWOu54NxFnRVTQFLRKQSuEdETlfVTUOajPTNqkfddk1V7wTuBFixYoWuWrVqwoEB/uuZ39MrpdTOPxVeeJ7J7mcqNTY25mSuoSxjZljGzMj1jLmeDyZ4louqdgCNwBuHrWoGThgyXw/sO55gx1Lic4dcSirtFrrGGOMaz1kuM92eOSJSAlwCbBvW7F7gve7ZLucCnaq6nylS6sM5KBqqBBTiXVP1VMYYkzfGM+QyB/ixO47uAX6pqveJyHUAqvpt4H7gUmA70Ae8b4ryAs5B0Z54gnSowvlEinW8ejtdY4wpUmMWdFV9HjjqJsBuIR94rMBHMhttdKU+Z8g+6i13btEV7YCq6Xp2Y4zJTXl3pShAifsx1Otxb6Frpy4aY0x+FvRSv9ND7xy8J3pH9sIYY0yOyM+C7g65dA3cQte+5MIYY/K0oDs3XKRd3Vvo2pCLMcbkaUF3e+jt/T7w+G3IxRhjyNeC7o6hd8VT7sVFHVnNY4wxuSAvC/rAWS7dsYTdE90YY1x5WdA9IkSCPrqiSeuhG2OMKy8LOjjfLdplPXRjjBmUtwW9vMTv3qDLvrXIGGMgnwt6yE+3fWuRMcYMyt+CXjJkyMVuoWuMMflb0CMhv1PQSypxbqHbme1IxhiTVXlb0MtDPmfIJVTpLLBhF2NMkcvfgu4eFNVQhbPADowaY4pc3hb0SMhHWiHqdwu63aDLGFPk8ragl4ecO3T1SMRZ0NeWxTTGGJN9+VvQS5yC3iXlzgLroRtjilzeFvQZZQEAWhIlzgLroRtjilzeFvS6SqeQN3clIFgOUSvoxpjilrcFfXZFCI/A3vaoc/m/9dCNMUUubwu63+uhtjxEc0cUSquh73C2IxljTFblbUEHZ9hlb3sUSmfYkIsxpujld0GvKmFvRxRKZtiQizGm6OV3Qa8s4UBnjHRJlZ22aIwpemMWdBE5QUQ2iMhWEdksIh8boc0qEekUkWfd6aapiXukuqoSkmmlx1MO8S5IJabjaY0xJif5xtEmCXxSVZ8WkQjwlIj8UVW3DGv3iKpelvmIoxs4dbFdw5SDM+wSqZ3OCMYYkzPG7KGr6n5Vfdp93A1sBeqmOth41Fc5Bf1gKuwssAOjxpgiNqExdBFpAJYCT4yw+jwReU5EHhCRxZkIN5a5bg99X79dLWqMMaKq42soEgb+DHxVVe8etq4cSKtqj4hcCnxDVU8eYR/XAtcC1NbWLl+3bt2kQvf09BAOO73yf1rfy1uq9/Cljs+wafFnOTTzvEntM9OGZsxVljEzLGNm5HrGXMm3evXqp1R1xYgrVXXMCfADfwBuGGf7JqDmWG2WL1+uk7Vhw4bBx5fd9oh+/Du/U725XHXjjya9z0wbmjFXWcbMsIyZkesZcyUfsFFHqavjOctFgO8DW1X1P0dpM9tth4isxBnKmZZLN+sqS3ipyz22a2PoxpgiNp6zXC4A3gO8ICLPustuBOYBqOq3gSuBD4lIEogCV7mfJFOurqqExpfSaDCE2OX/xpgiNmZBV9VHARmjze3A7ZkKNRF1lSXEEkq6ogpvn11cZIwpXnl9pSg4PXSA/kClDbkYY4pa3hf02eUhAKK+Cjtt0RhT1PK+oNdEggD0eCLWQzfGFLW8L+jV7lfRdRKxe6IbY4pa3hf0kN9LJOSjTSPOHRfT6WxHMsaYrMj7gg4wMxykNV0GmoZ4Z7bjGGNMVhREQa8JB2npL3Vm7MCoMaZIFUZBjwTYO3CDLvuiC2NMkSqMgh4Osjs6cMdFOzBqjClOBVPQX4m7Bb33UHbDGGNMlhRMQT+sFc5Mb2t2wxhjTJYUSEEP0EeQtDdkBd0YU7QKoqBXh4OAEA9W25CLMaZoFURBnxl2Lv/v81dZD90YU7QKoqDXRJzL/7u9lVbQjTFFqyAKemnAR2nAS4dU2JCLMaZoFURBB+dMl0Na7vTQp+fLkowxJqcUUEEPcCAVgXQCYnY/F2NM8Smggh5kb3/YmbFhF2NMESqcgh4Jsjvu3qDLDowaY4pQ4RT0cJBdMSvoxpjiVTAFfWY4wKG0e/l/nw25GGOKT8EU9JpwkHYizoyNoRtjilDhFPRIkAQ+EoEKG3IxxhSlginosyLO5f8x/wwr6MaYojRmQReRE0Rkg4hsFZHNIvKxEdqIiNwmIttF5HkRWTY1cUc3KxICoNtXaUMuxpiiNJ4eehL4pKq+BjgX+IiInDaszZuAk93pWuCOjKYch5KAl/KQz73833roxpjiM2ZBV9X9qvq0+7gb2ArUDWt2BfATdTwOVIrInIynHcOs8hCt6XIr6MaYojShMXQRaQCWAk8MW1UH7Bky38zRRX/K1ZYHOZAMQ18bpJLT/fTGGJNVvvE2FJEw8Gvg46raNXz1CJscdYcsEbkWZ0iG2tpaGhsbx590iJ6enhG31b44u/qCgPLY+vtIBContf9MGC1jLrGMmWEZMyPXM+Z6PgBUdcwJ8AN/AG4YZf13gDVD5l8E5hxrn8uXL9fJ2rBhw4jL//f9W/Wjn79J9eZy1QObJr3/TBgtYy6xjJlhGTMj1zPmSj5go45SV8dzlosA3we2qup/jtLsXuC97tku5wKdqrr/eD9sJqq2PEhLauDiIhtHN8YUl/EMuVwAvAd4QUSedZfdCMwDUNVvA/cDlwLbgT7gfRlPOg615SEOU+7M9BzMRgRjjMmaMQu6qj7KyGPkQ9so8JFMhZqs2vIgLVrlzHRP+x8IxhiTVQVzpSg4Fxf1UErCWwpdVtCNMcWlsAp6uXP5f09gpvXQjTFFp6AKetDnparUT5u3xgq6MaboFFRBB+fA6EGqbMjFGFN0Cq6gz4wE2ZeqdHro6XS24xhjzLQpuIJeWx6iqb8C0gmItmU7jjHGTJsCLOhBtsfcc9G79mU3jDHGTKMCLOghDqQrnRk7MGqMKSIFV9BnRUIc0BnOjPXQjTFFpOAKem15kFYqUAS6D2Q7jjHGTJuCK+hzK0tI4iMarIZu66EbY4pHwRX0meEgAa+HTm+1nYtujCkqBVfQPR6hrqqEVqm2IRdjTFEpuIIOUF9Vwt5UpQ25GGOKSoEW9FJ2xSPQdxiS8WzHMcaYaVGgBb2EXf0Vzoydi26MKRIFWdBPmFE65IsubBzdGFMcCrKg11eV2MVFxpiiU5AF/YSqUvZptTPTuSe7YYwxZpoUZEGvCQdI+ML0ecuh/ZVsxzHGmGlRkAVdRKivKqHVNxvam7IdxxhjpkVBFnRwDozu1lnQYT10Y0xxKNiCXl9Vwvb+aujYbd9cZIwpCgVb0E+oKmV7ohpS/XYuujGmKBRsQa+vcodcwIZdjDFFYcyCLiI/EJGDIrJplPWrRKRTRJ51p5syH3Pi6qtK2KMznRk708UYUwR842jzI+B24CfHaPOIql6WkUQZ0lBTxj6tQRHEznQxxhSBMXvoqvow0DYNWTKqosTPzMpy2n0zbcjFGFMUMjWGfp6IPCciD4jI4gzt87idNrfcGXaxIRdjTBEQVR27kUgDcJ+qnj7CunIgrao9InIp8A1VPXmU/VwLXAtQW1u7fN26dZMK3dPTQzgcHrPdb7b3c+Er/8VlJZt5/PwfTOq5Jmu8GbPJMmaGZcyMXM+YK/lWr179lKquGHGlqo45AQ3ApnG2bQJqxmq3fPlynawNGzaMq90fNu3X/7jxGk3fXKGaiE36+SZjvBmzyTJmhmXMjFzPmCv5gI06Sl097iEXEZktIuI+XokzjHP4ePebCQNDLoJCh92kyxhT2MY8y0VEfgGsAmpEpBm4GfADqOq3gSuBD4lIEogCV7mfIllXV1lCe2COM9PeBDUnZTWPMcZMpTELuqquGWP97TinNeYcEaG09iRoAdp3ZTuOMcZMqYK9UnTA3PoGOrWMdMuWbEcxxpgpVfAF/bS6CrbpCcT3vpDtKMYYM6UKv6DPLWdreh6+Q1sgN4b2jTFmShR8QT9xZpjtzMef7HVupWuMMQWq4Au63+uhu/JUZ6Zlc3bDGGPMFCr4gg4QqltMGrGCbowpaEVR0BfMrWV3ehb9++zAqDGmcBVFQV80O8I2nUdqvxV0Y0zhKpKCXs42PYFgVxP092U7jjHGTImiKOi15UF2+xbiIQ2t27IdxxhjpkRRFHQRITnzNGemZcRv0jPGmLxXFAUdYEbdyXRoGN39eLajGGPMlCiagn7KnEr+mn4NqR2NdsWoMaYgFU1BXzQnwmPp0/F177U7LxpjClLRFPRTayM8ru44+q6HsxvGGGOmQNEU9LKgjzkLz6RVZqA7/5ztOMYYk3FFU9AB/mFJHY8kTyO54882jm6MKThFVdDfsHg2T+jp+GOH4aB94YUxprAUVUGvKPEjC18HQHrHhiynMcaYzCqqgg5w4YqlbEnPp/ep/z/bUYwxJqOKrqC/flEt98oqIoefh4Nbsx3HGGMypugKeknAS3Lx20iol/hTP8t2HGOMyZiiK+gAl513FhvSS0g/uw5SyWzHMcaYjCjKgn5WfQV/jbyBkvgh2PGnbMcxxpiMKMqCLiI0nPcWWrWc3savZzuOMcZkRFEWdIArljfwnfSbKdv3GGqnMBpjCsCYBV1EfiAiB0VkxBuJi+M2EdkuIs+LyLLMx8y8ytIAM1d/mGatoe23N9qVo8aYvDeeHvqPgDceY/2bgJPd6VrgjuOPNT0+sGoR/1N9DdVdW2h+7OfZjmOMMcdlzIKuqg8DbcdocgXwE3U8DlSKyJxMBZxKHo/wtrWf4CUaKF1/I33tB7IdyRhjJk10HEMNItIA3Keqp4+w7j7ga6r6qDu/HviMqm4coe21OL14amtrl69bt25SoXt6egiHw5PadiQtzTt488v/zAuBJfSc/wUQOe59ZjrjVLCMmWEZMyPXM+ZKvtWrVz+lqitGXKmqY05AA7BplHX/A1w4ZH49sHysfS5fvlwna8OGDZPedtR9/vBfVG8u143//e+Z2d8UZMw0y5gZljEzcj1jruQDNuoodTUTZ7k0AycMma8H9mVgv9PqwnffxLPBsznrhX9l058m95eDMcZkUyYK+r3Ae92zXc4FOlV1fwb2O618Ph8LP/xLdvhO5KQ/f5SXnngg25GMMWZCxnPa4i+AvwKnikiziLxfRK4TkevcJvcDO4HtwHeBD09Z2ilWXjGDGdf+hhbPLBbc/y523v91O53RGJM3fGM1UNU1Y6xX4CMZS5RlM2vrabluPU9/992c8+TN7Gp6FM+l/868+QuQDBwsNcaYqVK0V4oeS23tHE75+P9wV9nVzG3ZQNUPL+COf/sUDz73ysCBX2OMyTlW0EdRFQ7xzk99g5Z3/4nemjP5cOx7nHn3RfzXv36Sz/3iL/z22b0kU+lsxzTGmEFjDrkUMxFh3slnwUm/J7Xjz3jv/xLXt32fnhd/wX9vfi0f+sMbOee8i6irKmV+dRmvmROxYRljTNZYQR8PEbwnrWLm9augeSNlj3+Lq7fcy/uif2DXQ7VsTJ/KT/QUmkoWU91wBn6fj+62OH3V+zm7YQZVpX48IrT2xIn2p2ioKcv2KzLGFCAr6BNVvwK58gdIXxu66W7mbnuQt+z7G2+PPQxJ6NlexmbPKWxMNPDgLxr5rtbSpLV0ecoZGKG56JSZXHfRiRzujbOztZeKEj+zK0Kcu7CaihI/AG29/VSW+PF4rMdvjBkfK+iTVToDWfmPBFf+o3NqY9tO2PME4T1Pcs6eJ1l58DeI99UDqDFvmN6yebQF62jcHebuH8ykVStp1QpatZI2IojXz4r5M9jd1sfejigzygKsbJiB1yt0RRPUV5VyZn0F5SGn6NdVlbBodoSQ35utd8EYk0OsoGeCCFSf6ExL3gnAw3/6IxedMd8p9G07CblTddt2TpLdiD911G76fBUcPFBBPFiNZ/5s9iUibN0dotcTJh0Is2uPh1//LUAPpXRrCV2UEfWUUBYM4PUIHgGPCDPKAiyoKcPv9dDW24/fK9RXldKfTLNpXychv5d3rpxHKKF09iXoiiXY3dZHdyzBzEiQ8pCf3v4UAa/HjgsYk0esoE8R9fhh5inONIykEtDZDD0Hofeg87PnIKW9B2lwH9O7lZN7Wrko0etsFHc3Dg57HoS4lBLzhIl5w0Q9ZfTEg7Tv8hEjAP4QMQ3Q+oqHfgmytLyc1l7hibshpgHu3fBnogSIaYAYzjQwHyfA7Ooqzj15NjsP9dF0uJcZZQFml4dYNr+KpSdUokB/Mk1dVQknVJUS8NmJU8ZkixX0bPD6YcYCZxpLfy/EuiDe7U6dzs9YF8Q6kXgXoVgXoVgnxJ1l9PdCshcSUWdKRsEbg1QcOt39+seZtRdSzwj9EiTlDRLrD9J32E/3ducDI6oB+gmwlQDP4cfvEQIeJUaQXimhNFzJrJoaQqEQafFxsDdFc1eCQCDArIowkdIQwWCQWRVlnFBTjtfnB48PEECJtG2i9yUI+Hz4AyXgC4A3+OpPb2DIY39G7pRpTL6ygp7rAmXORAZuMZ9OQTIGiRgkozzxWCPnLD3TXTak+CdikOgbXO5JRClxH4eTzrp4tI++vh68qSjeVJxUfyeaiJFSSOPBn44RTPcS6opC1yh5xnH7+eUAz4//Jao3iHoDpD1+vP4Q4gs4HxAeH2nxkFQPAf+ry/B43ckPgVIIhJ33KdXvTOkkiOfVNgPbeX2Dj0/cdwD617vL/UP26391mTcAvuCQn8GjP5w8XkglQFNOO2/A3d/AB5W8+hOGLWPIh5kc1d6bjDof9COsO3pfkp0PRtXju9WGfZhbQS8qHu+QDwiIltbDnDPH3GykX5MgR43+jCydpr2jnVg8BukkNaUe/KQgnSIWj9PVG6UvFmf3oU62H+igv7+foCdNwCP4fR4OtbYyv76O7ng/ew910tHVTTQahVScEk8KPwm86QQBkgQkQVk6hSfeT4AEpb40c8o8+EVJpxJ09TkZasM+GmYEifX1E431EfSmKfGkCRHHn+ojLV5S4scXCOLx+ACFVNIp7sOnVII5iTh6AEgnkHRy/P8e0+i1AI9mO8WxrQL4c5ZDHMMqgMYM7eyCj8PffTFDO3uVFXQztTweqmZUj7gq5E7g3HD/dSO0aWxsZNWqVUctV9XBg7XJVJqmw300vniQ5vYop9RGKAt6eWz7IV7Y20V/MoU/6OGcRTOoKPHz/Ud30dvuHJT2e4VEauReYcDn4Yy6CqL9KfZ2RKkOBzhxZpiTZoVZUFPGjtYeHtt+iJcOdNGfgkjQx8WLZnL2/ArqK/zMLQ8wJ+Il7E2T6I/j0wSedD8k42gyTjoZx5vuh2S/MxyWdnvm4oF0wumtD/yloAoM68EesUxHXua237FjOycuXDjiOucxR+9rmu1qamJBQ8PkNp6GW3I0NTXRMNl8w807JzP7GcYKuslLQ8+88Xk9nDTLKbRDXbGkbsRt333ufB7c0sLiueWcWV9JVzTBrsO97Grtpbk9SnmJj4oSP1v3d/Hcnk5qy4MsnVfJoZ44O1p72bDtIMm04vcKy+ZVsbrex9mnn8xLLd38cUsLv31u5LtH14SDXH7WXCKhUn7zbDv7OqLMry5jTsWRZxJ5BMoCPspL/NRXlTBvRikrGqqYU1FCPJniUE8/Po8Q8nspD/nGdRbSnkQjJ164ahzvbPa80tjIghE+vHNFU2MjDTmcD6ygmyI0qzzEu8+dPzhfVRagqizAsnlV49o+kUrT3B5lZiRIOOhz/op47UIA0mmltSfO3o4oe9uj7O2I0tefIuAVNu3t4mePv0Iinea8hdW8cfFsmg730tIVP2L/qbSyp7+Pjr4Eh3v7B5dXlvrpjCaO6Iz6PEJlqR9VSKtSX1XKiTPLqCoLEAn5OX1uOcvnV7GtLcUTv9/G880dvHigmzPrK/nI6hNJKzy0pYWXD/awryNK0O/lxJoyqsMBfF4PB7vibNnfRbQ/yaxy5++pve1RPB5YMX8G5SEff2tqp7c/yVuW1rH61Fm09fbT259kYU2YhTPLCPm9HOqJ8x8PvsT6rS1cubye61adiM8j7OuI0d7XT3cswfb2FBW723loawsbtrWyaHaE/7V4Nj6PcLA7TloVjwjbD/bwXHMHADXhADXhINXhIDPDAarDQYI+D4mUkkoryXSagNdDVVmAQz1xHt95mGh/mvNPrKahppS9HTEOdsXojafweYUV86t4zdxy0mll56Fe7n66meebO1k+v4pgd5KXHt7B7rY+2nsTRBMpTptTzoqGKk6vq6CqNMBj2w+xfmsLQb+XmeEgPq+QSKXZdaiXl1t6WFBTxhsWz+bCk2um5PqRcX2n6FRYsWKFbtx41NeOjstof4bnEsuYGYWWsTOaIJ5MMSsSGrsxEO1PsaO1hyd2tbH9YDe15SFml4dIqRLtT9HW2097XwKvxxl12N3Wx87WXrqiCXr6k0cV/9fMKeekWWEaXzxIe18CgID7F87cyhKiiSS7WnvpiCZIpNJUlgY4bU45kZCPg91x1P3QiCdTPLmrnZ54guXzq/CI8Oj2QyOOfFSV+okn0/Qn06xoqOLxnW3HHOryCCyfX8VLLT10RhNHrQ/5PZw+twK/18Ph3jiHevpp7+sf16hLWcBL0O+lbcgH5bGE/B4Wz61g095O4knnUu/KUj/VZQH8Xg8vH+whlXaeOOjzEE+mKQ14SaV1sP3ANifNDPPigW6640nec+58vvzmo76ieVxEZNTvFLUeujHTyLm1w3jPGYWSgJfT6yo4va5iws8VT6Z4bk8nT+9up/fALq694iIi7lXGvfEkv312H5GQj1WnzhxcPhHO91gyeHuK3Yf72Lyvk1nlIUJ+Dztae2k61MvB7hipNLz/wgWcNCvMpr2d3PPMXmaUBairLKE6HCAc9PHYk0+x8NTFrGioYlYkRCKV5tk9Hfi9HmZGgvg9QjKtzmPvkdc7JFNp2vr6OdTdTzKdxusR/F4PXo8QT6Rp6+0nHPJx+txyPCJs2d9Fa3ecuqoSaiMhyoJeeuJJ/tbUzs7WHgI+DzPKAly8aBaRkJ9YIsUvH2jkikteR0Xpq+9VbzzJc3s62Hqgmz1tfZyzYAarF80i6PPQHU+STisejxAJOkNj/ck0j+88TG35+D7QJ8oKujEFKujzsnLBDFYumEFj454jinZZ0Mc7z5l3XPsXkSPOFJxXXcq86tLB+cVzR/4QGu0DqnOnj1VnvHp6rt/r4eyGGePK4vN6mBUJjfsvn5Gev7I0wN+dVgvUHrUu5Pcyr9x7RDEH5308/6Qazj+p5qhtykf4kAz4PLzulJnjyjgZdlmfMcYUCCvoxhhTIKygG2NMgbCCbowxBcIKujHGFAgr6MYYUyCsoBtjTIGwgm6MMQUia5f+i0gr8MokN68BDmUwzlSwjJlhGTPDMh6/XMk3X1VHvDopawX9eIjIxtHuZZArLGNmWMbMsIzHL9fzgQ25GGNMwbCCbowxBSJfC/qd2Q4wDpYxMyxjZljG45fr+fJzDN0YY8zR8rWHbowxZhgr6MYYUyDyrqCLyBtF5EUR2S4in812HgAROUFENojIVhHZLCIfc5fPEJE/isjL7s/xfWnl1OX0isgzInJfjuarFJFficg29708LwczfsL9N94kIr8QkVC2M4rID0TkoIhsGrJs1Ewi8jn39+dFEXlDFjP+H/ff+nkRuUdEKnMt45B1nxIRFZGaIcumPeNY8qqgi4gX+CbwJuA0YI2InJbdVAAkgU+q6muAc4GPuLk+C6xX1ZOB9e58Nn0M2DpkPtfyfQP4vaouAs7CyZozGUWkDrgeWKGqpwNe4KocyPgj4I3Dlo2Yyf1/eRWw2N3mW+7vVTYy/hE4XVXPBF4CPpeDGRGRE4C/A3YPWZatjMeUVwUdWAlsV9WdqtoPrAOuyHImVHW/qj7tPu7GKUR1ONl+7Db7MfDmrAQERKQe+Hvge0MW51K+cuB1wPcBVLVfVTvIoYwuH1AiIj6gFNhHljOq6sNA27DFo2W6AlinqnFV3QVsx/m9mvaMqvqgqibd2ceB+lzL6Pq/wD8DQ88gyUrGseRbQa8D9gyZb3aX5QwRaQCWAk8Ataq6H5yiD8zKYrSv4/ynTA9Zlkv5FgKtwA/dYaHviUhZLmVU1b3ArTg9tf1Ap6o+mEsZhxgtU67+Dl0DPOA+zpmMInI5sFdVnxu2KmcyDpVvBV1GWJYz512KSBj4NfBxVe3Kdp4BInIZcFBVn8p2lmPwAcuAO1R1KdBL9oeAjuCOQ18BLADmAmUi8u7sppqwnPsdEpHP4wxb3jWwaIRm055RREqBzwM3jbR6hGVZr0X5VtCbgROGzNfj/MmbdSLixynmd6nq3e7iFhGZ466fAxzMUrwLgMtFpAlnmOpiEflZDuUD59+2WVWfcOd/hVPgcynjJcAuVW1V1QRwN3B+jmUcMFqmnPodEpGrgcuAd+mrF8XkSsYTcT68n3N/d+qBp0VkNrmT8Qj5VtD/BpwsIgtEJIBzUOLeLGdCRARn7Herqv7nkFX3Ale7j68Gfjvd2QBU9XOqWq+qDTjv2Z9U9d25kg9AVQ8Ae0TkVHfR64Et5FBGnKGWc0Wk1P03fz3O8ZJcyjhgtEz3AleJSFBEFgAnA09mIR8i8kbgM8Dlqto3ZFVOZFTVF1R1lqo2uL87zcAy9/9qTmQ8iqrm1QRcinNEfAfw+WzncTNdiPPn1vPAs+50KVCNc4bBy+7PGTmQdRVwn/s4p/IBS4CN7vv4G6AqBzN+EdgGbAJ+CgSznRH4Bc6YfgKn6Lz/WJlwhhF2AC8Cb8pixu0449ADvzPfzrWMw9Y3ATXZzDjWZJf+G2NMgci3IRdjjDGjsIJujDEFwgq6McYUCCvoxhhTIKygG2NMgbCCbgqWiKRE5NkhU8auPBWRhpHuymdMNvmyHcCYKRRV1SXZDmHMdLEeuik6ItIkIv8mIk+600nu8vkist69P/d6EZnnLq9179f9nDud7+7KKyLfde+P/qCIlGTtRRmDFXRT2EqGDbm8Y8i6LlVdCdyOcydK3Mc/Uef+3HcBt7nLbwP+rKpn4dxfZrO7/GTgm6q6GOgA3jalr8aYMdiVoqZgiUiPqoZHWN4EXKyqO92bqh1Q1WoROQTMUdWEu3y/qtaISCtQr6rxIftoAP6ozhdIICKfAfyq+pVpeGnGjMh66KZY6SiPR2szkviQxynsmJTJMivopli9Y8jPv7qP/4JzN0qAdwGPuo/XAx+Cwe9lLZ+ukMZMhPUoTCErEZFnh8z/XlUHTl0MisgTOJ2aNe6y64EfiMincb496X3u8o8Bd4rI+3F64h/CuSufMTnFxtBN0XHH0Feo6qFsZzEmk2zIxRhjCoT10I0xpkBYD90YYwqEFXRjjCkQVtCNMaZAWEE3xpgCYQXdGGMKxP8DWZN6POyDZCQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_history(histories[model_nno])\n",
    "display_history(histories[model_obj])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bdade68-e0c7-407a-a121-e0de7acb7274",
   "metadata": {},
   "source": [
    "From the data we can infer the epoch that allowed the model to generalize better by inspecting the loss on data that it had never seen before (data on the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09a32785-11b6-4762-8e83-376d0d7c1cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 currently uses the weights that have been determined on epoch 95.\n",
      "Model 2 currently uses the weights that have been determined on epoch 8.\n"
     ]
    }
   ],
   "source": [
    "models = [model_nno, model_obj]\n",
    "for model_idx in range(len(models)):\n",
    "    if histories[models[model_idx]] is None:\n",
    "        print(f'The data cannot be calculated since the history of the training of the model {model_idx + 1} is not available.')\n",
    "    else:\n",
    "        print(f'Model {model_idx + 1} currently uses the weights that have been determined on epoch {np.argmin(histories[models[model_idx]].history[\"val_loss\"]) + 1}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0834406e-26ba-43b5-bd77-cbcaec5ea551",
   "metadata": {},
   "source": [
    "The following classification reports sum up the performances of the trained networks with respect to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "beedbb20-faa2-4ff8-b44d-d2315794f334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the Model 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.56      0.20      0.29        25\n",
      "           2       0.71      0.94      0.81        66\n",
      "\n",
      "    accuracy                           0.70        96\n",
      "   macro avg       0.42      0.38      0.37        96\n",
      "weighted avg       0.63      0.70      0.63        96\n",
      "\n",
      "Classification report for the Model 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.77      0.68        35\n",
      "           1       0.50      0.32      0.39        25\n",
      "\n",
      "    accuracy                           0.58        60\n",
      "   macro avg       0.56      0.55      0.54        60\n",
      "weighted avg       0.57      0.58      0.56        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models)):\n",
    "    print(f'Classification report for the Model {i+1}:')    \n",
    "    y_true = ds_splits[models[i]]['y_test']\n",
    "    y_pred = np.argmax(models[i].predict(ds_splits[models[i]]['x_test']).logits,axis=-1)\n",
    "    print(classification_report(y_true, y_pred, zero_division = 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "472ca42b-439e-4754-959f-4ee0e2ede325",
   "metadata": {},
   "source": [
    "Once again, given the fact that only few records were available, we compare the performances of the networks with a random classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ec3adf2-3b06-4cab-ac10-89cb639e1708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 216.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation of the expected performances of a uniform random classifier for the task solved by Model 1\n",
      "\tMacro:\n",
      "\t\tPrecision:\n",
      "\t\t\tMean:0.333\n",
      "\t\t\tVar:0.002\n",
      "\t\tRecall\n",
      "\t\t\tMean:0.334\n",
      "\t\t\tVar:0.004\n",
      "\t\tF1-score\n",
      "\t\t\tMean:0.295\n",
      "\t\t\tVar:0.002\n",
      "\tWeighted:\n",
      "\t\tPrecision:\n",
      "\t\t\tMean:0.463\n",
      "\t\t\tVar:0.003\n",
      "\t\tRecall\n",
      "\t\t\tMean:0.333\n",
      "\t\t\tVar:0.002\n",
      "\t\tF1-score\n",
      "\t\t\tMean:0.368\n",
      "\t\t\tVar:0.003\n",
      "Approximation of the expected performances of a uniform random classifier for the task solved by Model 2\n",
      "\tMacro:\n",
      "\t\tPrecision:\n",
      "\t\t\tMean:0.502\n",
      "\t\t\tVar:0.004\n",
      "\t\tRecall\n",
      "\t\t\tMean:0.502\n",
      "\t\t\tVar:0.004\n",
      "\t\tF1-score\n",
      "\t\t\tMean:0.498\n",
      "\t\t\tVar:0.004\n",
      "\tWeighted:\n",
      "\t\tPrecision:\n",
      "\t\t\tMean:0.511\n",
      "\t\t\tVar:0.004\n",
      "\t\tRecall\n",
      "\t\t\tMean:0.502\n",
      "\t\t\tVar:0.004\n",
      "\t\tF1-score\n",
      "\t\t\tMean:0.503\n",
      "\t\t\tVar:0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random_experiment([model_nno, model_obj], [ds_splits[model_nno]['y_test'], ds_splits[model_obj]['y_test']], [{'min': 0, 'max': 2}, {'min': 0, 'max': 1}], 1000, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d06d82f5-676b-4b3c-a7f0-8484098a3607",
   "metadata": {},
   "source": [
    "By comparing the averages we can be sure of the fact that that the models perform ever so slightly better than a random classifier. The performances make sense considering that only few records were available for training and by recalling the distribution of the classes in the provided dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53793491",
   "metadata": {},
   "source": [
    "### Models with weighted classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8d69f80-84f3-4121-a6c1-bfbc5b5b9675",
   "metadata": {},
   "source": [
    "Recall the first histogram of the notebook: it showed that the distribution of the classes is not uniform. We'd like to take into account this unfortunate property of the dataset and to correct it by way of re-weighting the under-represented classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9edd7006-3d83-481d-86b2-1ad58789ecfb",
   "metadata": {},
   "source": [
    "Defining some copies of the previous models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92399a26-608f-4097-9399-6538271b8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nno_weighted = TFDistilBertForSequenceClassification.from_pretrained(\"typeform/distilbert-base-uncased-mnli\", num_labels=3)\n",
    "model_obj_weighted = TFDistilBertForSequenceClassification.from_pretrained(\"typeform/distilbert-base-uncased-mnli\", num_labels=2, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49d2cfc8-0147-449e-94bd-1a17ddaf4e46",
   "metadata": {},
   "source": [
    "Retrieving the previous train, val, test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "376524ae-55e0-4690-9c03-ea015ca9b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_splits[model_nno_weighted] = ds_splits[model_nno]\n",
    "ds_splits[model_obj_weighted] = ds_splits[model_obj]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a228cf7-6703-4785-b07c-ecc8b853fcb1",
   "metadata": {
    "id": "285e9106-c344-4596-a31c-19c66b368d42"
   },
   "source": [
    "Compiling the networks to prepare them for training (fine-tuning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "513fffed-e6bf-4f08-8a72-0fe057735ba3",
   "metadata": {
    "id": "2502a93f-3f5d-4d4f-a0c8-6222c2ad168c"
   },
   "outputs": [],
   "source": [
    "for model in [model_nno_weighted, model_obj_weighted]:\n",
    "    model.distilbert.trainable = False\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate = stance_config.INIT_LR),\n",
    "        metrics=keras.metrics.SparseCategoricalAccuracy()\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "745a8033-4698-4ca7-91dc-6f4bc5db5900",
   "metadata": {
    "id": "415e181f-1144-4c2c-9293-0a9df8c7822a"
   },
   "source": [
    "Now configuring the callbacks for the training of the networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80213619-df60-4a97-b08c-9a8f9a2fc098",
   "metadata": {
    "id": "9c2d27c0-6505-4d4c-9088-4422728dd6fc"
   },
   "outputs": [],
   "source": [
    "model_nno_weighted_checkpoint = keras.callbacks.ModelCheckpoint(stance_config.MODEL_NNO_WEIGHTED_CHECKPOINT_FOLDER, save_best_only=True, save_weights_only=True, verbose=1, mode=\"min\", monitor=\"val_loss\")\n",
    "model_obj_weighted_checkpoint = keras.callbacks.ModelCheckpoint(stance_config.MODEL_OBJ_WEIGHTED_CHECKPOINT_FOLDER, save_best_only=True, save_weights_only=True, verbose=1, mode=\"min\", monitor=\"val_loss\")\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', factor=.1, patience=5, min_lr=stance_config.MIN_LR)\n",
    "\n",
    "model_weighted_callbacks = {model_nno_weighted: [model_nno_weighted_checkpoint, reduce_lr], model_obj_weighted: [model_obj_weighted_checkpoint, reduce_lr]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20bd618c-53a9-4f09-a96a-386e97f64f75",
   "metadata": {
    "id": "d6cc39e2-20f7-4e99-b02b-5d2e65b9a95d"
   },
   "source": [
    "Ensuring that the folders needed to store the best weights of the networks exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3e358ab-056e-4c6b-b2f8-8a4ac4d471a9",
   "metadata": {
    "id": "01b35863-029a-4e59-8393-225561b904cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint folders created (if not existent). If the training has been already done, please copy the corresponding weights in the following folders:\n",
      "C:\\Users\\Andrea\\Script Python\\model_nno_checkpoint_weighted\n",
      "C:\\Users\\Andrea\\Script Python\\model_obj_checkpoint_weighted\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(stance_config.MODEL_NNO_WEIGHTED_CHECKPOINT_FOLDER, exist_ok = True)\n",
    "os.makedirs(stance_config.MODEL_OBJ_WEIGHTED_CHECKPOINT_FOLDER, exist_ok = True)\n",
    "\n",
    "print('Model checkpoint folders created (if not existent). If the training has been already done, please copy the corresponding weights in the following folders:')\n",
    "print(os.path.abspath(stance_config.MODEL_NNO_WEIGHTED_CHECKPOINT_FOLDER))\n",
    "print(os.path.abspath(stance_config.MODEL_OBJ_WEIGHTED_CHECKPOINT_FOLDER))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "986f0747",
   "metadata": {},
   "source": [
    "#### Training and evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d827d9c6-0c6f-416f-9b52-79634b446456",
   "metadata": {},
   "source": [
    "Starting the training of the networks (or loading the weights if available).\n",
    "\n",
    "This time, the `class_weight` parameter has been set. Each class is assigned a weight that is inversely proportional to the fraction of records of the dataset having a certain class. By doing so, the under-represented classes will have more weight than the other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0c36912-2f7a-4fb1-9cc0-62aa9046ba0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d4ed6fb-0a89-4c18-9ba1-f06206efa109",
    "outputId": "2e9adfd4-362f-4a36-8f68-eefe484744c8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 5.1182 - sparse_categorical_accuracy: 0.3338\n",
      "Epoch 1: val_loss improved from inf to 3.65844, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 24s 388ms/step - loss: 5.1182 - sparse_categorical_accuracy: 0.3338 - val_loss: 3.6584 - val_sparse_categorical_accuracy: 0.2812 - lr: 1.0000e-05\n",
      "Epoch 2/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 4.4595 - sparse_categorical_accuracy: 0.3469\n",
      "Epoch 2: val_loss improved from 3.65844 to 3.28605, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 18s 371ms/step - loss: 4.4595 - sparse_categorical_accuracy: 0.3469 - val_loss: 3.2861 - val_sparse_categorical_accuracy: 0.2812 - lr: 1.0000e-05\n",
      "Epoch 3/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 4.2239 - sparse_categorical_accuracy: 0.3377\n",
      "Epoch 3: val_loss improved from 3.28605 to 2.92811, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 18s 366ms/step - loss: 4.2239 - sparse_categorical_accuracy: 0.3377 - val_loss: 2.9281 - val_sparse_categorical_accuracy: 0.2812 - lr: 1.0000e-05\n",
      "Epoch 4/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 3.7894 - sparse_categorical_accuracy: 0.3194\n",
      "Epoch 4: val_loss improved from 2.92811 to 2.61193, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 18s 368ms/step - loss: 3.7894 - sparse_categorical_accuracy: 0.3194 - val_loss: 2.6119 - val_sparse_categorical_accuracy: 0.2812 - lr: 1.0000e-05\n",
      "Epoch 5/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 3.4630 - sparse_categorical_accuracy: 0.3495\n",
      "Epoch 5: val_loss improved from 2.61193 to 2.32641, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 18s 369ms/step - loss: 3.4630 - sparse_categorical_accuracy: 0.3495 - val_loss: 2.3264 - val_sparse_categorical_accuracy: 0.2812 - lr: 1.0000e-05\n",
      "Epoch 6/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 3.0523 - sparse_categorical_accuracy: 0.3429\n",
      "Epoch 6: val_loss improved from 2.32641 to 2.08967, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 393ms/step - loss: 3.0523 - sparse_categorical_accuracy: 0.3429 - val_loss: 2.0897 - val_sparse_categorical_accuracy: 0.2812 - lr: 1.0000e-05\n",
      "Epoch 7/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 2.9798 - sparse_categorical_accuracy: 0.3390\n",
      "Epoch 7: val_loss improved from 2.08967 to 1.87947, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 20s 425ms/step - loss: 2.9798 - sparse_categorical_accuracy: 0.3390 - val_loss: 1.8795 - val_sparse_categorical_accuracy: 0.2917 - lr: 1.0000e-05\n",
      "Epoch 8/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 2.7061 - sparse_categorical_accuracy: 0.3469\n",
      "Epoch 8: val_loss improved from 1.87947 to 1.69744, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 21s 430ms/step - loss: 2.7061 - sparse_categorical_accuracy: 0.3469 - val_loss: 1.6974 - val_sparse_categorical_accuracy: 0.3021 - lr: 1.0000e-05\n",
      "Epoch 9/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 2.5043 - sparse_categorical_accuracy: 0.3272\n",
      "Epoch 9: val_loss improved from 1.69744 to 1.54885, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 20s 422ms/step - loss: 2.5043 - sparse_categorical_accuracy: 0.3272 - val_loss: 1.5489 - val_sparse_categorical_accuracy: 0.3021 - lr: 1.0000e-05\n",
      "Epoch 10/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 2.3140 - sparse_categorical_accuracy: 0.3521\n",
      "Epoch 10: val_loss improved from 1.54885 to 1.42632, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 400ms/step - loss: 2.3140 - sparse_categorical_accuracy: 0.3521 - val_loss: 1.4263 - val_sparse_categorical_accuracy: 0.3021 - lr: 1.0000e-05\n",
      "Epoch 11/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 2.2295 - sparse_categorical_accuracy: 0.3429\n",
      "Epoch 11: val_loss improved from 1.42632 to 1.33270, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 398ms/step - loss: 2.2295 - sparse_categorical_accuracy: 0.3429 - val_loss: 1.3327 - val_sparse_categorical_accuracy: 0.3021 - lr: 1.0000e-05\n",
      "Epoch 12/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 2.1165 - sparse_categorical_accuracy: 0.3639\n",
      "Epoch 12: val_loss improved from 1.33270 to 1.25544, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 400ms/step - loss: 2.1165 - sparse_categorical_accuracy: 0.3639 - val_loss: 1.2554 - val_sparse_categorical_accuracy: 0.3021 - lr: 1.0000e-05\n",
      "Epoch 13/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 2.0170 - sparse_categorical_accuracy: 0.3835\n",
      "Epoch 13: val_loss improved from 1.25544 to 1.19872, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 395ms/step - loss: 2.0170 - sparse_categorical_accuracy: 0.3835 - val_loss: 1.1987 - val_sparse_categorical_accuracy: 0.3542 - lr: 1.0000e-05\n",
      "Epoch 14/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.9146 - sparse_categorical_accuracy: 0.3848\n",
      "Epoch 14: val_loss improved from 1.19872 to 1.15857, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 397ms/step - loss: 1.9146 - sparse_categorical_accuracy: 0.3848 - val_loss: 1.1586 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-05\n",
      "Epoch 15/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.8599 - sparse_categorical_accuracy: 0.3901\n",
      "Epoch 15: val_loss improved from 1.15857 to 1.13223, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 403ms/step - loss: 1.8599 - sparse_categorical_accuracy: 0.3901 - val_loss: 1.1322 - val_sparse_categorical_accuracy: 0.3750 - lr: 1.0000e-05\n",
      "Epoch 16/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.7986 - sparse_categorical_accuracy: 0.4346\n",
      "Epoch 16: val_loss improved from 1.13223 to 1.11621, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 394ms/step - loss: 1.7986 - sparse_categorical_accuracy: 0.4346 - val_loss: 1.1162 - val_sparse_categorical_accuracy: 0.4167 - lr: 1.0000e-05\n",
      "Epoch 17/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.7604 - sparse_categorical_accuracy: 0.4306\n",
      "Epoch 17: val_loss improved from 1.11621 to 1.10165, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 396ms/step - loss: 1.7604 - sparse_categorical_accuracy: 0.4306 - val_loss: 1.1016 - val_sparse_categorical_accuracy: 0.4688 - lr: 1.0000e-05\n",
      "Epoch 18/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.7302 - sparse_categorical_accuracy: 0.4529\n",
      "Epoch 18: val_loss improved from 1.10165 to 1.08766, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 400ms/step - loss: 1.7302 - sparse_categorical_accuracy: 0.4529 - val_loss: 1.0877 - val_sparse_categorical_accuracy: 0.4792 - lr: 1.0000e-05\n",
      "Epoch 19/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.6890 - sparse_categorical_accuracy: 0.4699\n",
      "Epoch 19: val_loss improved from 1.08766 to 1.08183, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 400ms/step - loss: 1.6890 - sparse_categorical_accuracy: 0.4699 - val_loss: 1.0818 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 20/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.6923 - sparse_categorical_accuracy: 0.4712\n",
      "Epoch 20: val_loss improved from 1.08183 to 1.07870, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 400ms/step - loss: 1.6923 - sparse_categorical_accuracy: 0.4712 - val_loss: 1.0787 - val_sparse_categorical_accuracy: 0.4896 - lr: 1.0000e-05\n",
      "Epoch 21/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.6675 - sparse_categorical_accuracy: 0.4647\n",
      "Epoch 21: val_loss improved from 1.07870 to 1.07221, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 20s 412ms/step - loss: 1.6675 - sparse_categorical_accuracy: 0.4647 - val_loss: 1.0722 - val_sparse_categorical_accuracy: 0.4792 - lr: 1.0000e-05\n",
      "Epoch 22/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.6365 - sparse_categorical_accuracy: 0.4725\n",
      "Epoch 22: val_loss improved from 1.07221 to 1.06741, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 399ms/step - loss: 1.6365 - sparse_categorical_accuracy: 0.4725 - val_loss: 1.0674 - val_sparse_categorical_accuracy: 0.4792 - lr: 1.0000e-05\n",
      "Epoch 23/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.6291 - sparse_categorical_accuracy: 0.4882\n",
      "Epoch 23: val_loss improved from 1.06741 to 1.06534, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 398ms/step - loss: 1.6291 - sparse_categorical_accuracy: 0.4882 - val_loss: 1.0653 - val_sparse_categorical_accuracy: 0.4896 - lr: 1.0000e-05\n",
      "Epoch 24/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.6314 - sparse_categorical_accuracy: 0.4607\n",
      "Epoch 24: val_loss improved from 1.06534 to 1.06493, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 20s 407ms/step - loss: 1.6314 - sparse_categorical_accuracy: 0.4607 - val_loss: 1.0649 - val_sparse_categorical_accuracy: 0.4583 - lr: 1.0000e-05\n",
      "Epoch 25/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.6019 - sparse_categorical_accuracy: 0.4843\n",
      "Epoch 25: val_loss improved from 1.06493 to 1.06144, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 397ms/step - loss: 1.6019 - sparse_categorical_accuracy: 0.4843 - val_loss: 1.0614 - val_sparse_categorical_accuracy: 0.4479 - lr: 1.0000e-05\n",
      "Epoch 26/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.6031 - sparse_categorical_accuracy: 0.4738\n",
      "Epoch 26: val_loss did not improve from 1.06144\n",
      "48/48 [==============================] - 18s 375ms/step - loss: 1.6031 - sparse_categorical_accuracy: 0.4738 - val_loss: 1.0665 - val_sparse_categorical_accuracy: 0.4688 - lr: 1.0000e-05\n",
      "Epoch 27/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.6034 - sparse_categorical_accuracy: 0.4935\n",
      "Epoch 27: val_loss improved from 1.06144 to 1.05518, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 401ms/step - loss: 1.6034 - sparse_categorical_accuracy: 0.4935 - val_loss: 1.0552 - val_sparse_categorical_accuracy: 0.4792 - lr: 1.0000e-05\n",
      "Epoch 28/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5833 - sparse_categorical_accuracy: 0.5170\n",
      "Epoch 28: val_loss did not improve from 1.05518\n",
      "48/48 [==============================] - 18s 378ms/step - loss: 1.5833 - sparse_categorical_accuracy: 0.5170 - val_loss: 1.0573 - val_sparse_categorical_accuracy: 0.4583 - lr: 1.0000e-05\n",
      "Epoch 29/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5905 - sparse_categorical_accuracy: 0.4542\n",
      "Epoch 29: val_loss did not improve from 1.05518\n",
      "48/48 [==============================] - 18s 376ms/step - loss: 1.5905 - sparse_categorical_accuracy: 0.4542 - val_loss: 1.0619 - val_sparse_categorical_accuracy: 0.4688 - lr: 1.0000e-05\n",
      "Epoch 30/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5692 - sparse_categorical_accuracy: 0.4620\n",
      "Epoch 30: val_loss improved from 1.05518 to 1.05287, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 406ms/step - loss: 1.5692 - sparse_categorical_accuracy: 0.4620 - val_loss: 1.0529 - val_sparse_categorical_accuracy: 0.4792 - lr: 1.0000e-05\n",
      "Epoch 31/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5718 - sparse_categorical_accuracy: 0.4686\n",
      "Epoch 31: val_loss did not improve from 1.05287\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.5718 - sparse_categorical_accuracy: 0.4686 - val_loss: 1.0585 - val_sparse_categorical_accuracy: 0.4896 - lr: 1.0000e-05\n",
      "Epoch 32/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5651 - sparse_categorical_accuracy: 0.4594\n",
      "Epoch 32: val_loss did not improve from 1.05287\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.5651 - sparse_categorical_accuracy: 0.4594 - val_loss: 1.0554 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 33/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5589 - sparse_categorical_accuracy: 0.4764\n",
      "Epoch 33: val_loss improved from 1.05287 to 1.04720, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 397ms/step - loss: 1.5589 - sparse_categorical_accuracy: 0.4764 - val_loss: 1.0472 - val_sparse_categorical_accuracy: 0.4896 - lr: 1.0000e-05\n",
      "Epoch 34/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5550 - sparse_categorical_accuracy: 0.4699\n",
      "Epoch 34: val_loss did not improve from 1.04720\n",
      "48/48 [==============================] - 18s 379ms/step - loss: 1.5550 - sparse_categorical_accuracy: 0.4699 - val_loss: 1.0511 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 35/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5523 - sparse_categorical_accuracy: 0.4895\n",
      "Epoch 35: val_loss did not improve from 1.04720\n",
      "48/48 [==============================] - 19s 391ms/step - loss: 1.5523 - sparse_categorical_accuracy: 0.4895 - val_loss: 1.0505 - val_sparse_categorical_accuracy: 0.4896 - lr: 1.0000e-05\n",
      "Epoch 36/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5495 - sparse_categorical_accuracy: 0.4817\n",
      "Epoch 36: val_loss improved from 1.04720 to 1.04043, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 398ms/step - loss: 1.5495 - sparse_categorical_accuracy: 0.4817 - val_loss: 1.0404 - val_sparse_categorical_accuracy: 0.4896 - lr: 1.0000e-05\n",
      "Epoch 37/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5378 - sparse_categorical_accuracy: 0.4974\n",
      "Epoch 37: val_loss did not improve from 1.04043\n",
      "48/48 [==============================] - 18s 382ms/step - loss: 1.5378 - sparse_categorical_accuracy: 0.4974 - val_loss: 1.0405 - val_sparse_categorical_accuracy: 0.4896 - lr: 1.0000e-05\n",
      "Epoch 38/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5450 - sparse_categorical_accuracy: 0.4961\n",
      "Epoch 38: val_loss improved from 1.04043 to 1.03888, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 400ms/step - loss: 1.5450 - sparse_categorical_accuracy: 0.4961 - val_loss: 1.0389 - val_sparse_categorical_accuracy: 0.4896 - lr: 1.0000e-05\n",
      "Epoch 39/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5179 - sparse_categorical_accuracy: 0.4529\n",
      "Epoch 39: val_loss did not improve from 1.03888\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.5179 - sparse_categorical_accuracy: 0.4529 - val_loss: 1.0408 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 40/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5437 - sparse_categorical_accuracy: 0.4830\n",
      "Epoch 40: val_loss improved from 1.03888 to 1.03373, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 402ms/step - loss: 1.5437 - sparse_categorical_accuracy: 0.4830 - val_loss: 1.0337 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 41/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5147 - sparse_categorical_accuracy: 0.5065\n",
      "Epoch 41: val_loss did not improve from 1.03373\n",
      "48/48 [==============================] - 18s 376ms/step - loss: 1.5147 - sparse_categorical_accuracy: 0.5065 - val_loss: 1.0342 - val_sparse_categorical_accuracy: 0.5104 - lr: 1.0000e-05\n",
      "Epoch 42/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5129 - sparse_categorical_accuracy: 0.4830\n",
      "Epoch 42: val_loss improved from 1.03373 to 1.03249, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 398ms/step - loss: 1.5129 - sparse_categorical_accuracy: 0.4830 - val_loss: 1.0325 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 43/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5099 - sparse_categorical_accuracy: 0.5065\n",
      "Epoch 43: val_loss improved from 1.03249 to 1.02965, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 403ms/step - loss: 1.5099 - sparse_categorical_accuracy: 0.5065 - val_loss: 1.0296 - val_sparse_categorical_accuracy: 0.5208 - lr: 1.0000e-05\n",
      "Epoch 44/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5169 - sparse_categorical_accuracy: 0.5052\n",
      "Epoch 44: val_loss improved from 1.02965 to 1.02446, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 395ms/step - loss: 1.5169 - sparse_categorical_accuracy: 0.5052 - val_loss: 1.0245 - val_sparse_categorical_accuracy: 0.5104 - lr: 1.0000e-05\n",
      "Epoch 45/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.5188 - sparse_categorical_accuracy: 0.4882\n",
      "Epoch 45: val_loss did not improve from 1.02446\n",
      "48/48 [==============================] - 18s 376ms/step - loss: 1.5188 - sparse_categorical_accuracy: 0.4882 - val_loss: 1.0248 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 46/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4930 - sparse_categorical_accuracy: 0.5275\n",
      "Epoch 46: val_loss improved from 1.02446 to 1.02042, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 397ms/step - loss: 1.4930 - sparse_categorical_accuracy: 0.5275 - val_loss: 1.0204 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 47/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4950 - sparse_categorical_accuracy: 0.4987\n",
      "Epoch 47: val_loss did not improve from 1.02042\n",
      "48/48 [==============================] - 18s 382ms/step - loss: 1.4950 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.0207 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 48/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4963 - sparse_categorical_accuracy: 0.4908\n",
      "Epoch 48: val_loss improved from 1.02042 to 1.01787, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 400ms/step - loss: 1.4963 - sparse_categorical_accuracy: 0.4908 - val_loss: 1.0179 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 49/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4835 - sparse_categorical_accuracy: 0.5353\n",
      "Epoch 49: val_loss improved from 1.01787 to 1.01521, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 404ms/step - loss: 1.4835 - sparse_categorical_accuracy: 0.5353 - val_loss: 1.0152 - val_sparse_categorical_accuracy: 0.5208 - lr: 1.0000e-05\n",
      "Epoch 50/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4792 - sparse_categorical_accuracy: 0.5249\n",
      "Epoch 50: val_loss improved from 1.01521 to 1.01444, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 403ms/step - loss: 1.4792 - sparse_categorical_accuracy: 0.5249 - val_loss: 1.0144 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 51/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4724 - sparse_categorical_accuracy: 0.5288\n",
      "Epoch 51: val_loss improved from 1.01444 to 1.00971, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 400ms/step - loss: 1.4724 - sparse_categorical_accuracy: 0.5288 - val_loss: 1.0097 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 52/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4624 - sparse_categorical_accuracy: 0.5249\n",
      "Epoch 52: val_loss did not improve from 1.00971\n",
      "48/48 [==============================] - 18s 379ms/step - loss: 1.4624 - sparse_categorical_accuracy: 0.5249 - val_loss: 1.0121 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 53/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4731 - sparse_categorical_accuracy: 0.5079\n",
      "Epoch 53: val_loss did not improve from 1.00971\n",
      "48/48 [==============================] - 18s 382ms/step - loss: 1.4731 - sparse_categorical_accuracy: 0.5079 - val_loss: 1.0115 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 54/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4493 - sparse_categorical_accuracy: 0.5170\n",
      "Epoch 54: val_loss improved from 1.00971 to 1.00599, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 397ms/step - loss: 1.4493 - sparse_categorical_accuracy: 0.5170 - val_loss: 1.0060 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 55/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4477 - sparse_categorical_accuracy: 0.5209\n",
      "Epoch 55: val_loss improved from 1.00599 to 1.00096, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 398ms/step - loss: 1.4477 - sparse_categorical_accuracy: 0.5209 - val_loss: 1.0010 - val_sparse_categorical_accuracy: 0.5104 - lr: 1.0000e-05\n",
      "Epoch 56/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4544 - sparse_categorical_accuracy: 0.5105\n",
      "Epoch 56: val_loss did not improve from 1.00096\n",
      "48/48 [==============================] - 18s 381ms/step - loss: 1.4544 - sparse_categorical_accuracy: 0.5105 - val_loss: 1.0020 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 57/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4484 - sparse_categorical_accuracy: 0.5406\n",
      "Epoch 57: val_loss improved from 1.00096 to 0.99543, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 398ms/step - loss: 1.4484 - sparse_categorical_accuracy: 0.5406 - val_loss: 0.9954 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 58/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4437 - sparse_categorical_accuracy: 0.5131\n",
      "Epoch 58: val_loss did not improve from 0.99543\n",
      "48/48 [==============================] - 18s 378ms/step - loss: 1.4437 - sparse_categorical_accuracy: 0.5131 - val_loss: 0.9977 - val_sparse_categorical_accuracy: 0.5104 - lr: 1.0000e-05\n",
      "Epoch 59/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4232 - sparse_categorical_accuracy: 0.5537\n",
      "Epoch 59: val_loss did not improve from 0.99543\n",
      "48/48 [==============================] - 18s 381ms/step - loss: 1.4232 - sparse_categorical_accuracy: 0.5537 - val_loss: 0.9958 - val_sparse_categorical_accuracy: 0.5104 - lr: 1.0000e-05\n",
      "Epoch 60/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4356 - sparse_categorical_accuracy: 0.5314\n",
      "Epoch 60: val_loss improved from 0.99543 to 0.99310, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 398ms/step - loss: 1.4356 - sparse_categorical_accuracy: 0.5314 - val_loss: 0.9931 - val_sparse_categorical_accuracy: 0.5208 - lr: 1.0000e-05\n",
      "Epoch 61/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4263 - sparse_categorical_accuracy: 0.5353\n",
      "Epoch 61: val_loss did not improve from 0.99310\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.4263 - sparse_categorical_accuracy: 0.5353 - val_loss: 0.9968 - val_sparse_categorical_accuracy: 0.5104 - lr: 1.0000e-05\n",
      "Epoch 62/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4336 - sparse_categorical_accuracy: 0.5432\n",
      "Epoch 62: val_loss did not improve from 0.99310\n",
      "48/48 [==============================] - 18s 380ms/step - loss: 1.4336 - sparse_categorical_accuracy: 0.5432 - val_loss: 0.9974 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 63/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4281 - sparse_categorical_accuracy: 0.5550\n",
      "Epoch 63: val_loss did not improve from 0.99310\n",
      "48/48 [==============================] - 18s 384ms/step - loss: 1.4281 - sparse_categorical_accuracy: 0.5550 - val_loss: 0.9990 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 64/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4132 - sparse_categorical_accuracy: 0.5602\n",
      "Epoch 64: val_loss improved from 0.99310 to 0.99072, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 395ms/step - loss: 1.4132 - sparse_categorical_accuracy: 0.5602 - val_loss: 0.9907 - val_sparse_categorical_accuracy: 0.5104 - lr: 1.0000e-05\n",
      "Epoch 65/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4190 - sparse_categorical_accuracy: 0.5353\n",
      "Epoch 65: val_loss did not improve from 0.99072\n",
      "48/48 [==============================] - 18s 379ms/step - loss: 1.4190 - sparse_categorical_accuracy: 0.5353 - val_loss: 0.9938 - val_sparse_categorical_accuracy: 0.5104 - lr: 1.0000e-05\n",
      "Epoch 66/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4054 - sparse_categorical_accuracy: 0.5223\n",
      "Epoch 66: val_loss improved from 0.99072 to 0.99022, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 403ms/step - loss: 1.4054 - sparse_categorical_accuracy: 0.5223 - val_loss: 0.9902 - val_sparse_categorical_accuracy: 0.5104 - lr: 1.0000e-05\n",
      "Epoch 67/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4006 - sparse_categorical_accuracy: 0.5615\n",
      "Epoch 67: val_loss improved from 0.99022 to 0.98454, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 21s 429ms/step - loss: 1.4006 - sparse_categorical_accuracy: 0.5615 - val_loss: 0.9845 - val_sparse_categorical_accuracy: 0.5104 - lr: 1.0000e-05\n",
      "Epoch 68/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3944 - sparse_categorical_accuracy: 0.5301\n",
      "Epoch 68: val_loss did not improve from 0.98454\n",
      "48/48 [==============================] - 19s 394ms/step - loss: 1.3944 - sparse_categorical_accuracy: 0.5301 - val_loss: 0.9903 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 69/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3802 - sparse_categorical_accuracy: 0.5641\n",
      "Epoch 69: val_loss improved from 0.98454 to 0.98257, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 20s 408ms/step - loss: 1.3802 - sparse_categorical_accuracy: 0.5641 - val_loss: 0.9826 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 70/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3778 - sparse_categorical_accuracy: 0.5798\n",
      "Epoch 70: val_loss improved from 0.98257 to 0.97636, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 401ms/step - loss: 1.3778 - sparse_categorical_accuracy: 0.5798 - val_loss: 0.9764 - val_sparse_categorical_accuracy: 0.5104 - lr: 1.0000e-05\n",
      "Epoch 71/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3959 - sparse_categorical_accuracy: 0.5497\n",
      "Epoch 71: val_loss did not improve from 0.97636\n",
      "48/48 [==============================] - 18s 379ms/step - loss: 1.3959 - sparse_categorical_accuracy: 0.5497 - val_loss: 0.9795 - val_sparse_categorical_accuracy: 0.4896 - lr: 1.0000e-05\n",
      "Epoch 72/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3666 - sparse_categorical_accuracy: 0.5772\n",
      "Epoch 72: val_loss did not improve from 0.97636\n",
      "48/48 [==============================] - 18s 383ms/step - loss: 1.3666 - sparse_categorical_accuracy: 0.5772 - val_loss: 0.9815 - val_sparse_categorical_accuracy: 0.4792 - lr: 1.0000e-05\n",
      "Epoch 73/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3807 - sparse_categorical_accuracy: 0.5602\n",
      "Epoch 73: val_loss did not improve from 0.97636\n",
      "48/48 [==============================] - 18s 382ms/step - loss: 1.3807 - sparse_categorical_accuracy: 0.5602 - val_loss: 0.9804 - val_sparse_categorical_accuracy: 0.4792 - lr: 1.0000e-05\n",
      "Epoch 74/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3720 - sparse_categorical_accuracy: 0.5654\n",
      "Epoch 74: val_loss improved from 0.97636 to 0.97342, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 399ms/step - loss: 1.3720 - sparse_categorical_accuracy: 0.5654 - val_loss: 0.9734 - val_sparse_categorical_accuracy: 0.5104 - lr: 1.0000e-05\n",
      "Epoch 75/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3725 - sparse_categorical_accuracy: 0.5654\n",
      "Epoch 75: val_loss improved from 0.97342 to 0.96822, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 405ms/step - loss: 1.3725 - sparse_categorical_accuracy: 0.5654 - val_loss: 0.9682 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 76/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3678 - sparse_categorical_accuracy: 0.5615\n",
      "Epoch 76: val_loss did not improve from 0.96822\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.3678 - sparse_categorical_accuracy: 0.5615 - val_loss: 0.9706 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 77/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3572 - sparse_categorical_accuracy: 0.5563\n",
      "Epoch 77: val_loss did not improve from 0.96822\n",
      "48/48 [==============================] - 18s 380ms/step - loss: 1.3572 - sparse_categorical_accuracy: 0.5563 - val_loss: 0.9718 - val_sparse_categorical_accuracy: 0.5208 - lr: 1.0000e-05\n",
      "Epoch 78/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3567 - sparse_categorical_accuracy: 0.5785\n",
      "Epoch 78: val_loss improved from 0.96822 to 0.96551, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 402ms/step - loss: 1.3567 - sparse_categorical_accuracy: 0.5785 - val_loss: 0.9655 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 79/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3405 - sparse_categorical_accuracy: 0.5589\n",
      "Epoch 79: val_loss improved from 0.96551 to 0.96208, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 399ms/step - loss: 1.3405 - sparse_categorical_accuracy: 0.5589 - val_loss: 0.9621 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 80/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3479 - sparse_categorical_accuracy: 0.5877\n",
      "Epoch 80: val_loss did not improve from 0.96208\n",
      "48/48 [==============================] - 18s 378ms/step - loss: 1.3479 - sparse_categorical_accuracy: 0.5877 - val_loss: 0.9655 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 81/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3460 - sparse_categorical_accuracy: 0.5576\n",
      "Epoch 81: val_loss did not improve from 0.96208\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.3460 - sparse_categorical_accuracy: 0.5576 - val_loss: 0.9639 - val_sparse_categorical_accuracy: 0.5208 - lr: 1.0000e-05\n",
      "Epoch 82/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3459 - sparse_categorical_accuracy: 0.5524\n",
      "Epoch 82: val_loss did not improve from 0.96208\n",
      "48/48 [==============================] - 18s 380ms/step - loss: 1.3459 - sparse_categorical_accuracy: 0.5524 - val_loss: 0.9621 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 83/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3412 - sparse_categorical_accuracy: 0.5785\n",
      "Epoch 83: val_loss did not improve from 0.96208\n",
      "48/48 [==============================] - 18s 379ms/step - loss: 1.3412 - sparse_categorical_accuracy: 0.5785 - val_loss: 0.9623 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 84/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3244 - sparse_categorical_accuracy: 0.5785\n",
      "Epoch 84: val_loss improved from 0.96208 to 0.95780, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 404ms/step - loss: 1.3244 - sparse_categorical_accuracy: 0.5785 - val_loss: 0.9578 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 85/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3261 - sparse_categorical_accuracy: 0.5903\n",
      "Epoch 85: val_loss did not improve from 0.95780\n",
      "48/48 [==============================] - 18s 382ms/step - loss: 1.3261 - sparse_categorical_accuracy: 0.5903 - val_loss: 0.9617 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 86/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3189 - sparse_categorical_accuracy: 0.5812\n",
      "Epoch 86: val_loss improved from 0.95780 to 0.95525, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 400ms/step - loss: 1.3189 - sparse_categorical_accuracy: 0.5812 - val_loss: 0.9552 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 87/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2980 - sparse_categorical_accuracy: 0.5877\n",
      "Epoch 87: val_loss improved from 0.95525 to 0.95176, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 397ms/step - loss: 1.2980 - sparse_categorical_accuracy: 0.5877 - val_loss: 0.9518 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 88/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3230 - sparse_categorical_accuracy: 0.5785\n",
      "Epoch 88: val_loss improved from 0.95176 to 0.94648, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 402ms/step - loss: 1.3230 - sparse_categorical_accuracy: 0.5785 - val_loss: 0.9465 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 89/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3071 - sparse_categorical_accuracy: 0.5825\n",
      "Epoch 89: val_loss did not improve from 0.94648\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.3071 - sparse_categorical_accuracy: 0.5825 - val_loss: 0.9505 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 90/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3163 - sparse_categorical_accuracy: 0.5916\n",
      "Epoch 90: val_loss did not improve from 0.94648\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.3163 - sparse_categorical_accuracy: 0.5916 - val_loss: 0.9484 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 91/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2959 - sparse_categorical_accuracy: 0.5903\n",
      "Epoch 91: val_loss did not improve from 0.94648\n",
      "48/48 [==============================] - 18s 378ms/step - loss: 1.2959 - sparse_categorical_accuracy: 0.5903 - val_loss: 0.9481 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 92/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3113 - sparse_categorical_accuracy: 0.5877\n",
      "Epoch 92: val_loss improved from 0.94648 to 0.94574, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 398ms/step - loss: 1.3113 - sparse_categorical_accuracy: 0.5877 - val_loss: 0.9457 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 93/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2873 - sparse_categorical_accuracy: 0.5877\n",
      "Epoch 93: val_loss did not improve from 0.94574\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.2873 - sparse_categorical_accuracy: 0.5877 - val_loss: 0.9493 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 94/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2907 - sparse_categorical_accuracy: 0.5942\n",
      "Epoch 94: val_loss improved from 0.94574 to 0.94240, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 399ms/step - loss: 1.2907 - sparse_categorical_accuracy: 0.5942 - val_loss: 0.9424 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 95/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2799 - sparse_categorical_accuracy: 0.5812\n",
      "Epoch 95: val_loss improved from 0.94240 to 0.93868, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 404ms/step - loss: 1.2799 - sparse_categorical_accuracy: 0.5812 - val_loss: 0.9387 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 96/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2785 - sparse_categorical_accuracy: 0.5942\n",
      "Epoch 96: val_loss did not improve from 0.93868\n",
      "48/48 [==============================] - 18s 380ms/step - loss: 1.2785 - sparse_categorical_accuracy: 0.5942 - val_loss: 0.9428 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 97/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2785 - sparse_categorical_accuracy: 0.6204\n",
      "Epoch 97: val_loss improved from 0.93868 to 0.93834, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 400ms/step - loss: 1.2785 - sparse_categorical_accuracy: 0.6204 - val_loss: 0.9383 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 98/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2825 - sparse_categorical_accuracy: 0.5942\n",
      "Epoch 98: val_loss did not improve from 0.93834\n",
      "48/48 [==============================] - 18s 383ms/step - loss: 1.2825 - sparse_categorical_accuracy: 0.5942 - val_loss: 0.9445 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 99/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2670 - sparse_categorical_accuracy: 0.6034\n",
      "Epoch 99: val_loss improved from 0.93834 to 0.93767, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 400ms/step - loss: 1.2670 - sparse_categorical_accuracy: 0.6034 - val_loss: 0.9377 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 100/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2628 - sparse_categorical_accuracy: 0.5982\n",
      "Epoch 100: val_loss improved from 0.93767 to 0.93569, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 399ms/step - loss: 1.2628 - sparse_categorical_accuracy: 0.5982 - val_loss: 0.9357 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 101/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2644 - sparse_categorical_accuracy: 0.6139\n",
      "Epoch 101: val_loss improved from 0.93569 to 0.93246, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 406ms/step - loss: 1.2644 - sparse_categorical_accuracy: 0.6139 - val_loss: 0.9325 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 102/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2523 - sparse_categorical_accuracy: 0.6008\n",
      "Epoch 102: val_loss did not improve from 0.93246\n",
      "48/48 [==============================] - 18s 378ms/step - loss: 1.2523 - sparse_categorical_accuracy: 0.6008 - val_loss: 0.9331 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 103/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2805 - sparse_categorical_accuracy: 0.5903\n",
      "Epoch 103: val_loss improved from 0.93246 to 0.92826, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 397ms/step - loss: 1.2805 - sparse_categorical_accuracy: 0.5903 - val_loss: 0.9283 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 104/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2550 - sparse_categorical_accuracy: 0.5982\n",
      "Epoch 104: val_loss did not improve from 0.92826\n",
      "48/48 [==============================] - 18s 381ms/step - loss: 1.2550 - sparse_categorical_accuracy: 0.5982 - val_loss: 0.9288 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 105/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2468 - sparse_categorical_accuracy: 0.6060\n",
      "Epoch 105: val_loss improved from 0.92826 to 0.92520, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 398ms/step - loss: 1.2468 - sparse_categorical_accuracy: 0.6060 - val_loss: 0.9252 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 106/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2482 - sparse_categorical_accuracy: 0.5851\n",
      "Epoch 106: val_loss did not improve from 0.92520\n",
      "48/48 [==============================] - 18s 376ms/step - loss: 1.2482 - sparse_categorical_accuracy: 0.5851 - val_loss: 0.9263 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 107/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2453 - sparse_categorical_accuracy: 0.6139\n",
      "Epoch 107: val_loss improved from 0.92520 to 0.92468, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 20s 410ms/step - loss: 1.2453 - sparse_categorical_accuracy: 0.6139 - val_loss: 0.9247 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 108/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2465 - sparse_categorical_accuracy: 0.6034\n",
      "Epoch 108: val_loss improved from 0.92468 to 0.92021, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 399ms/step - loss: 1.2465 - sparse_categorical_accuracy: 0.6034 - val_loss: 0.9202 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 109/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2489 - sparse_categorical_accuracy: 0.6152\n",
      "Epoch 109: val_loss did not improve from 0.92021\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.2489 - sparse_categorical_accuracy: 0.6152 - val_loss: 0.9285 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 110/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2370 - sparse_categorical_accuracy: 0.6139\n",
      "Epoch 110: val_loss improved from 0.92021 to 0.91834, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 397ms/step - loss: 1.2370 - sparse_categorical_accuracy: 0.6139 - val_loss: 0.9183 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 111/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2454 - sparse_categorical_accuracy: 0.6086\n",
      "Epoch 111: val_loss did not improve from 0.91834\n",
      "48/48 [==============================] - 18s 382ms/step - loss: 1.2454 - sparse_categorical_accuracy: 0.6086 - val_loss: 0.9197 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 112/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2192 - sparse_categorical_accuracy: 0.6217\n",
      "Epoch 112: val_loss did not improve from 0.91834\n",
      "48/48 [==============================] - 18s 379ms/step - loss: 1.2192 - sparse_categorical_accuracy: 0.6217 - val_loss: 0.9210 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 113/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2557 - sparse_categorical_accuracy: 0.5982\n",
      "Epoch 113: val_loss did not improve from 0.91834\n",
      "48/48 [==============================] - 18s 376ms/step - loss: 1.2557 - sparse_categorical_accuracy: 0.5982 - val_loss: 0.9257 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 114/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2116 - sparse_categorical_accuracy: 0.6270\n",
      "Epoch 114: val_loss improved from 0.91834 to 0.91766, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 404ms/step - loss: 1.2116 - sparse_categorical_accuracy: 0.6270 - val_loss: 0.9177 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 115/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2071 - sparse_categorical_accuracy: 0.6309\n",
      "Epoch 115: val_loss did not improve from 0.91766\n",
      "48/48 [==============================] - 18s 379ms/step - loss: 1.2071 - sparse_categorical_accuracy: 0.6309 - val_loss: 0.9197 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 116/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2214 - sparse_categorical_accuracy: 0.6165\n",
      "Epoch 116: val_loss did not improve from 0.91766\n",
      "48/48 [==============================] - 18s 378ms/step - loss: 1.2214 - sparse_categorical_accuracy: 0.6165 - val_loss: 0.9220 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 117/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1948 - sparse_categorical_accuracy: 0.6283\n",
      "Epoch 117: val_loss improved from 0.91766 to 0.91182, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 403ms/step - loss: 1.1948 - sparse_categorical_accuracy: 0.6283 - val_loss: 0.9118 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 118/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.2300 - sparse_categorical_accuracy: 0.6217\n",
      "Epoch 118: val_loss improved from 0.91182 to 0.91148, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 403ms/step - loss: 1.2300 - sparse_categorical_accuracy: 0.6217 - val_loss: 0.9115 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 119/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1815 - sparse_categorical_accuracy: 0.6401\n",
      "Epoch 119: val_loss did not improve from 0.91148\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.1815 - sparse_categorical_accuracy: 0.6401 - val_loss: 0.9121 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 120/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1905 - sparse_categorical_accuracy: 0.6243\n",
      "Epoch 120: val_loss improved from 0.91148 to 0.90804, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 406ms/step - loss: 1.1905 - sparse_categorical_accuracy: 0.6243 - val_loss: 0.9080 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 121/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1772 - sparse_categorical_accuracy: 0.6335\n",
      "Epoch 121: val_loss improved from 0.90804 to 0.90722, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 400ms/step - loss: 1.1772 - sparse_categorical_accuracy: 0.6335 - val_loss: 0.9072 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 122/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1767 - sparse_categorical_accuracy: 0.6152\n",
      "Epoch 122: val_loss improved from 0.90722 to 0.90390, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 398ms/step - loss: 1.1767 - sparse_categorical_accuracy: 0.6152 - val_loss: 0.9039 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 123/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1932 - sparse_categorical_accuracy: 0.6139\n",
      "Epoch 123: val_loss did not improve from 0.90390\n",
      "48/48 [==============================] - 18s 379ms/step - loss: 1.1932 - sparse_categorical_accuracy: 0.6139 - val_loss: 0.9118 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 124/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1761 - sparse_categorical_accuracy: 0.6387\n",
      "Epoch 124: val_loss improved from 0.90390 to 0.89977, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 402ms/step - loss: 1.1761 - sparse_categorical_accuracy: 0.6387 - val_loss: 0.8998 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 125/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1751 - sparse_categorical_accuracy: 0.6387\n",
      "Epoch 125: val_loss improved from 0.89977 to 0.89769, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 403ms/step - loss: 1.1751 - sparse_categorical_accuracy: 0.6387 - val_loss: 0.8977 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 126/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1780 - sparse_categorical_accuracy: 0.6361\n",
      "Epoch 126: val_loss did not improve from 0.89769\n",
      "48/48 [==============================] - 18s 379ms/step - loss: 1.1780 - sparse_categorical_accuracy: 0.6361 - val_loss: 0.8985 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 127/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1897 - sparse_categorical_accuracy: 0.6191\n",
      "Epoch 127: val_loss did not improve from 0.89769\n",
      "48/48 [==============================] - 18s 381ms/step - loss: 1.1897 - sparse_categorical_accuracy: 0.6191 - val_loss: 0.8988 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 128/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1864 - sparse_categorical_accuracy: 0.6361\n",
      "Epoch 128: val_loss improved from 0.89769 to 0.89715, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 399ms/step - loss: 1.1864 - sparse_categorical_accuracy: 0.6361 - val_loss: 0.8971 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 129/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1596 - sparse_categorical_accuracy: 0.6531\n",
      "Epoch 129: val_loss did not improve from 0.89715\n",
      "48/48 [==============================] - 18s 378ms/step - loss: 1.1596 - sparse_categorical_accuracy: 0.6531 - val_loss: 0.8978 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 130/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1717 - sparse_categorical_accuracy: 0.6387\n",
      "Epoch 130: val_loss improved from 0.89715 to 0.89463, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 404ms/step - loss: 1.1717 - sparse_categorical_accuracy: 0.6387 - val_loss: 0.8946 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 131/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1503 - sparse_categorical_accuracy: 0.6335\n",
      "Epoch 131: val_loss did not improve from 0.89463\n",
      "48/48 [==============================] - 18s 382ms/step - loss: 1.1503 - sparse_categorical_accuracy: 0.6335 - val_loss: 0.8958 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 132/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1502 - sparse_categorical_accuracy: 0.6518\n",
      "Epoch 132: val_loss did not improve from 0.89463\n",
      "48/48 [==============================] - 18s 380ms/step - loss: 1.1502 - sparse_categorical_accuracy: 0.6518 - val_loss: 0.9001 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 133/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1429 - sparse_categorical_accuracy: 0.6309\n",
      "Epoch 133: val_loss did not improve from 0.89463\n",
      "48/48 [==============================] - 18s 383ms/step - loss: 1.1429 - sparse_categorical_accuracy: 0.6309 - val_loss: 0.8957 - val_sparse_categorical_accuracy: 0.5312 - lr: 1.0000e-05\n",
      "Epoch 134/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1529 - sparse_categorical_accuracy: 0.6453\n",
      "Epoch 134: val_loss improved from 0.89463 to 0.89349, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 403ms/step - loss: 1.1529 - sparse_categorical_accuracy: 0.6453 - val_loss: 0.8935 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 135/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1414 - sparse_categorical_accuracy: 0.6545\n",
      "Epoch 135: val_loss did not improve from 0.89349\n",
      "48/48 [==============================] - 18s 379ms/step - loss: 1.1414 - sparse_categorical_accuracy: 0.6545 - val_loss: 0.9010 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 136/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1465 - sparse_categorical_accuracy: 0.6479\n",
      "Epoch 136: val_loss did not improve from 0.89349\n",
      "48/48 [==============================] - 18s 382ms/step - loss: 1.1465 - sparse_categorical_accuracy: 0.6479 - val_loss: 0.8951 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 137/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1458 - sparse_categorical_accuracy: 0.6322\n",
      "Epoch 137: val_loss improved from 0.89349 to 0.88696, saving model to ./model_nno_checkpoint_weighted\\\n",
      "48/48 [==============================] - 19s 398ms/step - loss: 1.1458 - sparse_categorical_accuracy: 0.6322 - val_loss: 0.8870 - val_sparse_categorical_accuracy: 0.5417 - lr: 1.0000e-05\n",
      "Epoch 138/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1218 - sparse_categorical_accuracy: 0.6636\n",
      "Epoch 138: val_loss did not improve from 0.88696\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.1218 - sparse_categorical_accuracy: 0.6636 - val_loss: 0.8915 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 139/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1198 - sparse_categorical_accuracy: 0.6767\n",
      "Epoch 139: val_loss did not improve from 0.88696\n",
      "48/48 [==============================] - 18s 378ms/step - loss: 1.1198 - sparse_categorical_accuracy: 0.6767 - val_loss: 0.8963 - val_sparse_categorical_accuracy: 0.5521 - lr: 1.0000e-05\n",
      "Epoch 140/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1446 - sparse_categorical_accuracy: 0.6584\n",
      "Epoch 140: val_loss did not improve from 0.88696\n",
      "48/48 [==============================] - 18s 382ms/step - loss: 1.1446 - sparse_categorical_accuracy: 0.6584 - val_loss: 0.8987 - val_sparse_categorical_accuracy: 0.5625 - lr: 1.0000e-05\n",
      "Epoch 141/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1428 - sparse_categorical_accuracy: 0.6335\n",
      "Epoch 141: val_loss did not improve from 0.88696\n",
      "48/48 [==============================] - 18s 379ms/step - loss: 1.1428 - sparse_categorical_accuracy: 0.6335 - val_loss: 0.9035 - val_sparse_categorical_accuracy: 0.5833 - lr: 1.0000e-05\n",
      "Epoch 142/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1412 - sparse_categorical_accuracy: 0.6427\n",
      "Epoch 142: val_loss did not improve from 0.88696\n",
      "48/48 [==============================] - 18s 376ms/step - loss: 1.1412 - sparse_categorical_accuracy: 0.6427 - val_loss: 0.8955 - val_sparse_categorical_accuracy: 0.5625 - lr: 1.0000e-05\n",
      "Epoch 143/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1284 - sparse_categorical_accuracy: 0.6505\n",
      "Epoch 143: val_loss did not improve from 0.88696\n",
      "48/48 [==============================] - 18s 381ms/step - loss: 1.1284 - sparse_categorical_accuracy: 0.6505 - val_loss: 0.8955 - val_sparse_categorical_accuracy: 0.5625 - lr: 1.0000e-06\n",
      "Epoch 144/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1033 - sparse_categorical_accuracy: 0.6531\n",
      "Epoch 144: val_loss did not improve from 0.88696\n",
      "48/48 [==============================] - 18s 378ms/step - loss: 1.1033 - sparse_categorical_accuracy: 0.6531 - val_loss: 0.8951 - val_sparse_categorical_accuracy: 0.5625 - lr: 1.0000e-06\n",
      "Epoch 145/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1015 - sparse_categorical_accuracy: 0.6675\n",
      "Epoch 145: val_loss did not improve from 0.88696\n",
      "48/48 [==============================] - 18s 376ms/step - loss: 1.1015 - sparse_categorical_accuracy: 0.6675 - val_loss: 0.8950 - val_sparse_categorical_accuracy: 0.5625 - lr: 1.0000e-06\n",
      "Epoch 146/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1196 - sparse_categorical_accuracy: 0.6505\n",
      "Epoch 146: val_loss did not improve from 0.88696\n",
      "48/48 [==============================] - 18s 382ms/step - loss: 1.1196 - sparse_categorical_accuracy: 0.6505 - val_loss: 0.8942 - val_sparse_categorical_accuracy: 0.5625 - lr: 1.0000e-06\n",
      "Epoch 147/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1126 - sparse_categorical_accuracy: 0.6414\n",
      "Epoch 147: val_loss did not improve from 0.88696\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.1126 - sparse_categorical_accuracy: 0.6414 - val_loss: 0.8933 - val_sparse_categorical_accuracy: 0.5625 - lr: 1.0000e-06\n",
      "Epoch 148/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1206 - sparse_categorical_accuracy: 0.6610\n",
      "Epoch 148: val_loss did not improve from 0.88696\n",
      "48/48 [==============================] - 18s 379ms/step - loss: 1.1206 - sparse_categorical_accuracy: 0.6610 - val_loss: 0.8933 - val_sparse_categorical_accuracy: 0.5625 - lr: 1.0000e-07\n",
      "Epoch 149/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1148 - sparse_categorical_accuracy: 0.6584\n",
      "Epoch 149: val_loss did not improve from 0.88696\n",
      "48/48 [==============================] - 18s 377ms/step - loss: 1.1148 - sparse_categorical_accuracy: 0.6584 - val_loss: 0.8933 - val_sparse_categorical_accuracy: 0.5625 - lr: 1.0000e-07\n",
      "Epoch 150/150\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1123 - sparse_categorical_accuracy: 0.6649\n",
      "Epoch 150: val_loss did not improve from 0.88696\n",
      "48/48 [==============================] - 18s 380ms/step - loss: 1.1123 - sparse_categorical_accuracy: 0.6649 - val_loss: 0.8932 - val_sparse_categorical_accuracy: 0.5625 - lr: 1.0000e-07\n",
      "Epoch 1/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.7058 - sparse_categorical_accuracy: 0.4979\n",
      "Epoch 1: val_loss improved from inf to 0.68944, saving model to ./model_obj_checkpoint_weighted\\\n",
      "30/30 [==============================] - 18s 453ms/step - loss: 0.7058 - sparse_categorical_accuracy: 0.4979 - val_loss: 0.6894 - val_sparse_categorical_accuracy: 0.5500 - lr: 1.0000e-05\n",
      "Epoch 2/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6986 - sparse_categorical_accuracy: 0.5396\n",
      "Epoch 2: val_loss did not improve from 0.68944\n",
      "30/30 [==============================] - 11s 355ms/step - loss: 0.6986 - sparse_categorical_accuracy: 0.5396 - val_loss: 0.6904 - val_sparse_categorical_accuracy: 0.5667 - lr: 1.0000e-05\n",
      "Epoch 3/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6998 - sparse_categorical_accuracy: 0.5271\n",
      "Epoch 3: val_loss did not improve from 0.68944\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6998 - sparse_categorical_accuracy: 0.5271 - val_loss: 0.6907 - val_sparse_categorical_accuracy: 0.5667 - lr: 1.0000e-05\n",
      "Epoch 4/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6828 - sparse_categorical_accuracy: 0.5562\n",
      "Epoch 4: val_loss did not improve from 0.68944\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6828 - sparse_categorical_accuracy: 0.5562 - val_loss: 0.6912 - val_sparse_categorical_accuracy: 0.5667 - lr: 1.0000e-05\n",
      "Epoch 5/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6975 - sparse_categorical_accuracy: 0.5250\n",
      "Epoch 5: val_loss improved from 0.68944 to 0.68896, saving model to ./model_obj_checkpoint_weighted\\\n",
      "30/30 [==============================] - 12s 388ms/step - loss: 0.6975 - sparse_categorical_accuracy: 0.5250 - val_loss: 0.6890 - val_sparse_categorical_accuracy: 0.6333 - lr: 1.0000e-05\n",
      "Epoch 6/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6930 - sparse_categorical_accuracy: 0.5167\n",
      "Epoch 6: val_loss improved from 0.68896 to 0.68895, saving model to ./model_obj_checkpoint_weighted\\\n",
      "30/30 [==============================] - 12s 389ms/step - loss: 0.6930 - sparse_categorical_accuracy: 0.5167 - val_loss: 0.6890 - val_sparse_categorical_accuracy: 0.5833 - lr: 1.0000e-05\n",
      "Epoch 7/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6885 - sparse_categorical_accuracy: 0.5562\n",
      "Epoch 7: val_loss improved from 0.68895 to 0.68834, saving model to ./model_obj_checkpoint_weighted\\\n",
      "30/30 [==============================] - 12s 390ms/step - loss: 0.6885 - sparse_categorical_accuracy: 0.5562 - val_loss: 0.6883 - val_sparse_categorical_accuracy: 0.6167 - lr: 1.0000e-05\n",
      "Epoch 8/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6874 - sparse_categorical_accuracy: 0.5458\n",
      "Epoch 8: val_loss improved from 0.68834 to 0.68752, saving model to ./model_obj_checkpoint_weighted\\\n",
      "30/30 [==============================] - 12s 388ms/step - loss: 0.6874 - sparse_categorical_accuracy: 0.5458 - val_loss: 0.6875 - val_sparse_categorical_accuracy: 0.6000 - lr: 1.0000e-05\n",
      "Epoch 9/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6849 - sparse_categorical_accuracy: 0.5479\n",
      "Epoch 9: val_loss did not improve from 0.68752\n",
      "30/30 [==============================] - 11s 359ms/step - loss: 0.6849 - sparse_categorical_accuracy: 0.5479 - val_loss: 0.6878 - val_sparse_categorical_accuracy: 0.6167 - lr: 1.0000e-05\n",
      "Epoch 10/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6955 - sparse_categorical_accuracy: 0.5125\n",
      "Epoch 10: val_loss improved from 0.68752 to 0.68696, saving model to ./model_obj_checkpoint_weighted\\\n",
      "30/30 [==============================] - 12s 390ms/step - loss: 0.6955 - sparse_categorical_accuracy: 0.5125 - val_loss: 0.6870 - val_sparse_categorical_accuracy: 0.6167 - lr: 1.0000e-05\n",
      "Epoch 11/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6891 - sparse_categorical_accuracy: 0.5625\n",
      "Epoch 11: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 362ms/step - loss: 0.6891 - sparse_categorical_accuracy: 0.5625 - val_loss: 0.6874 - val_sparse_categorical_accuracy: 0.5833 - lr: 1.0000e-05\n",
      "Epoch 12/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6873 - sparse_categorical_accuracy: 0.5479\n",
      "Epoch 12: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 355ms/step - loss: 0.6873 - sparse_categorical_accuracy: 0.5479 - val_loss: 0.6878 - val_sparse_categorical_accuracy: 0.5333 - lr: 1.0000e-05\n",
      "Epoch 13/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6814 - sparse_categorical_accuracy: 0.5354\n",
      "Epoch 13: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 355ms/step - loss: 0.6814 - sparse_categorical_accuracy: 0.5354 - val_loss: 0.6885 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-05\n",
      "Epoch 14/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6769 - sparse_categorical_accuracy: 0.6042\n",
      "Epoch 14: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 361ms/step - loss: 0.6769 - sparse_categorical_accuracy: 0.6042 - val_loss: 0.6881 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 15/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6750 - sparse_categorical_accuracy: 0.5938\n",
      "Epoch 15: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6750 - sparse_categorical_accuracy: 0.5938 - val_loss: 0.6883 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-05\n",
      "Epoch 16/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6691 - sparse_categorical_accuracy: 0.5958\n",
      "Epoch 16: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6691 - sparse_categorical_accuracy: 0.5958 - val_loss: 0.6883 - val_sparse_categorical_accuracy: 0.5000 - lr: 1.0000e-06\n",
      "Epoch 17/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6718 - sparse_categorical_accuracy: 0.5917\n",
      "Epoch 17: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6718 - sparse_categorical_accuracy: 0.5917 - val_loss: 0.6885 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-06\n",
      "Epoch 18/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6845 - sparse_categorical_accuracy: 0.5688\n",
      "Epoch 18: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 355ms/step - loss: 0.6845 - sparse_categorical_accuracy: 0.5688 - val_loss: 0.6889 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-06\n",
      "Epoch 19/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6821 - sparse_categorical_accuracy: 0.5646\n",
      "Epoch 19: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6821 - sparse_categorical_accuracy: 0.5646 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4833 - lr: 1.0000e-06\n",
      "Epoch 20/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6723 - sparse_categorical_accuracy: 0.6042\n",
      "Epoch 20: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6723 - sparse_categorical_accuracy: 0.6042 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-06\n",
      "Epoch 21/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6770 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 21: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6770 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-07\n",
      "Epoch 22/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6699 - sparse_categorical_accuracy: 0.5917\n",
      "Epoch 22: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6699 - sparse_categorical_accuracy: 0.5917 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-07\n",
      "Epoch 23/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6755 - sparse_categorical_accuracy: 0.5771\n",
      "Epoch 23: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6755 - sparse_categorical_accuracy: 0.5771 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-07\n",
      "Epoch 24/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6787 - sparse_categorical_accuracy: 0.5625\n",
      "Epoch 24: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6787 - sparse_categorical_accuracy: 0.5625 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-07\n",
      "Epoch 25/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6736 - sparse_categorical_accuracy: 0.6125\n",
      "Epoch 25: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 364ms/step - loss: 0.6736 - sparse_categorical_accuracy: 0.6125 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-07\n",
      "Epoch 26/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6741 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 26: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 365ms/step - loss: 0.6741 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 27/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6813 - sparse_categorical_accuracy: 0.5750\n",
      "Epoch 27: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6813 - sparse_categorical_accuracy: 0.5750 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 28/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6739 - sparse_categorical_accuracy: 0.6146\n",
      "Epoch 28: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6739 - sparse_categorical_accuracy: 0.6146 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 29/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6752 - sparse_categorical_accuracy: 0.5979\n",
      "Epoch 29: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6752 - sparse_categorical_accuracy: 0.5979 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 30/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6831 - sparse_categorical_accuracy: 0.5437\n",
      "Epoch 30: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6831 - sparse_categorical_accuracy: 0.5437 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 31/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6810 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 31: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 362ms/step - loss: 0.6810 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 32/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6772 - sparse_categorical_accuracy: 0.5750\n",
      "Epoch 32: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6772 - sparse_categorical_accuracy: 0.5750 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 33/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6754 - sparse_categorical_accuracy: 0.6083\n",
      "Epoch 33: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6754 - sparse_categorical_accuracy: 0.6083 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 34/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6733 - sparse_categorical_accuracy: 0.5688\n",
      "Epoch 34: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6733 - sparse_categorical_accuracy: 0.5688 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 35/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6713 - sparse_categorical_accuracy: 0.6208\n",
      "Epoch 35: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6713 - sparse_categorical_accuracy: 0.6208 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 36/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6792 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 36: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6792 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 37/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6727 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 37: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6727 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 38/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6789 - sparse_categorical_accuracy: 0.5646\n",
      "Epoch 38: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6789 - sparse_categorical_accuracy: 0.5646 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 39/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6763 - sparse_categorical_accuracy: 0.5979\n",
      "Epoch 39: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6763 - sparse_categorical_accuracy: 0.5979 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 40/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6737 - sparse_categorical_accuracy: 0.5875\n",
      "Epoch 40: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6737 - sparse_categorical_accuracy: 0.5875 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 41/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6720 - sparse_categorical_accuracy: 0.6083\n",
      "Epoch 41: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6720 - sparse_categorical_accuracy: 0.6083 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 42/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6812 - sparse_categorical_accuracy: 0.5542\n",
      "Epoch 42: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6812 - sparse_categorical_accuracy: 0.5542 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 43/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6770 - sparse_categorical_accuracy: 0.5958\n",
      "Epoch 43: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6770 - sparse_categorical_accuracy: 0.5958 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 44/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6766 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 44: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6766 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 45/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6751 - sparse_categorical_accuracy: 0.5875\n",
      "Epoch 45: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6751 - sparse_categorical_accuracy: 0.5875 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 46/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6852 - sparse_categorical_accuracy: 0.5604\n",
      "Epoch 46: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6852 - sparse_categorical_accuracy: 0.5604 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 47/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6767 - sparse_categorical_accuracy: 0.5979\n",
      "Epoch 47: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6767 - sparse_categorical_accuracy: 0.5979 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 48/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6760 - sparse_categorical_accuracy: 0.5833\n",
      "Epoch 48: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 363ms/step - loss: 0.6760 - sparse_categorical_accuracy: 0.5833 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 49/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6800 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 49: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6800 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 50/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6763 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 50: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6763 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 51/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6742 - sparse_categorical_accuracy: 0.5875\n",
      "Epoch 51: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6742 - sparse_categorical_accuracy: 0.5875 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 52/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6773 - sparse_categorical_accuracy: 0.5896\n",
      "Epoch 52: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6773 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 53/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6844 - sparse_categorical_accuracy: 0.5417\n",
      "Epoch 53: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6844 - sparse_categorical_accuracy: 0.5417 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 54/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6803 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 54: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 361ms/step - loss: 0.6803 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 55/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6748 - sparse_categorical_accuracy: 0.6000\n",
      "Epoch 55: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6748 - sparse_categorical_accuracy: 0.6000 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 56/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6741 - sparse_categorical_accuracy: 0.5917\n",
      "Epoch 56: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6741 - sparse_categorical_accuracy: 0.5917 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 57/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6798 - sparse_categorical_accuracy: 0.5688\n",
      "Epoch 57: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6798 - sparse_categorical_accuracy: 0.5688 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 58/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6744 - sparse_categorical_accuracy: 0.5875\n",
      "Epoch 58: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6744 - sparse_categorical_accuracy: 0.5875 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 59/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6832 - sparse_categorical_accuracy: 0.5708\n",
      "Epoch 59: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6832 - sparse_categorical_accuracy: 0.5708 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 60/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6820 - sparse_categorical_accuracy: 0.5437\n",
      "Epoch 60: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 359ms/step - loss: 0.6820 - sparse_categorical_accuracy: 0.5437 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 61/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6739 - sparse_categorical_accuracy: 0.5729\n",
      "Epoch 61: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 359ms/step - loss: 0.6739 - sparse_categorical_accuracy: 0.5729 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 62/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6789 - sparse_categorical_accuracy: 0.5625\n",
      "Epoch 62: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6789 - sparse_categorical_accuracy: 0.5625 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 63/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6799 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 63: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6799 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 64/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6831 - sparse_categorical_accuracy: 0.5437\n",
      "Epoch 64: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6831 - sparse_categorical_accuracy: 0.5437 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 65/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6712 - sparse_categorical_accuracy: 0.5896\n",
      "Epoch 65: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6712 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 66/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6715 - sparse_categorical_accuracy: 0.5896\n",
      "Epoch 66: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6715 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 67/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6748 - sparse_categorical_accuracy: 0.5750\n",
      "Epoch 67: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6748 - sparse_categorical_accuracy: 0.5750 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 68/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6742 - sparse_categorical_accuracy: 0.5771\n",
      "Epoch 68: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6742 - sparse_categorical_accuracy: 0.5771 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 69/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6777 - sparse_categorical_accuracy: 0.5750\n",
      "Epoch 69: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6777 - sparse_categorical_accuracy: 0.5750 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 70/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6771 - sparse_categorical_accuracy: 0.5833\n",
      "Epoch 70: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6771 - sparse_categorical_accuracy: 0.5833 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 71/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6755 - sparse_categorical_accuracy: 0.5792\n",
      "Epoch 71: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 366ms/step - loss: 0.6755 - sparse_categorical_accuracy: 0.5792 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 72/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6745 - sparse_categorical_accuracy: 0.5833\n",
      "Epoch 72: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 364ms/step - loss: 0.6745 - sparse_categorical_accuracy: 0.5833 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 73/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6712 - sparse_categorical_accuracy: 0.5771\n",
      "Epoch 73: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 367ms/step - loss: 0.6712 - sparse_categorical_accuracy: 0.5771 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 74/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6754 - sparse_categorical_accuracy: 0.6062\n",
      "Epoch 74: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 370ms/step - loss: 0.6754 - sparse_categorical_accuracy: 0.6062 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 75/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6816 - sparse_categorical_accuracy: 0.5729\n",
      "Epoch 75: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 370ms/step - loss: 0.6816 - sparse_categorical_accuracy: 0.5729 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 76/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6742 - sparse_categorical_accuracy: 0.5896\n",
      "Epoch 76: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 371ms/step - loss: 0.6742 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 77/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6772 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 77: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 374ms/step - loss: 0.6772 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 78/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6830 - sparse_categorical_accuracy: 0.5792\n",
      "Epoch 78: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 373ms/step - loss: 0.6830 - sparse_categorical_accuracy: 0.5792 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 79/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6758 - sparse_categorical_accuracy: 0.5625\n",
      "Epoch 79: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 375ms/step - loss: 0.6758 - sparse_categorical_accuracy: 0.5625 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 80/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6742 - sparse_categorical_accuracy: 0.6000\n",
      "Epoch 80: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 371ms/step - loss: 0.6742 - sparse_categorical_accuracy: 0.6000 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 81/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6765 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 81: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 370ms/step - loss: 0.6765 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 82/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6805 - sparse_categorical_accuracy: 0.5604\n",
      "Epoch 82: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 370ms/step - loss: 0.6805 - sparse_categorical_accuracy: 0.5604 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 83/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6739 - sparse_categorical_accuracy: 0.5958\n",
      "Epoch 83: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 370ms/step - loss: 0.6739 - sparse_categorical_accuracy: 0.5958 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 84/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6731 - sparse_categorical_accuracy: 0.5896\n",
      "Epoch 84: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 370ms/step - loss: 0.6731 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 85/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6779 - sparse_categorical_accuracy: 0.5646\n",
      "Epoch 85: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6779 - sparse_categorical_accuracy: 0.5646 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 86/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6746 - sparse_categorical_accuracy: 0.5688\n",
      "Epoch 86: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6746 - sparse_categorical_accuracy: 0.5688 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 87/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6724 - sparse_categorical_accuracy: 0.5917\n",
      "Epoch 87: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6724 - sparse_categorical_accuracy: 0.5917 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 88/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6796 - sparse_categorical_accuracy: 0.5917\n",
      "Epoch 88: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6796 - sparse_categorical_accuracy: 0.5917 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 89/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6790 - sparse_categorical_accuracy: 0.5771\n",
      "Epoch 89: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6790 - sparse_categorical_accuracy: 0.5771 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 90/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6744 - sparse_categorical_accuracy: 0.6271\n",
      "Epoch 90: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6744 - sparse_categorical_accuracy: 0.6271 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 91/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6755 - sparse_categorical_accuracy: 0.5958\n",
      "Epoch 91: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6755 - sparse_categorical_accuracy: 0.5958 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 92/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6769 - sparse_categorical_accuracy: 0.5979\n",
      "Epoch 92: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6769 - sparse_categorical_accuracy: 0.5979 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 93/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6766 - sparse_categorical_accuracy: 0.5562\n",
      "Epoch 93: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6766 - sparse_categorical_accuracy: 0.5562 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 94/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6805 - sparse_categorical_accuracy: 0.5458\n",
      "Epoch 94: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6805 - sparse_categorical_accuracy: 0.5458 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 95/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6818 - sparse_categorical_accuracy: 0.5396\n",
      "Epoch 95: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6818 - sparse_categorical_accuracy: 0.5396 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 96/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6797 - sparse_categorical_accuracy: 0.5938\n",
      "Epoch 96: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6797 - sparse_categorical_accuracy: 0.5938 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 97/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6776 - sparse_categorical_accuracy: 0.6021\n",
      "Epoch 97: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6776 - sparse_categorical_accuracy: 0.6021 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 98/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6756 - sparse_categorical_accuracy: 0.5938\n",
      "Epoch 98: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6756 - sparse_categorical_accuracy: 0.5938 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 99/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6743 - sparse_categorical_accuracy: 0.6042\n",
      "Epoch 99: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6743 - sparse_categorical_accuracy: 0.6042 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 100/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6739 - sparse_categorical_accuracy: 0.6000\n",
      "Epoch 100: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6739 - sparse_categorical_accuracy: 0.6000 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 101/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6715 - sparse_categorical_accuracy: 0.6042\n",
      "Epoch 101: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6715 - sparse_categorical_accuracy: 0.6042 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 102/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6673 - sparse_categorical_accuracy: 0.5979\n",
      "Epoch 102: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6673 - sparse_categorical_accuracy: 0.5979 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 103/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6760 - sparse_categorical_accuracy: 0.5750\n",
      "Epoch 103: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6760 - sparse_categorical_accuracy: 0.5750 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 104/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6725 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 104: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 359ms/step - loss: 0.6725 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 105/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6770 - sparse_categorical_accuracy: 0.5896\n",
      "Epoch 105: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6770 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 106/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6671 - sparse_categorical_accuracy: 0.6146\n",
      "Epoch 106: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6671 - sparse_categorical_accuracy: 0.6146 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 107/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6853 - sparse_categorical_accuracy: 0.5354\n",
      "Epoch 107: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6853 - sparse_categorical_accuracy: 0.5354 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 108/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6723 - sparse_categorical_accuracy: 0.5833\n",
      "Epoch 108: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6723 - sparse_categorical_accuracy: 0.5833 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 109/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6803 - sparse_categorical_accuracy: 0.5938\n",
      "Epoch 109: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6803 - sparse_categorical_accuracy: 0.5938 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 110/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6762 - sparse_categorical_accuracy: 0.5938\n",
      "Epoch 110: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 362ms/step - loss: 0.6762 - sparse_categorical_accuracy: 0.5938 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 111/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6839 - sparse_categorical_accuracy: 0.5521\n",
      "Epoch 111: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6839 - sparse_categorical_accuracy: 0.5521 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 112/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6755 - sparse_categorical_accuracy: 0.6083\n",
      "Epoch 112: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6755 - sparse_categorical_accuracy: 0.6083 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 113/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6760 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 113: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6760 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 114/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6792 - sparse_categorical_accuracy: 0.5625\n",
      "Epoch 114: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6792 - sparse_categorical_accuracy: 0.5625 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 115/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6726 - sparse_categorical_accuracy: 0.6187\n",
      "Epoch 115: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 361ms/step - loss: 0.6726 - sparse_categorical_accuracy: 0.6187 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 116/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6848 - sparse_categorical_accuracy: 0.5625\n",
      "Epoch 116: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6848 - sparse_categorical_accuracy: 0.5625 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 117/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6720 - sparse_categorical_accuracy: 0.6042\n",
      "Epoch 117: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6720 - sparse_categorical_accuracy: 0.6042 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 118/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6750 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 118: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6750 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 119/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6736 - sparse_categorical_accuracy: 0.6229\n",
      "Epoch 119: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6736 - sparse_categorical_accuracy: 0.6229 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 120/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6718 - sparse_categorical_accuracy: 0.6167\n",
      "Epoch 120: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6718 - sparse_categorical_accuracy: 0.6167 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 121/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6797 - sparse_categorical_accuracy: 0.5833\n",
      "Epoch 121: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6797 - sparse_categorical_accuracy: 0.5833 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 122/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6733 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 122: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6733 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 123/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6661 - sparse_categorical_accuracy: 0.6021\n",
      "Epoch 123: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6661 - sparse_categorical_accuracy: 0.6021 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 124/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6832 - sparse_categorical_accuracy: 0.5750\n",
      "Epoch 124: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6832 - sparse_categorical_accuracy: 0.5750 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 125/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6791 - sparse_categorical_accuracy: 0.5813\n",
      "Epoch 125: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6791 - sparse_categorical_accuracy: 0.5813 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 126/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6780 - sparse_categorical_accuracy: 0.5688\n",
      "Epoch 126: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 361ms/step - loss: 0.6780 - sparse_categorical_accuracy: 0.5688 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 127/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6790 - sparse_categorical_accuracy: 0.5938\n",
      "Epoch 127: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6790 - sparse_categorical_accuracy: 0.5938 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 128/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6769 - sparse_categorical_accuracy: 0.5792\n",
      "Epoch 128: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6769 - sparse_categorical_accuracy: 0.5792 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 129/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6731 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 129: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6731 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 130/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6765 - sparse_categorical_accuracy: 0.5938\n",
      "Epoch 130: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6765 - sparse_categorical_accuracy: 0.5938 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 131/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6777 - sparse_categorical_accuracy: 0.5667\n",
      "Epoch 131: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6777 - sparse_categorical_accuracy: 0.5667 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 132/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6743 - sparse_categorical_accuracy: 0.6125\n",
      "Epoch 132: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6743 - sparse_categorical_accuracy: 0.6125 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 133/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6833 - sparse_categorical_accuracy: 0.5708\n",
      "Epoch 133: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6833 - sparse_categorical_accuracy: 0.5708 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 134/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6746 - sparse_categorical_accuracy: 0.5792\n",
      "Epoch 134: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 360ms/step - loss: 0.6746 - sparse_categorical_accuracy: 0.5792 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 135/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6731 - sparse_categorical_accuracy: 0.5938\n",
      "Epoch 135: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6731 - sparse_categorical_accuracy: 0.5938 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 136/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6739 - sparse_categorical_accuracy: 0.6000\n",
      "Epoch 136: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6739 - sparse_categorical_accuracy: 0.6000 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 137/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6734 - sparse_categorical_accuracy: 0.6021\n",
      "Epoch 137: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 362ms/step - loss: 0.6734 - sparse_categorical_accuracy: 0.6021 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 138/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6730 - sparse_categorical_accuracy: 0.6083\n",
      "Epoch 138: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6730 - sparse_categorical_accuracy: 0.6083 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 139/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6761 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 139: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6761 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 140/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6729 - sparse_categorical_accuracy: 0.5875\n",
      "Epoch 140: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6729 - sparse_categorical_accuracy: 0.5875 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 141/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6720 - sparse_categorical_accuracy: 0.6062\n",
      "Epoch 141: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6720 - sparse_categorical_accuracy: 0.6062 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 142/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6817 - sparse_categorical_accuracy: 0.5583\n",
      "Epoch 142: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6817 - sparse_categorical_accuracy: 0.5583 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 143/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6767 - sparse_categorical_accuracy: 0.5792\n",
      "Epoch 143: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 359ms/step - loss: 0.6767 - sparse_categorical_accuracy: 0.5792 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 144/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6905 - sparse_categorical_accuracy: 0.5312\n",
      "Epoch 144: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6905 - sparse_categorical_accuracy: 0.5312 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 145/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6769 - sparse_categorical_accuracy: 0.5646\n",
      "Epoch 145: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6769 - sparse_categorical_accuracy: 0.5646 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 146/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6729 - sparse_categorical_accuracy: 0.5979\n",
      "Epoch 146: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6729 - sparse_categorical_accuracy: 0.5979 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 147/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6721 - sparse_categorical_accuracy: 0.5854\n",
      "Epoch 147: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6721 - sparse_categorical_accuracy: 0.5854 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 148/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6700 - sparse_categorical_accuracy: 0.6062\n",
      "Epoch 148: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 358ms/step - loss: 0.6700 - sparse_categorical_accuracy: 0.6062 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 149/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6715 - sparse_categorical_accuracy: 0.5792\n",
      "Epoch 149: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 361ms/step - loss: 0.6715 - sparse_categorical_accuracy: 0.5792 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n",
      "Epoch 150/150\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6728 - sparse_categorical_accuracy: 0.5896\n",
      "Epoch 150: val_loss did not improve from 0.68696\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.6728 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.6888 - val_sparse_categorical_accuracy: 0.4667 - lr: 1.0000e-08\n"
     ]
    }
   ],
   "source": [
    "histories = {model_nno_weighted: None, model_obj_weighted: None}\n",
    "\n",
    "try:\n",
    "    model_nno_weighted.load_weights(stance_config.MODEL_NNO_WEIGHTED_CHECKPOINT_FOLDER)\n",
    "    model_obj_weighted.load_weights(stance_config.MODEL_OBJ_WEIGHTED_CHECKPOINT_FOLDER)\n",
    "    print('The weights of the models have already been found. Skipping training.')\n",
    "\n",
    "except:\n",
    "    \n",
    "    for model, epochs, batch_size in [(model_nno_weighted, stance_config.MODEL_NNO_WEIGHTED_EPOCHS, stance_config.MODEL_NNO_WEIGHTED_BATCH_SIZE),\n",
    "                                      (model_obj_weighted, stance_config.MODEL_OBJ_WEIGHTED_EPOCHS, stance_config.MODEL_OBJ_WEIGHTED_BATCH_SIZE)]:\n",
    "\n",
    "        histories[model] = model.fit(\n",
    "            ds_splits[model]['x_train'], \n",
    "            ds_splits[model]['y_train'], \n",
    "            validation_data = (\n",
    "                ds_splits[model]['x_val'],\n",
    "                ds_splits[model]['y_val']\n",
    "            ),\n",
    "            epochs = epochs,\n",
    "            callbacks = model_weighted_callbacks[model],\n",
    "            batch_size = batch_size,\n",
    "            #The classes are unbalanced, we re-weight the classes in such a way that underrepresented classes have more weight.\n",
    "            #We weight considering the train set since the splitting was made in such a way that all the sets have the same distribution of classes.\n",
    "            class_weight = pd.Series(ds_splits[model]['y_train']).value_counts().map(lambda x: len(ds_splits[model]['y_train'])/(2.*x)).to_dict()\n",
    "        )\n",
    "\n",
    "    #Finally, load the best weights obtained during the training:\n",
    "    model_nno_weighted.load_weights(stance_config.MODEL_NNO_WEIGHTED_CHECKPOINT_FOLDER)\n",
    "    model_obj_weighted.load_weights(stance_config.MODEL_OBJ_WEIGHTED_CHECKPOINT_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "865e8a5f-4d91-49c7-a48e-f02dfeca7351",
   "metadata": {},
   "source": [
    "Let us inspect the history of the training of both networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80870657-4602-4bb4-b4a9-5011052b86ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEWCAYAAABPON1ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA3zUlEQVR4nO3deXxcdb3/8ddnlsxMMtn3NGmT7itd2VqElk02QRaBikqtXrguFwX14oKCgr+fXrlX9AciuKFQqFwVBWSvLWUvpRul+5K26ZY0bfbMZJbv748zSdNmT5PMSfJ5Ph7zmJmzzWemzXu+8z3fc44YY1BKKWVfjngXoJRSqnMa1EopZXMa1EopZXMa1EopZXMa1EopZXMa1EopZXMa1Mo2RGSRiLx5CuvfJCKv9GVNfUlEfi0i3+/rZdXQJzqOWolIKfBFY8xrca5jUayOc+JZR3vs8hmp4Ulb1GpIEBHXcH59NbRpUKsOiYhHRB4QkQOx2wMi4onNyxKR50WkSkSOisgbIuKIzbtTRPaLSK2IbBWRCzrYfqaIPCsiNSKyChjTal6xiJjWASgiK0Tki7HHi0TkLRH5uYgcBe45uesktv6/i8h2ETkmIg+JiMTmOUXkv0XkiIjsFpGvnvx6rbbzODASeE5E6kTkP1vV9wUR2Qv8K7bs/4rIIRGpFpGVIjKl1XYeE5H7Yo/ni0iZiHxDRMpF5KCIfL6Xy2aKyHOxz/F9EbnvVLqQlP1oUKvOfA84C5gBTAfOAO6KzfsGUAZkA7nAdwEjIhOArwKnG2OSgY8DpR1s/yEgAOQDi2O3njgT2AXkAD/uYJkrgNNj9V8fqwfg34BLY+9tFvDJjl7EGPNZYC/wCWOM3xjzX61mnwdMarXdF4FxsZrWAEs6qT8PSAVGAF8AHhKR9F4s+xBQH1vm5thNDSEa1KozNwE/MsaUG2MqgB8Cn43NC2EF7ChjTMgY84axdnhEAA8wWUTcxphSY8zOkzcsIk7gWuAHxph6Y8xG4I89rO+AMeb/GWPCxpjGDpb5iTGmyhizF1iOFcxghfYvjDFlxphjwE96+NrN7onV3whgjPm9MabWGBME7gGmi0hqB+uGsD7fkDHmBaAOmNCTZVt9jncbYxqMMZvo+eeobE6DWnWmANjT6vme2DSAnwE7gFdEZJeIfBvAGLMD+DpWSJWLyFIRKaCtbMAF7Dtp+z2xr+tFONTqcQPgjz0uOGn97myr0xpi3Sk/EZGdIlLD8V8SWR2sW2mMCXdQX3eXbe9z7O17UTalQa06cwAY1er5yNg0Yq3GbxhjRgOfAO5o7os2xjwZG7kxCjDAT9vZdgUQBopO2n6z+th9YqtpeSdt41SGLB0ECls9L+powS5eq/X0TwNXARdidVMUx6ZLL+rrrubPsSfvRQ0yGtSqmVtEvK1uLuAp4C4RyRaRLOAHwBMAInKFiIyN7ZyrweryiIjIBBE5P7bTMQA0xuadwBgTAf6GtRMwUUQm06pvNdbVsh/4TKyluphWOxv7wNPA10RkhIikAXd2sfxhYHQXyyQDQaAS6wvm/5xqkV1p53OcCHyuv19XDSwNatXsBaxQbb7dA9wHrAY2AB9i7Ry7L7b8OOA1rL7Sd4BfGWNWYPVP/wQ4gtXtkIO1o7E9X8X6+X4IeAz4w0nz/w34FlbwTQHePpU3eJLfAK9gvbe1WO8/TDtfKjH/F+tLq0pEvtnBMn/C6r7ZD2wC3u3DejvzVawW/CHgcawv2OAAvbYaAHrAi1KAiFwK/NoYM6rLhW1ORH4K5BljdPTHEKEtajUsiYhPRC4TEZeIjADuBp6Jd129ISITReQ0sZyBNXxvUL4X1T4NajVcCdZww2NYXR+bsfrgB6NkrH7qeqy+9/8G/hHXilSf0q4PpZSyOW1RK6WUzfXLiWSysrJMcXFxr9atr68nKSmpbwvqY1rjqbN7faA19hWtsXs++OCDI8aY7HZnGmP6/DZ79mzTW8uXL+/1ugNFazx1dq/PGK2xr2iN3QOsNh1kqnZ9KKWUzWlQK6WUzWlQK6WUzelVKZRSnQqFQpSVlREIBHq1fmpqKps3b+7jqvrWQNbo9XopLCzE7XZ3ex0NaqVUp8rKykhOTqa4uJjYBXJ6pLa2luTk5H6orO8MVI3GGCorKykrK6OkpKTb62nXh1KqU4FAgMzMzF6FtDqRiJCZmdnjXyca1EqpLmlI953efJa2CWpjDL9ctp0PK8JdL6yUUsOIbYJaRHh05S42HOnodMBKqeGosrKSGTNmMGPGDPLy8hgxYkTL86ampk7XXb16NbfddluXr3HhhRf2Vbn9wlY7E1N9bhpCoXiXoZSykczMTNatWwfAPffcg9/v55vfPH7thnA4jMvVfpTNmTOHOXPmdPkar732Wp/U2l9s06IGSPG5qQ/p2fyUUp1btGgRd9xxBwsWLODOO+9k1apVzJ07l5kzZzJ37ly2bt0KwIoVK7jiiisAK+QXL17M/PnzGT16NL/85S9btpefn9+y/Pz587nuuuuYOHEiN910EyZ2htEXXniBiRMncs4553Dbbbe1bHcgdKtFLSKlQC3WZYrCxpiuv6J6IdXn4ugxDWql7OqHz33EpgM1PVonEongdDo7nD+5IIW7PzGlx7Vs27aN1157DafTSU1NDStXrsTlcvHaa6/x3e9+l7/+9a9t1tmyZQvLly+ntraWCRMm8KUvfanNeOa1a9fy0UcfUVBQwLx583jrrbeYM2cOt956KytXrqSkpISFCxf2uN5T0ZOujwXGmCP9VglW18e+cg1qpVTXPvWpT7V8AVRXV3PzzTezfft2RIRQB12ol19+OR6PB4/HQ05ODocPH6awsPCEZc4444yWaTNmzKC0tBS/38/o0aNbxj4vXLiQRx99tB/f3Yls10ddr13UStlWb1q+/XUwSevTkn7/+99nwYIFPPPMM5SWljJ//vx21/F4PC2PnU4n4XDbUWbtLdPc/REv3Q1qA7wiIgZ4xBjT5qtERG4BbgHIzc1lxYoVPS6m5kiQ+lC0V+sOpLq6Oq3xFNm9PtAam6WmplJbW9vr9SORyCmt31owGMTtdhMKhWhsbGzZbmVlJRkZGdTW1vLII49gjKG2tpaGhgbC4TC1tbUt6zavE41Gqaura3l+8vIATU1NBAIBRowYwc6dO9m4cSOjRo3iiSeeOGG5ngoEAj36d+tuUM8zxhwQkRzgVRHZYoxZ2XqBWHg/CjBnzhzT0TdaZzZGt/NS6TbOPudjeFwd92nFW/MOBzuze412rw+0xmabN28+pRZxX7aom7st3G43Pp+vZbvf/e53ufnmm3n44Yc5//zzERGSk5NJTEzE5XKRnJzcsm7zOg6HA7/f3/L85OUBEhIS8Hq95OTk8PDDD3PdddeRlZXFGWecweHDh3v9vrxeLzNnzuz28t0KamPMgdh9uYg8A5wBrOx8rZ5L9Vmd+tWNIXKS7RvUSqn4uOeee9qdfvbZZ7Nt27aW5/feey8A8+fPb/kiO3ndjRs3tjw+ePBgm+UBHnzwwZbHCxYsYMuWLRhj+MpXvtKtYX99pcvheSKSJCLJzY+Bi4GNna/VOymxoK5p1I5qpZS9/OY3v2HGjBlMmTKF6upqbr311gF77e60qHOBZ2LHp7uAJ40xL/VHMa1b1EopZSe33347t99+e1xeu8ugNsbsAqYPQC0a1Eop1Q5bHZmoQa2UUm3ZM6gbNKiVUqqZrYI6paVFrac6VUqpZrYKarfTgdepXR9KqePmz5/Pyy+/fMK0Bx54gC9/+csdLr969WoALrvsMqqqqtosc88993D//fd3+rp///vf2bRpU8vzH/zgB3E7y56tghog0S0a1EqpFgsXLmTp0qUnTFu6dGm3Toz0wgsvkJaW1qvXPTmof/SjH8XtvNW2C+okDWqlVCvXXXcdzz//PMFgEIDS0lIOHDjAk08+yZw5c5gyZQp33313u+sWFxdz5Ih1Lrkf//jHTJgwgQsvvLDlNKhgjY8+77zzmD59Otdeey0NDQ28/fbbPPvss3zrW99ixowZ7Ny5k0WLFvGXv/wFgGXLljFz5kymTZvG4sWLW2orLi7m7rvvZtasWUybNo0tW7b0yWdgq5MyASS69IAXpWzrxW/DoQ97tIovEgZnJ1GTNw0u/UmHszMzMznjjDN46aWXuOqqq1i6dCk33HAD3/nOd8jIyCASiXDBBRewYcMGTjvttHa38cEHH7B06VLWrl1LOBxm1qxZzJ49G4BrrrmGG2+8keTkZO666y5+97vf8R//8R9ceeWVXHHFFVx33XUnbCsQCLBo0SKWLVvG+PHj+dznPsfDDz/M17/+dQCysrJYs2YNv/rVr7j//vv57W9/26PPqz3aolZK2V7r7o/mbo+nn36aWbNmMXPmTD766KMTuilO9sYbb3D11VeTmJhISkoKV155Zcu8jRs38vGPf5xp06axZMkSPvroo05r2bp1KyUlJYwfPx6Am2++mZUrj59R45prrgFg9uzZlJaW9vYtn8B2Leokt3CgVoNaKVvqpOXbkcY+OCnTJz/5Se644w7WrFlDY2Mj6enp3H///bz//vukp6ezaNEiAoFAp9vo6OrfixYtYsmSJcydO5fHHnusy7PadXXK0+bTpHZ0GtXesF2LOtGtoz6UUify+/3Mnz+fxYsXs3DhQmpqakhKSiI1NZXDhw/z4osvdrr+ueeeyzPPPNNyatTnnnuuZV5tbS15eXmEQiGWLFnSMj05Obnd05hOnDiR0tJSduzYAcDjjz/Oeeed10fvtH22C+okt9AYitAUjsa7FKWUjSxcuJD169dz4403Mn36dGbOnMmUKVNYvHgx8+bN63TdWbNmccMNNzBjxgyuvfZaPvaxj7XMu/feezn//PO56KKLmDhxYsv0G2+8kZ/97GfMnDmTnTt3tkz3er384Q9/4FOf+hTTpk3D4XDw7//+733/hlszxvT5bfbs2aa3vvfYK2bUnc+b8ppAr7fR35YvXx7vErpk9xrtXp8xWmOzTZs2ndL6NTU1fVRJ/xnoGtv7TIHVpoNMtWWLGrT7QymlmtkwqK17DWqllLLYLqgTYy1qHUutlH2YOF/cdSjpzWdpv6B2adeHUnbi9XqprKzUsO4DxhgqKyvxer09Ws+W46hBg1opuygsLKSsrIyKioperR8IBHocTANtIGv0er0UFhb2aB0bBrV1r0GtlD243W5KSkp6vf6KFSt6dMXteLB7jbbr+nA5BJ/bqUGtlFIxtgtqgII0L3uPNsS7DKWUsgVbBvWk/BQ2HaiJdxlKKWULtgzqyQUp7K9q1O4PpZTCpkE9KT8FgM0HtVWtlFK2DOopGtRKKdXClkGdnewhy5+g/dRKKYVNg1pEmJSfwuZDGtRKKWXLoAaYnJ/CtkN1hCJ6Xmql1PBm26CelJ9CUyTKror6eJeilFJxZdugnlxg7VDcdLA6zpUopVR82TaoR2clkeBysPlg22uWKaXUcGLboHY5HRRnJrKnUrs+lFLDm22DGiA3xcvhmmC8y1BKqbiydVBnJ3uoqNWgVkoNb7YO6twUL+W1AaJRvbKEUmr4sndQJ3sIRQzHGpriXYpSSsWNvYM6xbo0jvZTK6WGM1sHdU6KB4Dy2kCcK1FKqfjpdlCLiFNE1orI8/1ZUGs5yVaLulxb1EqpYawnLeqvAZv7q5D2NLeoD9doi1opNXx1K6hFpBC4HPht/5ZzIo/LSXqim8Pa9aGUGsbEmK6HvonIX4D/CyQD3zTGXNHOMrcAtwDk5ubOXrp0aa8Kqqurw+/3tzy/680GshMdfG2Wt1fb6w8n12hHdq/R7vWB1thXtMbuWbBgwQfGmDntzjTGdHoDrgB+FXs8H3i+q3Vmz55temv58uUnPP/Mb981Vz74Zq+31x9OrtGO7F6j3eszRmvsK1pj9wCrTQeZ2p2uj3nAlSJSCiwFzheRJ079+6N7clO8lGsftVJqGOsyqI0x3zHGFBpjioEbgX8ZYz7T75XF5KZ4KK8N6tGJSqlhy9bjqMEaoheJGirr9ehEpdTw1KOgNsasMO3sSOxPuTpETyk1zNm/RR07jFzPoqeUGq7sH9TJ2qJWSg1vtg/q7Jag1ha1Ump4sn1Qe1xOMpIS9OhEpdSwZfugBmss9cGqxniXoZRScTEognpkho99xzSolVLD0yAJ6kT2HW3Qg16UUsPSoAnqYDhKRZ3uUFRKDT+DI6gzkwDYU9kQ50qUUmrg2Seoo1HY/ipJdaVtZo3MSARg71ENaqXU8GOfoBaBpz9H3qFlbWaNSPMhokGtlBqe7BXUqUV4AxVtZiW4HBSk+tinQa2UGobsE9QAqYV4A+XtzhqZkcieyvoBLkgppeLPXkGdVoQneKTdWSMzEtl7VMdSK6WGH3sFdWoRCaFqaGrbxTEyM5EjdUEamsJxKEwppeLHdkENQM3+NrOaR37s01a1UmqYsVdQp8WCumpvm1nNQa391Eqp4cZeQZ1aaN1X72szS8dSK6WGK3sFdXIBBgdUl7WZlZboJtnr0iF6Sqlhx15B7XQR9GRCVdsWtYhQlJ6oLWql1LBjr6AGAt7sdrs+AArSvBys1gsIKKWGF9sFddDTcVDnpnj12olKqWHHdkEd8GZDzQGIRtrMy0/1cqwhRCDUdp5SSg1V9gzqaBhqD7aZl5viBaBcL3SrlBpGbBfUQU+O9aCdkR95qVZQH6zWg16UUsOH7YI64M2yHrQz8iMv1qI+pP3USqlhxIZB3dyibieoYy1q3aGolBpObBfUUacXfBntBnWy101SglOH6CmlhhXbBTVgHUreTtcHQG6qDtFTSg0v9gzqtJHt7kwEa4jeIW1RK6WGEXsGdWqR1fVhTJtZ1kEvOjxPKTV82DSoC6GpDhqPtZmVFzs6MRptG+JKKTUU2TOom89L3cFY6nDUcKReW9VKqeHBnkHdfKWX9oboxcZSH67WoFZKDQ/2Dur2DnpJ1YNelFLDiz2DOikLXN5OW9SH9DBypdQwYc+gFrF2KLYT1Jl+D06HaItaKTVs2DOower+aKfrw+kQcpM9HNI+aqXUMNFlUIuIV0RWich6EflIRH44EIWRVtThQS95qV62l9di2hlnrZRSQ013WtRB4HxjzHRgBnCJiJzVr1WB1aKuL4dQ2y6Oq2aMYENZNS9uPNTvZSilVLx1GdTGUhd76o7d+r8p2zzyo2Z/m1k3nTmSyfkp/Oi5TdQHw/1eilJKxZN0p/tARJzAB8BY4CFjzJ3tLHMLcAtAbm7u7KVLl/aqoLq6Ovx+P6lVG5m57nusP+2HHMuY0Wa5Hcci3PdegMtK3Fw/IaFXr9VbzTXamd1rtHt9oDX2Fa2xexYsWPCBMWZOuzONMd2+AWnAcmBqZ8vNnj3b9Nby5cutB0d3G3N3ijEf/LHDZb/w2PvmnJ8u6/Vr9VZLjTZm9xrtXp8xWmNf0Rq7B1htOsjUHo36MMZUASuAS07hi6N7UkYA0uEORYCpI1IoO9aoF7tVSg1p3Rn1kS0iabHHPuBCYEs/1wVON6QUQNXeDhcZm+PHGNhVUd/v5SilVLx0p0WdDywXkQ3A+8Crxpjn+7esmPRiOLq7w9ljc6w+pR0VdR0uo5RSg52rqwWMMRuAmQNQS1sZJbD91Q5nF2cm4RDYUa5BrZQauux7ZCJAegnUHYam9rs2vG4nRRmJ7NSgVkoNYfYO6owS676z7o9sv7aolVJDms2DerR1f6zzfurdR+oJR6IDVJRSSg0sewd1enOLeleHi4zJ8dMUiVJ2TE97qpQamuwd1L408GV0b+SHdn8opYYoewc1WP3UnbSodYieUmqoGwRBPbrTPuoUr5ucZI+2qJVSQ5b9gzq9xDqMPNzU4SJjc/xs16BWSg1R9g/qjNFgop0eSj4pP4UtB2sI6cgPpdQQNAiCOjbyo5Puj5kj0wiGo2w5WDtARSml1MAZBEEdG0vdyQ7FGUVpAKzbd2wAClJKqYFl/6BOygZ3UqdD9Eak+cjye1i7t2rg6lJKqQFi/6AWsVrVR3d2sogwc2Qa6/ZVDVxdSik1QOwf1ABZY+HI9k4XmVGUxq4j9VQ1dDw6RCmlBqPBEdSZ46BqD4SDHS4yc2QagLaqlVJDzuAI6qxx1hC9TvqpTytMQwTtp1ZKDTmDI6gzx1r3R7Z1uIjf42JCbrK2qJVSQ87gCurKzvupZ45MY+3eY0SjZgCKUkqpgTE4gtqbAv48OLKj08VmjUynJhDWEzQppYaUwRHUYPVTd9GinlOcAcAHe/TAF6XU0DG4gvrIdjAdd2sUZyaSmZTA6lINaqXU0DF4gjpzHASqoKGyw0VEhFmj0vlgz9GBq0sppfrZ4AnqrHHWfScjPwDmjEqntLKBitqOx1wrpdRgMniCumWIXlf91OmA9lMrpYaOwRPUaSPB6elyh+KUglQSnA7W7NWgVkoNDYMnqB1OyBzTZYva63YyrTCV1aXaT62UGhoGT1AD5EyGwx91udi8sVms3VfF9sN6IQGl1OA3uII6bypU74PGzrs1Fs0tJinBxc9f63zHo1JKDQaDK6hzp1n3hzZ2ulhGUgKLzynhhQ8PsXF/9QAUppRS/WdwBXXeVOv+cOdBDfDFj5WQ6nPz369s7eeilFKqfw2uoPbnWpfm6qJFDZDidfPFc0pYvrWC3UfqB6A4pZTqH4MrqEUgdyoc/rBbi99wehFOh/D06n39XJhSSvWfwRXUYHV/lG+GSKjLRXNSvCyYkM1fPygjHIkOQHFKKdX3BmFQnwaRpi7HUze7fk4R5bVBVmyt6OfClFKqfwy+oM7t/g5FgAUTc8jye/izdn8opQapwRfUWePAmQCHNnRrcbfTwXWzC/nXlnIOVjf2c3FKKdX3Bl9QO92QMwkOdW+HIsBNZ47EGMPj7+zpx8KUUqp/DL6gBsifDgfWdXoRgdaKMhK5aHIuT63aSyAU6d/alFKqj3UZ1CJSJCLLRWSziHwkIl8biMI6NWK2dRGBo7u6vcrn55VwrCHE39fu77+6lFKqH3SnRR0GvmGMmQScBXxFRCb3b1ldKJhl3R9Y2+1VzizJYGJeMn94qxTTzZa4UkrZQZdBbYw5aIxZE3tcC2wGRvR3YZ3KmQQuL+xf0+1VRITF80rYeriWd3Z1fDkvpZSyG+lJ61JEioGVwFRjTM1J824BbgHIzc2dvXTp0l4VVFdXh9/v73K5mWvuBIS1s37S7W03RQzfWNHA2HQnX5vl7VV9Pakxnuxeo93rA62xr2iN3bNgwYIPjDFz2p1pjOnWDfADHwDXdLXs7NmzTW8tX768ewu+cKcx9+YaEw71aPs/e2mLKf7282bPkfqeFxfT7RrjyO412r0+Y7TGvqI1dg+w2nSQqd0a9SEibuCvwBJjzN/65vvjFI2YBeFGqNjSo9U+e/YonCL88Z3S/qlLKaX6WHdGfQjwO2CzMeZ/+r+kbmrZodj9fmqA3BQvl03L5+n391FeG+iHwpRSqm91p0U9D/gscL6IrIvdLuvnurqWMRq8qbD/gx6v+h/njyUUjfKVJWtoCuvJmpRS9tadUR9vGmPEGHOaMWZG7PbCQBTXKYcDCmZCWc+DelxuMv913XTeLz3Gff/c1A/FKaVU3xmcRyY2G3m2dXKmQM8vt3Xl9AJuOXc0f3pnD8+tP9APxSmlVN8Y/EGNgb3v9Wr1//z4BGaOTOO7z3zI/io9YZNSyp4Gd1AXng4OF+x9u1eru5wOHrhhBtGo4fY/ryOkFxdQStnQ4A7qhESrn3pP74IaYFRmEvddPZVVu4/yhT+upjbQ9ZVjlFJqIA3uoAYYNdc6lDzU+66Lq2cW8l/XnsZbO45w/SPvUqoXw1VK2cjgD+qRcyEagrLVp7SZ608v4veLTmf/sQYu/cUbPPHuHj15k1LKFoZAUJ8JCOx955Q3dd74bF6+/VzmFKdz19838v1/bCQa1bBWSsWXK94FnDJfOuROgT1v9cnm8lN9/GnxGfzkpS088vouahrDnDMui4ZgmI9PzSM/1dcnr6OUUt01+IMaoORcWP17aGqwdjCeIhHh25dMJMXr5mcvb+XZ2Djrn7y0hVvPHcMUh7aylVIDZ2gE9biL4d1fwe6VMOGSPtmkiPCVBWO5cnoBAMFwlJ+/uo1fLNtOgV8onFTD5IKUPnktpZTqzNAI6lHzIMEP217qs6BuVpRxvIX+0E2zuH5bBbcteZ9PPvQWZ47OYFxOMk4HNIYiXDw5j3PHZ/fp6yul1NAIalcCjJ4P21+xLngr0m8vdd74bO6b52NVQzbr9lXx1Kq9ADgEnnh3LzeeXsRVM0YQiRrC0SiRqGFifgoj0rRvWynVO0MjqAHGXwJbnofDH0He1H59qeQE4d6LT3yNQCjCA69t59GVO1n6/r4T5jkdwhWn5TMhL5kdh+vIT/OyaG4J2cmefq1TKTU0DJ2gHnexdb/tpX4P6vZ43U6+felErp9TyKGaAG6nA6fDatm/tPEQT763l3+sO0BuioeK2iC/e3M3M4rSqA9GcDuF4qwkRmclUZLlpyjDR3piAulJCSQlOJF+/IWglLK/oRPUybmQPwO2vQznfjNuZYzO9jM6+8Rrr80amc7tF44nFI2S4nWzq6KOR17fxc6KOrL8CTSGIry9o5K/rdnfZntup5DqSyDV5yIn2ctZozOZmJ/MvqMN1AXDfOasUWT5PeyvauTFDw9y3exC0hITBurtKqUGwNAJaoBJV8C/7oOqfZBWFO9qTuBLcOLDCVhh/tPrTmuzTH0wTGllPQeqAhxraKKqoYmj9SGqG5uoaQyz52g9DyzbRusDJv/wVimXn5bP39aUEQhF+fXru/jhlVMIBqLUBkIkJbgQgYq6ILsr6qkJhAlFoswdk6mBrtQgMbSCeup1VlB/+L/wsTviXU2PJXlcTClIZUpBaofLHKtvYndlPcWZSRytD/LdZzby5Ht7uXRqHjecXsTPXt7KV56MXZ5sxSuIQILTQfCkK9mkJ7r55scnMGtkOo2hCIFQhGA4yths/wkjXZRS8Te0gjqjBIrOHLRB3R3pSVbfNUBGUgJ/vuUsDlQHWkaVzBubxfIt5byz5kMKRo2hNhimsSnMiDQfo7P9pCcm0NAU5r9f3cb3ntnY7muMzfEzpSCFnGQPRRmJjMn2Myk/hYzY6x6sbsTndmqLXKkBMrSCGuC06+Gf34BDG+OyU3GgicgJQ//cTgcXT8kjoWIL888d3eF6f77lLN7eWUltIITH7cTnduJ2Cuv3VbNiWwVr91ZxuCZwQkt8dFYSTZEoZcesoP7COSXcdNZI/B4XDU0RdlbUUR+MUJDmpTA9kVSfu1/fu1LDxdAL6slXw4t3woY/D4ug7i0RYd7YrDbTZ4/KYPE5JQAYYzhcE2RnRR0byqr5YM8xXA7h8/NKWLevigeX7+DB5Ts6fI1kj4uJ+cl8fl4JCybksHrPUXYfqWdyfgpVwSirdh/lYHUjeSleSrKSyEnx9tv7VWowG3pBnZQJYy+0uj8u+AE4tVXXWyJCXqqXvFRvu6F+67mjWbv3GIFQFI/bwZhsP0keFweqGtl/rJGyYw28vq2CLy9Zg0OgzYkIl594xsOpI1I4f2IutYEQR+qamD0yjQsm5VKY7tMhimpYG3pBDTDnC/Dkp+DDv8CMhfGuZsiaOiKVqSPa7vicUZTW8jgSNby08RAbyqo4c3QG43OT2XywluWrNnDRWdMpSPNxuCbA5oM1vPDhQX65bDuJCU5SfW6eW3+Ae57bRFKCk6KMRArTfRSmJzJ3TCbnjs/G67ZG0eyqqOPZ9QeYNTJdD+FXQ9LQDOpxF0HOFHjrATjtBnAM/tNuD1ZOh3D5aflcflp+y7TC9ETc5W7mT8wBYEJeMueOz+bW88bQ0BTG57YO8tlVUcfKbRWUVjZQFmuhv72zksfeLiUxwUlBmg+XQ9hyqLZl29fPKSQ/1cdrmw/jdAhTR6SS5fcQjRqSPC7yUj2MzU5mUn4yLqf+v1CDw9AMahE45+vwt3+D7S/DhEvjXZHqpsSE4/8l2zt4KBSJ8s7OSv61pZzy2gD1wQiXT8vnmtmFLHl3D79+fScGmDMqHZfDwXPrD1AbCLfpevF7XBRnJeIUYUS6j6tnFjJ1RAo7yus4Wt9EYoKLHZURMsuqSfI48XtcpCa68bicA/RJKHXc0AxqgCnXwL/uhTf+xzoPiPZxDglup4Nzx2e328Xxn5dM5KazRpHgdLScR6X5cmoiQn0wzMHqAJsO1rBqdyUHqgJEooZVu4/ywoeH2n29n77/Zstjj8vBJVPzuGRKXqzP3FCclURJVlJLgBtj2F/VSH6qr+UUAkqdqqEb1E4XnHM7PH87bPmnddSiGvJOPkth652QSR4XY3P8jM3xt5xnHKxW+sptFeyvamRsjp/cFC8NwQhvvreasZOmUh8MUxcMs+VQDc+uO8A/1h044TU8LgfXzyniosm5/GrFDt7ddZT8VC8XTc7lUHWAXUfqWTAhm8+dXcyO8jpe31bB2WMyuXhyru4kVd0ydIMaYObn4L1H4NXvWydtcukBGqott9PBBZNy20yv3OFk/uQTp991+WS2HKrF7RSMgV1H6nlzewVL39/L4+/uIT3RzdcvHMeGsmqefG8vRRmJjEjz8fu3SvnNG7sBq9/+sbdLmTMqnXG5fo7VhxiVmciZozPIT7X63Z0OweVwUBsMUVEbpDA9kbE5/jY1tlYfDFMbCJOXqsMch5qhHdROF1z8Y1hyLax6FOZ+Nd4VqUHO63aeMKpl6ohUrpxewO0XjeednZVcMDGX1ERrSKgxpqXFvO9oA8+uP8CE3GTmjc3imbX7eWj5DvYcbSDZ62LZlsM8snJXp689OT+FuWMyyU3x4k1wEmiKcHh/iInVATbur+auv2/kSF2QRXOLue3CcaR4T31oaiRqtAvHBoZ2UAOMu9AaV/36T63uj/TieFekhqD8VB/XzCo8YVrrbo2ijES+smBsy/NPnzmST585suV5Y1OE9WVVVDU0EY4aIlFDKGLwe5xkJHnYuL+aZ9cf4PF397Q5b8tvP1wG0PIl8Lu3dvOnd/cwITeZkRmJNEWiRKOGVJ8bX4KTQChKYyhMQ1MEAeYUZzC9MI3SynoOVQe4fk4RIzMTeWrVXu59fhN3XDSeL36s46NcVf8b+kENcNn98Mh58PTNsPhlcOtPQ2UvvgQnZ43O7HD+GSXWEaPGGGoCYYLhCD63k7+/+gYNKaPwuBx8+sxRJLgcfH5eMf9Yt5/NB2vZfKgGj8uJAFsP19LQZK3nS3CSmOAkEIqw/OWtJ7zWo2/sYu6YTFZsrSDL7+G+f24GaAnrA1WNvL2zktOL0xmVmUTZsQa+98xGLpycy2fPGtVvn9FwNjyCOqMErn4Yln4aXroTrnhAR4GoQUlEYudQsbo1ipIdzD9vzAnLdHQgUkcqaoNsPlhDSVYSbqeD//PCZp5df4BFc4v59qUTuf3P67jvn5v51YqdpPrc7D5SD0CCy8GnzxjJs+sPcLS+ide3VZDscfHJmSM6fK1AKALQcrCS6p7hEdQAEy+3RoG8+XNweuCSn+iBMEoB2ckespOPD3f85cKZ3HvV1Ja+9l8unMmf3tnDzoo6KuuCXDe7kLPHZPKHt0p57O1SxmQnseSLZ/LD5z7im/+7niff28umgzVkJCVw4aRcTFWI8vf38e7uSl788BAicMVp+XxsXDYel4NI1FDVGGL74Tre3FFB2bFG0nxuxuYm8+1LJjK5IIVwJEpDKNIn/e6D0fAJaoDzfwCRELzzIDQcsVrW3pR4V6WU7TSHNFijYr4QO1FXa7NGpvPl+WMYmZFIksfFo5+bw9eXrqOyvolPzixg/7FGnnhvD03hKHy0IdbaLiAahec2HODp1WUnbM/jcnBGSQbnjM2mujHEiq3lfOLBNzl7dCYbyqqoC4a5asYIPj+vmFSfmwSXg9xkLw6HUN0QYteROibkJZ9w0NRQMfTeUWccDrj4PkjMhGU/gtI34aIfWQfH6NA9pXpsUv7xhk6K183vF51+wvzGpggvLHudM848i+xkT0uXxw8+MZmyY42EIlEcIqQlusn0J5xw5GdVQxM/e3kr7+ys5JKpeSQmuPjz+/t4Zu3xS9YlJjjJ8nvYe7QBsC5dNzk/hdpgmPKaIABet4NrZxXy5fljSU10Y4xhxdYKfrFsOzWBED63k6aGRn638z0m5iWzYGIOZxRn2OoUA8MrqMHqm/7YHTD6POu81c/cap0WdfJVMO1TMGouOLT/TKm+4EtwkulztLlqUJLHxYS85E7XTUtM4MdXTzth2n+cP5Y3dxwhEjUt50A/XBPgU7MLGZPjZ/2+KjaUVTMi3ce547JxOoSD1Y08+sYunlq1l3G5yTSFo3y4v5rizESmjkglEIqwP1BHdWOIx962xruPyU7irisms2CCdT6aHeV1PLVqL8fqmxgTG8+++WANjU0RijISyUv1kux1kZ6YwGXT8tu8l1M1/IK62YjZ8MVlsGMZbPyLdaa9NX+E5HyY9AmYcBnkToGkbN3xqJRNZPo9XDWj452VHYXkpgM1/OaNXZTXBhDgrssn8bmzi0lwWa3mFStWMH/+OdQFwyzfUs7/vLqNz//hfdIT3XjdTg5WB3A7hcwkD3+LteiLMnwkJbh4d1cl9U3WTtLsZI8GdZ9zOGH8xdatqQG2vQgb/wZrHrcOkAFwJ0JiFiSmQ0ohpI9ixJEQbG0EXzokJFnLJPghIdF6HKiGmv3gzwO/nnZTqXibXJDCz2+Y0eVyfo+LT0wv4ONT8vjz+3vZdriOhqYIY3KSuH5OEVl+D3XBMMYYkr3HD2xqaIpQFwy3jGrpa10GtYj8HrgCKDfGDN1LpiQkwtRrrVtTPex5G47ugmN7oKHSuh3dCTv/xbhwI+z4Tfe2mzMFknMhWGftwKw5aL1WwUzwplrbD8VCP6XAuiqNP89aNlgL4rCWK5wDudOOd8sYA8ROBycO7a5Rqg8luBx89uziduf5PSfGpoiQ5HGR5Om/dm93tvwY8CDwp36rwm4SkqxzWrfHGN569R/Mm1xotZyb6iHUAE111uOmBmskSUoBHN0NpW9AoAY8fkidYXWpBKpg/1qo3AFpo8Cfa03b8xZ8+HTP6xWHtZ3MMeDygsPFpCOVcPQpK+R9adYXgS8dvM2P06zHDhdgrF8Cbp928yhlQ10GtTFmpYgUD0Atg4MIoYQ0q4XbHT29GnrDUeuWlAWeFDBRqC+Hfe9BxbaWGmIPQIBQwAr9Y7ut4YfRCMn1tRDcA8Fq6wulOxxuK9hb33xpVh0Yq4UfDYM4rXmpRZBWZN370iHSZIV9ejG4rNOMYkzH4W+MVW8kaL0XT+cnHVJquBrefdR2lJhh3Vo4rNb5lKt7tJlVK1Ywf/5860k0YoV14zForIrdH7Na8SYKCITqrWVOvtXst9YRhxWkDrcV1o3HrO6Z9jR31zTVW0HsSbb68JvXb6jk3PpKWBE6cb2UQsgeb33xNNVawyiTCyAlH5JyrC+s6v3gz4HsCZA1AbLGWV8EgRoI1lj3YA23TMqGlBHaLaQGPWk+sXqnC1kt6uc766MWkVuAWwByc3NnL126tFcF1dXV4ffbu2WlNVockSCeYAXeQAWucANRhwtXuBFf4wHcoRoiTh9GnDgjAZyRRpyRBhzRMCF3Cg3Gg9ObRNThxogLMVGS6vfgazxAxOkl4vSR0FRNQlMlnuAxhCgGB0FPOglN1ThMuFs1RsVFwJtLoy+fiNOLIxrEGQniiAaJOjw0+vIJeLMJuVMx4sAdqsURDRJxemkIgTsxjYjTE6vJuoEDd6gaZyRIwJtDoy+PqNPTr591R/T/Yt+wQ40LFiz4wBjT7k/1Pgvq1ubMmWNWr17doyKbrWjdErQprfHU9ai+aMTqDvKlW6eujYShag8c2QZHtsda+8nWvgFPsvU83AR1h6wdwkd3wdFSCDdaXTPuJOu+qQ4qd0Lj0VN/Q74Ma2hncq71S6CxytoB7XRb+w3cvlb3HqsbqfGY1erPGm+1/JOyrB3L9eXWaQ6S860aj+4CZ0JsuQJrH0qCHxKSWPHWe8xfsODU6+9Hdv+/CPaoUUQ6DGrt+lD253CeOMzR6bJ2nGaO6ZvrYYYarS+CaMgKWZcPQg28/fprzJ0zvdUO43rrFg1ZQzbdidYXxtFdUHsQag9b95U7rS+VxEwwEWv7jces+3DAunmSrWUqtsGGP/e69PNwwLv+WHgnnRDiJCRZgR9psrq43D5rXnKu9R7LN1ldW3nTrFFI4oBw0KqvqcH6oquvhPRRkDPJesFgnfXlEWqAzLFQdBYkZVr7G5obfXoOnT7XneF5TwHzgSwRKQPuNsb8rr8LU2rAuH2QetJBFM4UmjwZ1pdBZ4pO73x+dwTroO6w1QJ3ea0++HAQag9ZtWWMtsK2YivUV8S+MKzA3LN9E8X5Wa1GHcVudeWxfQRBqzUuDiuAAzXWvgmwvmxSR1hXQYo0ta3LmWB9mdQd7tn7SfBbvxQ8fnC4mN4QhvIx1pfV4Y+s9zhmPuTPsJZ1uq1fTSZi3UPs14fXund5rC8Wh9Pa59FUZ72/QJW1LyQx0/pF4/bFdqqXQmqhtR8jc6w1HaxfWfXl1vupi93XHob6CkZV1MOWems74mh14/hjpNV0OXE5Yss5XNYO9j7WnVEfC/v8VZVSx3liO1pP/lJIP+nczqPObrNqaXgFxT39yR4KWCGemGEFTvOoIYczFoqxgPSlW/ODdVC53Rrt4/FDQrK1s7Z8M+xbZW0Ljo/uCdRYIRhqgEgIR12Z1XpP8Fu/gALVsOk5WPtEz+ruFbFCu6nO+qJojzeVkkA1lD556i+XlAPf2n7q2zmJdn0oNdy4vSdePMPttQ606ojHb3WNnGzkWdatC2vb6/+NRqxfEE11VivZ4Tp+4JYxVgs/1Hi8KyYctLqcnG6ry8mfax0HEKw+PqS1qc769ZFebHXpVGy19mMc3RXr8smzfq34c4/fJ+WAK4E3XnuRj03MtkYOmah1LJmJxkZFmeOPjTlp+knz+unkbhrUSqmB53BaYUnOqW0nKdMK55P50qxz9XRTxOXr/rERcaC9/kopZXMa1EopZXMa1EopZXMa1EopZXMa1EopZXMa1EopZXMa1EopZXMa1EopZXPdOntejzcqUgHs6eXqWUAHJzq2Da3x1Nm9PtAa+4rW2D2jjDHtXmS1X4L6VIjI6o5O9WcXWuOps3t9oDX2Fa3x1GnXh1JK2ZwGtVJK2Zwdg/rReBfQDVrjqbN7faA19hWt8RTZro9aKaXUiezYolZKKdWKBrVSStmcbYJaRC4Rka0iskNEvh3vegBEpEhElovIZhH5SES+FpueISKvisj22H26DWp1ishaEXnejjWKSJqI/EVEtsQ+z7PtVKOI3B77N94oIk+JiNcO9YnI70WkXEQ2tprWYV0i8p3Y39BWEfl4nOr7WezfeYOIPCMiafGqr6MaW837pogYEcmKZ41dsUVQi4gTeAi4FJgMLBSRyfGtCoAw8A1jzCTgLOArsbq+DSwzxowDlsWex9vXgM2tntutxl8ALxljJgLTsWq1RY0iMgK4DZhjjJkKOIEbbVLfY8AlJ01rt67Y/80bgSmxdX4V+9sa6PpeBaYaY04DtgHfiWN9HdWIiBQBFwF7W02LV42dskVQA2cAO4wxu4wxTcBS4Ko414Qx5qAxZk3scS1WuIzAqu2PscX+CHwyLgXGiEghcDnw21aTbVOjiKQA5wK/AzDGNBljqrBRjViXpfOJiAtIBA5gg/qMMSuBoydN7qiuq4ClxpigMWY3sAPrb2tA6zPGvGKMCceevgsUxqu+jmqM+Tnwn1hXSGwWlxq7YpegHgHsa/W8LDbNNkSkGJgJvAfkGmMOghXmnPKF307ZA1j/4aKtptmpxtFABfCHWPfMb0UkyS41GmP2A/djtawOAtXGmFfsUl87OqrLjn9Hi4EXY49tU5+IXAnsN8asP2mWbWpszS5BLe1Ms824QRHxA38Fvm6MqYl3Pa2JyBVAuTHmg3jX0gkXMAt42BgzE6gn/l0xLWJ9vFcBJUABkCQin4lvVb1iq78jEfkeVvfhkuZJ7Sw24PWJSCLwPeAH7c1uZ1rcs8guQV0GFLV6Xoj10zPuRMSNFdJLjDF/i00+LCL5sfn5QHm86gPmAVeKSClWl9H5IvIE9qqxDCgzxrwXe/4XrOC2S40XAruNMRXGmBDwN2Cujeo7WUd12ebvSERuBq4AbjLHD9awS31jsL6U18f+bgqBNSKSh31qPIFdgvp9YJyIlIhIAlZn/rNxrgkREax+1c3GmP9pNetZ4ObY45uBfwx0bc2MMd8xxhQaY4qxPrd/GWM+g71qPATsE5EJsUkXAJuwT417gbNEJDH2b34B1v4Iu9R3so7qeha4UUQ8IlICjANWDXRxInIJcCdwpTGmodUsW9RnjPnQGJNjjCmO/d2UAbNi/09tUWMbxhhb3IDLsPYQ7wS+F+96YjWdg/WzZwOwLna7DMjE2tu+PXafEe9aY/XOB56PPbZVjcAMYHXss/w7kG6nGoEfAluAjcDjgMcO9QFPYfWbh7AC5Qud1YX1k34nsBW4NE717cDq523+m/l1vOrrqMaT5pcCWfGssaubHkKulFI2Z5euD6WUUh3QoFZKKZvToFZKKZvToFZKKZvToFZKKZvToFaDkohERGRdq1ufHekoIsXtnWlNqXhxxbsApXqp0RgzI95FKDUQtEWthhQRKRWRn4rIqthtbGz6KBFZFjtH8jIRGRmbnhs7Z/L62G1ubFNOEflN7BzVr4iIL25vSg17GtRqsPKd1PVxQ6t5NcaYM4AHsc4sSOzxn4x1juQlwC9j038JvG6MmY51/pGPYtPHAQ8ZY6YAVcC1/fpulOqEHpmoBiURqTPG+NuZXgqcb4zZFTuh1iFjTKaIHAHyjTGh2PSDxpgsEakACo0xwVbbKAZeNdaJ+RGROwG3Mea+AXhrSrWhLWo1FJkOHne0THuCrR5H0P05Ko40qNVQdEOr+3dij9/GOrsgwE3Am7HHy4AvQct1J1MGqkiluktbCWqw8onIulbPXzLGNA/R84jIe1gNkYWxabcBvxeRb2FdbebzselfAx4VkS9gtZy/hHWmNaVsQ/uo1ZAS66OeY4w5Eu9alOor2vWhlFI2py1qpZSyOW1RK6WUzWlQK6WUzWlQK6WUzWlQK6WUzWlQK6WUzf1/kCTOlVnxL/IAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_history(histories[model_nno_weighted])\n",
    "display_history(histories[model_obj_weighted])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "164c62e7-4b6a-4eb2-9351-4f4f86877136",
   "metadata": {},
   "source": [
    "From the data we can infer the epoch that allowed the model to generalize better by inspecting the loss on data that it had never seen before (data on the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53b2097f-6c37-4d36-93fe-a6ad5bc81360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 currently uses the weights that have been determined on epoch 137.\n",
      "Model 2 currently uses the weights that have been determined on epoch 10.\n"
     ]
    }
   ],
   "source": [
    "models = [model_nno_weighted, model_obj_weighted]\n",
    "for model_idx in range(len(models)):\n",
    "    if histories[models[model_idx]] is None:\n",
    "        print(f'The data cannot be calculated since the history of the training of the model {model_idx + 1} is not available.')\n",
    "    else:\n",
    "        print(f'Model {model_idx + 1} currently uses the weights that have been determined on epoch {np.argmin(histories[models[model_idx]].history[\"val_loss\"]) + 1}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7eb78d5-74bf-461e-9297-bde332145ba3",
   "metadata": {},
   "source": [
    "The following classification reports sum up the performances of the trained networks with respect to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ede1803-ecbe-4312-9956-5d4f83e9b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the Model 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      0.20      0.08         5\n",
      "           1       0.39      0.60      0.48        25\n",
      "           2       0.85      0.50      0.63        66\n",
      "\n",
      "    accuracy                           0.51        96\n",
      "   macro avg       0.43      0.43      0.40        96\n",
      "weighted avg       0.69      0.51      0.56        96\n",
      "\n",
      "Classification report for the Model 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.40      0.48        35\n",
      "           1       0.43      0.64      0.52        25\n",
      "\n",
      "    accuracy                           0.50        60\n",
      "   macro avg       0.52      0.52      0.50        60\n",
      "weighted avg       0.54      0.50      0.50        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models)):\n",
    "    print(f'Classification report for the Model {i+1}:')    \n",
    "    y_true = ds_splits[models[i]]['y_test']\n",
    "    y_pred = np.argmax(models[i].predict(ds_splits[models[i]]['x_test']).logits,axis=-1)\n",
    "    print(classification_report(y_true, y_pred, zero_division = 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0ead6fb-5082-46d5-a97b-4fece6dab0a1",
   "metadata": {},
   "source": [
    "Once again, given the fact that only few records were available, we compare the performances of the networks with a random classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cc93aeb-48cd-4520-85c0-6b11abe2f70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 96.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation of the expected performances of a uniform random classifier for the task solved by Model 1\n",
      "\tMacro:\n",
      "\t\tPrecision:\n",
      "\t\t\tMean:0.332\n",
      "\t\t\tVar:0.002\n",
      "\t\tRecall\n",
      "\t\t\tMean:0.331\n",
      "\t\t\tVar:0.006\n",
      "\t\tF1-score\n",
      "\t\t\tMean:0.274\n",
      "\t\t\tVar:0.002\n",
      "\tWeighted:\n",
      "\t\tPrecision:\n",
      "\t\t\tMean:0.543\n",
      "\t\t\tVar:0.003\n",
      "\t\tRecall\n",
      "\t\t\tMean:0.332\n",
      "\t\t\tVar:0.002\n",
      "\t\tF1-score\n",
      "\t\t\tMean:0.386\n",
      "\t\t\tVar:0.003\n",
      "Approximation of the expected performances of a uniform random classifier for the task solved by Model 2\n",
      "\tMacro:\n",
      "\t\tPrecision:\n",
      "\t\t\tMean:0.502\n",
      "\t\t\tVar:0.004\n",
      "\t\tRecall\n",
      "\t\t\tMean:0.502\n",
      "\t\t\tVar:0.004\n",
      "\t\tF1-score\n",
      "\t\t\tMean:0.496\n",
      "\t\t\tVar:0.004\n",
      "\tWeighted:\n",
      "\t\tPrecision:\n",
      "\t\t\tMean:0.515\n",
      "\t\t\tVar:0.004\n",
      "\t\tRecall\n",
      "\t\t\tMean:0.502\n",
      "\t\t\tVar:0.004\n",
      "\t\tF1-score\n",
      "\t\t\tMean:0.503\n",
      "\t\t\tVar:0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random_experiment([model_nno_weighted, model_obj_weighted], [ds_splits[model_nno_weighted]['y_test'], ds_splits[model_obj_weighted]['y_test']], [{'min': 0, 'max': 2}, {'min': 0, 'max': 1}], 1000, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a91a2b2c",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "145c5bfd-5222-4aeb-8871-1046b644eb46",
   "metadata": {},
   "source": [
    "To wrap up, we can see that the class re-weighting actually led to benefits for the first training but not for the second This makes sense, since only the dataset upon which the first network was trained was heavily unbalanced. For this reason, we consider the pipeline made of the following networks:\n",
    "- `model_nno_weighted`;\n",
    "- `model_obj`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4981c82b-4409-4583-bd6d-f7be2511d0f4",
   "metadata": {},
   "source": [
    "Finally, given these final models, it can also be interesting to inspect the performances of both models thought as a single one. In particular, we want to measure how well the models that have been defined, used in conjunction, predict all $4$ possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "247c01d0-e813-478b-9ac4-6ed93e1bc5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stance_detection(queries:list, documents:list, model_nno: TFDistilBertForSequenceClassification, model_obj: TFDistilBertForSequenceClassification, tokenizer:DistilBertTokenizer) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given a list of queries and a list of document, the function returns the list of stances of the documents with respect to the queries.\n",
    "    \n",
    "    Args:\n",
    "        queries (list): the list of queries\n",
    "        documents (list): the documents. It can also be a string, in which case it's as if the function was called once for each query and the same document.\n",
    "        model_nno (TFDistilBertForSequenceClassification): the model responsible of distinguishing between \"No stance\", \"Neutral\" and \"Pro object\"\n",
    "        model_obj (TFDistilBertForSequenceClassification): the model responsible of distinguishing between \"Pro 1st object\" and \"Pro 2nd object\"\n",
    "        tokenizer (DistilBertTokenizer): the tokenizer class used to tokenize the input of the TFDistilBertForSequenceClassification instances\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: NumPy array containing the stances of the document with respect to the queries. Possible values: 0: No stance, 1: Neutral, 2: Pro 1st object, 3: Pro 2nd object.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(documents) == str or (type(documents) == list and len(documents) == len(queries)), \"Invalid arguments.\"\n",
    "    \n",
    "    #Prepare the list of lists to give the tokenizer\n",
    "    batch_df = pd.DataFrame(queries, columns = ['query'])\n",
    "    batch_df['document'] = documents\n",
    "    batch_LL = batch_df.values.tolist()\n",
    "\n",
    "    #Tokenize the input\n",
    "    tokenized_input = dict(tokenizer(batch_LL, padding=True, truncation='longest_first', return_tensors='tf'))\n",
    "    #Predict the stance with the first model\n",
    "    output = np.argmax(model_nno.predict(tokenized_input).logits,axis=-1)\n",
    "    \n",
    "    #Looking for \"Pro object\" predictions\n",
    "    obj_idxs = np.argwhere(output == 2).flatten()\n",
    "\n",
    "    #If some predictions have been \"2\"\n",
    "    if obj_idxs.size > 0:\n",
    "\n",
    "      #Considering the tokenization of the relevant inputs:\n",
    "      tokenized_input['input_ids'] = tf.convert_to_tensor(tokenized_input['input_ids'].numpy()[obj_idxs,:])\n",
    "      tokenized_input['attention_mask'] = tf.convert_to_tensor(tokenized_input['attention_mask'].numpy()[obj_idxs, :])\n",
    "      \n",
    "      #model_obj classifies \"Pro 1st object\" as \"0\" and \"Pro 2nd object\" as \"1\", but they should be classified respectively \"2\" and \"3\", therefore I add \"2\" to the classes. \n",
    "      output_2 = np.argmax(model_obj.predict(tokenized_input).logits,axis=-1) + 2\n",
    "\n",
    "      #I replace output with output_2 where needed.\n",
    "      output[obj_idxs] = output_2\n",
    "\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d279659c-d46e-4126-836e-87d40f69ab44",
   "metadata": {
    "tags": []
   },
   "source": [
    "To perform the evaluation, It would be fair to use only the records being in the test sets of both models. However:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df6ff3d7-d17b-465c-a734-650f3243ae80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in the intersection of the test sets: 7\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of elements in the intersection of the test sets: {len(pd.merge(ds_splits_untok[model_nno][\"x_test\"], ds_splits_untok[model_obj][\"x_test\"]))}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcdcde84-c1c9-4e0c-a0ac-fcf47c51eaf2",
   "metadata": {},
   "source": [
    "We only show the number of elements in order to be sure to not accidentally show any records from the Yahoo dataset (as per license conditions).\n",
    "\n",
    "For this reason, we test the performances of the combined model in the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f3e5394-f53a-462f-b0c8-dbe6887a0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = stance_detection(df.question.values.tolist(), df.answer.values.tolist(), model_nno_weighted, model_obj, tokenizer)\n",
    "true_values = df.answer_stance.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d77dc63e-75bb-4b4d-84af-54a591e675cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the final model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.81      0.49        69\n",
      "           1       0.59      0.63      0.61       287\n",
      "           2       0.44      0.47      0.46       324\n",
      "           3       0.45      0.25      0.32       276\n",
      "\n",
      "    accuracy                           0.48       956\n",
      "   macro avg       0.46      0.54      0.47       956\n",
      "weighted avg       0.48      0.48      0.46       956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Classification report for the final model:')    \n",
    "print(classification_report(true_values, predictions, zero_division = 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f9f9f68-d034-4eac-9600-156c4794c9ee",
   "metadata": {},
   "source": [
    "We present the comparison with the initial model evaluated in the same data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "edf6c859-9e4f-4aac-b91b-7d5d68e2b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(model.predict(dict(tokenizer(df.loc[:, ['question','answer']].values.tolist(), padding=True, truncation='longest_first', return_tensors='tf'))).logits,axis=-1)\n",
    "true_values = df.answer_stance.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "acc4eda0-b29a-4597-83ea-303520975bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the baseline model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        69\n",
      "           1       0.37      0.46      0.41       287\n",
      "           2       0.36      0.37      0.37       324\n",
      "           3       0.33      0.30      0.32       276\n",
      "\n",
      "    accuracy                           0.35       956\n",
      "   macro avg       0.27      0.29      0.27       956\n",
      "weighted avg       0.33      0.35      0.34       956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Classification report for the baseline model:')    \n",
    "print(classification_report(true_values, predictions, zero_division = 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "442e43fa-38f8-4824-b806-1c04d47a759e",
   "metadata": {},
   "source": [
    "Once again we also report the expected performances of a random classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e364afc-0e2f-491b-b985-efa9afff658f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 328.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation of the expected performances of a uniform random classifier for the task solved by Model 1\n",
      "\tMacro:\n",
      "\t\tPrecision:\n",
      "\t\t\tMean:0.249\n",
      "\t\t\tVar:0.000\n",
      "\t\tRecall\n",
      "\t\t\tMean:0.249\n",
      "\t\t\tVar:0.000\n",
      "\t\tF1-score\n",
      "\t\t\tMean:0.234\n",
      "\t\t\tVar:0.000\n",
      "\tWeighted:\n",
      "\t\tPrecision:\n",
      "\t\t\tMean:0.293\n",
      "\t\t\tVar:0.000\n",
      "\t\tRecall\n",
      "\t\t\tMean:0.249\n",
      "\t\t\tVar:0.000\n",
      "\t\tF1-score\n",
      "\t\t\tMean:0.264\n",
      "\t\t\tVar:0.000\n"
     ]
    }
   ],
   "source": [
    "random_experiment(['final'],[true_values],[{'min': 0, 'max': 3}], 1000, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb477410-fae3-4291-833f-77845c3a68a3",
   "metadata": {},
   "source": [
    "The experiment has shown that the final model indeed performs better than a random classifier, even though the quantity of data available to train it was not ideal. Furthermore, the experiment provides some evidence in favour of the fact that we can actually gain something in terms of performances by splitting the networks as shown (compare the last classification report with the first one)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b3938eb",
   "metadata": {},
   "source": [
    "### Qrel evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "319c95ce",
   "metadata": {},
   "source": [
    "At the end we wanted to compare the performance of our models with the ones that the other teams were able to reach in the Touchè task.   \n",
    "Therefore we downloaded the stance.qrels file and we run the model on that documents in order to get a classification for each text passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d581fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file: 100%|████████████████████████████████████████████████████████████| 84.9k/84.9k [00:00<00:00, 881kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download the stance qrels\n",
    "url_stance = \"https://zenodo.org/record/6873567/files/touche-task2-2022-stance.qrels?download=1\"\n",
    "file_path_stance = \"stance.qrels\"\n",
    "\n",
    "download_stance = utils.manage_files.DownloadFile(file_path_stance, url=url_stance)\n",
    "download_stance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21dc395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>clueweb12-0002wb-18-34442___2</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>clueweb12-0004wb-69-30215___112</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>clueweb12-0004wb-78-20304___1</td>\n",
       "      <td>SECOND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>clueweb12-0004wb-78-20304___11</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>clueweb12-0008wb-62-05967___1</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic                           doc_id   stance\n",
       "0     12    clueweb12-0002wb-18-34442___2       NO\n",
       "1     12  clueweb12-0004wb-69-30215___112       NO\n",
       "2     12    clueweb12-0004wb-78-20304___1   SECOND\n",
       "3     12   clueweb12-0004wb-78-20304___11  NEUTRAL\n",
       "4     12    clueweb12-0008wb-62-05967___1       NO"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stance_df = pd.read_csv(download_stance.file_name, index_col=None, \n",
    "                    names=[\"topic\", \"0\", \"doc_id\", \"stance\"], sep=\" \").drop(\"0\", axis=1)\n",
    "\n",
    "stance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2e7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file: 100%|███████████████████████████████████████████████████████████████████| 17.3k/17.3k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'C:\\Users\\Andrea\\Downloads\\downloads\\zips\\topics-task-2.zip' unzipped in 'C:\\Users\\Andrea\\Downloads\\downloads\\topics-task-2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file: 100%|███████████████████████████████████████████████████████████████████| 15.5k/15.5k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'C:\\Users\\Andrea\\Downloads\\downloads\\zips\\topics-task-2-2021.zip' unzipped in 'C:\\Users\\Andrea\\Downloads\\downloads\\topics-task-2-2021'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the list of topics (queries)\n",
    "# Download and parse the xml file of the 0-50 topics\n",
    "url_topics = \"https://zenodo.org/record/6873559/files/topics-task-2.zip?download=1\"\n",
    "zip_path_topics = \"topics-task-2.zip\"\n",
    "file_path_topics = \"topics-task-2\"\n",
    "\n",
    "download_topics = utils.manage_files.DownloadFile(file_path_topics, zip_path_topics, url_topics)\n",
    "download_topics()\n",
    "# Retrieve a list of strings from the xml\n",
    "topics = utils.manage_files.open_xml(download_topics.file_name + \"/topics-task-2.xml\")\n",
    "\n",
    "# Topics from 51 to 100\n",
    "url_topics_21 = \"https://zenodo.org/record/6873565/files/topics-task-2-2021.zip?download=1\"\n",
    "zip_path_topics_21 = \"topics-task-2-2021.zip\"\n",
    "file_path_topics_21 = \"topics-task-2-2021\"\n",
    "\n",
    "download_topics_21 = utils.manage_files.DownloadFile(file_path_topics_21, zip_path_topics_21, url_topics_21)\n",
    "download_topics_21()\n",
    "topics += utils.manage_files.open_xml(download_topics_21.file_name + \"/topics-task2-51-100.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd195773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x16e0f157e20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"typeform/distilbert-base-uncased-mnli\")\n",
    "model_nno = TFDistilBertForSequenceClassification.from_pretrained(\"typeform/distilbert-base-uncased-mnli\", num_labels=3)\n",
    "model_obj = TFDistilBertForSequenceClassification.from_pretrained(\"typeform/distilbert-base-uncased-mnli\", num_labels=2, ignore_mismatched_sizes=True)\n",
    "\n",
    "model_nno.load_weights(stance_config.MODEL_NNO_WEIGHTED_CHECKPOINT_FOLDER)\n",
    "model_obj.load_weights(stance_config.MODEL_OBJ_CHECKPOINT_FOLDER) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402be31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stance_detection_qrels(queries, stance_df, model_nno, model_obj, tokenizer, corpus_df):\n",
    "    '''\n",
    "        It returns the results of the stance detection for each topic and document that are in the \n",
    "        stance_df dataframe.\n",
    "\n",
    "        Parameters:\n",
    "            - queries: list[str]\n",
    "                The list of topics.\n",
    "            - stance_df: pd.DataFrame\n",
    "                The dataframe that contains the topics, document ids and stances of the .qrels file.\n",
    "            - model_nno: TFDistilBertForSequenceClassification\n",
    "                The model responsible of distinguishing between \"No stance\", \"Neutral\" and \"Pro object\".\n",
    "            - model_obj: TFDistilBertForSequenceClassification\n",
    "                The model responsible of distinguishing between \"Pro 1st object\" and \"Pro 2nd object\".\n",
    "            - tokenizer: DistilBertTokenizer\n",
    "                The tokenizer class used to tokenize the input of the TFDistilBertForSequenceClassification instances.\n",
    "\n",
    "        Returns:\n",
    "            - pd.DataFrame\n",
    "                The dataframe that contains the stances computed by the model for each element within the .qrels file.\n",
    "    '''\n",
    "\n",
    "    # Where to append the results\n",
    "    trec = pd.DataFrame(columns=['topic', 'doc_id', 'stance'])\n",
    "    # Loop over the topic ids\n",
    "    for topic in stance_df.topic.unique():\n",
    "        print(f\"Computing topic {topic} ...\")\n",
    "        # Get the text for each document relative to a topic inside the stance_df\n",
    "        documents = [corpus_df[corpus_df['id']==docid]['contents'].item() for docid in stance_df[stance_df['topic']==topic]['doc_id']]\n",
    "        batch_df = pd.DataFrame([queries[topic-1] for _ in range(len(documents))], columns=['query'])\n",
    "\n",
    "        batch_df['document'] = documents\n",
    "        batch_LL = batch_df.values.tolist()\n",
    "        #Tokenize the input\n",
    "        tokenized_input = dict(tokenizer(batch_LL, padding=True, truncation='longest_first', return_tensors='tf'))\n",
    "        #Predict the stance with the first model\n",
    "        output = np.argmax(model_nno.predict(tokenized_input, verbose=0).logits,axis=-1)\n",
    "        #Looking for \"Pro object\" predictions\n",
    "        obj_idxs = np.argwhere(output == 2).flatten()\n",
    "\n",
    "        #If some predictions have been \"2\"\n",
    "        if obj_idxs.size > 0:\n",
    "            #Considering the tokenization of the relevant inputs:\n",
    "            tokenized_input['input_ids'] = tf.convert_to_tensor(tokenized_input['input_ids'].numpy()[obj_idxs,:])\n",
    "            tokenized_input['attention_mask'] = tf.convert_to_tensor(tokenized_input['attention_mask'].numpy()[obj_idxs, :])\n",
    "            \n",
    "            #model_obj classifies \"Pro 1st object\" as \"0\" and \"Pro 2nd object\" as \"1\", but they should be classified respectively \"2\" and \"3\", therefore I add \"2\" to the classes. \n",
    "            output_2 = np.argmax(model_obj.predict(tokenized_input, verbose=0).logits,axis=-1) + 2\n",
    "            #I replace output with output_2 where needed.\n",
    "            output[obj_idxs] = output_2\n",
    "\n",
    "        trec = trec.append(pd.DataFrame(zip([topic for _ in range(len(documents))], \n",
    "                                            stance_df[stance_df['topic']==topic]['doc_id'].tolist(),\n",
    "                                            output.tolist()), \n",
    "                                        columns=['topic', 'doc_id', 'stance']))\n",
    "    return trec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a775d81-86d1-447c-af4c-96901f9d2d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and extracting files\n",
      "'C:\\Users\\Andrea\\Downloads\\downloads\\touche-task2-passages-version-002.jsonl' already present\n"
     ]
    }
   ],
   "source": [
    "url_corpus = \"https://zenodo.org/record/6873567/files/touche-task2-passages-version-002.jsonl.gz?download=1\"\n",
    "corpus_df = utils.manage_files.open_df(utils.manage_files.download_files(url_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438333f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing topic 12 ...\n",
      "Computing topic 28 ...\n",
      "Computing topic 34 ...\n",
      "Computing topic 37 ...\n",
      "Computing topic 68 ...\n",
      "Computing topic 92 ...\n",
      "Computing topic 100 ...\n",
      "Computing topic 9 ...\n",
      "Computing topic 22 ...\n",
      "Computing topic 26 ...\n",
      "Computing topic 53 ...\n",
      "Computing topic 58 ...\n",
      "Computing topic 84 ...\n",
      "Computing topic 95 ...\n",
      "Computing topic 17 ...\n",
      "Computing topic 19 ...\n",
      "Computing topic 25 ...\n",
      "Computing topic 27 ...\n",
      "Computing topic 36 ...\n",
      "Computing topic 51 ...\n",
      "Computing topic 76 ...\n",
      "Computing topic 55 ...\n",
      "Computing topic 56 ...\n",
      "Computing topic 67 ...\n",
      "Computing topic 69 ...\n",
      "Computing topic 72 ...\n",
      "Computing topic 78 ...\n",
      "Computing topic 86 ...\n",
      "Computing topic 93 ...\n",
      "Computing topic 30 ...\n",
      "Computing topic 42 ...\n",
      "Computing topic 54 ...\n",
      "Computing topic 59 ...\n",
      "Computing topic 61 ...\n",
      "Computing topic 77 ...\n",
      "Computing topic 88 ...\n",
      "Computing topic 8 ...\n",
      "Computing topic 18 ...\n",
      "Computing topic 33 ...\n",
      "Computing topic 60 ...\n",
      "Computing topic 62 ...\n",
      "Computing topic 74 ...\n",
      "Computing topic 91 ...\n",
      "Computing topic 2 ...\n",
      "Computing topic 3 ...\n",
      "Computing topic 14 ...\n",
      "Computing topic 23 ...\n",
      "Computing topic 43 ...\n",
      "Computing topic 48 ...\n",
      "Computing topic 70 ...\n"
     ]
    }
   ],
   "source": [
    "res_qrels = stance_detection_qrels(topics, stance_df, model_nno, model_obj, tokenizer, corpus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d05e371d-ab91-45c5-8b8d-d6088266e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_map = {'NO': 0, 'NEUTRAL': 1, 'FIRST': 2, 'SECOND': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dd3287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.32      0.43      1012\n",
      "           1       0.22      0.44      0.30       418\n",
      "           2       0.24      0.39      0.30       393\n",
      "           3       0.19      0.10      0.13       284\n",
      "\n",
      "    accuracy                           0.33      2107\n",
      "   macro avg       0.32      0.31      0.29      2107\n",
      "weighted avg       0.42      0.33      0.34      2107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(stance_df['stance'].map(lambda x: stance_map[x]).values.tolist(), res_qrels['stance'].values.tolist()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9960baf88259386db57c734c8604c8e4ab789688672644b3cf73fda24b112c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
