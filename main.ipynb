{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"2bcd3306ccfa4c0cb6b01aeda7e3b5a6","deepnote_cell_type":"markdown","tags":[]},"source":["# Argument retrieval for comparative questions"]},{"cell_type":"markdown","metadata":{"cell_id":"cb6b1be6551347d2a2a308c16fafceb0","deepnote_cell_type":"markdown","tags":[]},"source":["The task has been selected among the ones proposed by Touché at CLEF 2022 and you can find a detailed explanation in the [relative website](https://touche.webis.de/clef22/touche22-web/argument-retrieval-for-comparative-questions.html).   \n","To recap, given a comparative question and a collection of documents we need to retrieve the most relevant text passages for either compared object or for both and to detect their respective stances with respect to the object they talk about. In the first part of the notebook we will explain in detail how we structured the document retrieval part and at the end we will also test the stance detection task (the training and full explanation of which model we used for stance detection can be found in the other notebook 'stance_detection.ipynb' ).\n","\n","In the notebook you will have to use the indexes or some different datasets, in order to speed up the download or the processes that are computationally heavy we created a shared folder on Drive where you can find the files needed to test our proposed system.\n","Link to Drive shared folder: TODO: add link.\n","\n","To be able to mount the shared files you first need to \"Add shortcut to your Drive\" when accessing the specified link."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"c011ae992ca84d848aeb7f7bc04ff958","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"cell_id":"0e7056cd729845fb9c695e7b7ed9195c","deepnote_cell_type":"markdown","tags":[]},"source":["## Installation and import of dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"fc02c5bde5314278947bee75f7b60814","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["!pip tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"b3c2091753704572815c96a6b40c956e","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["import os\n","import re\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","import xml.etree.ElementTree as ET\n","\n","import utils.manage_files"]},{"cell_type":"markdown","metadata":{"cell_id":"d9d5314403d3452f9e1d4f87d00914d4","deepnote_cell_type":"markdown","tags":[]},"source":["## Download datasets"]},{"cell_type":"markdown","metadata":{"cell_id":"06afb5d47fb9421995a85469c57f1a8a","deepnote_cell_type":"markdown","tags":[]},"source":["In order to simplify importing the different necessary files, we decided to create a .tar.gz that contains all of them.   \n","Otherwise the files will be downloaded from the links that were given by the Touché team."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f17bfc28ee03473b8521591ccbd52555","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["# Run the following cell if you want to import the files from Drive\n","!cp /content/drive/MyDrive/NLP_project/downloads.tar.gz .\n","!tar -xvf downloads.tar.gz\n","!rm downloads.tar.gz"]},{"cell_type":"markdown","metadata":{"cell_id":"2ee3ee11650a439390234bd585031b50","deepnote_cell_type":"markdown","tags":[]},"source":["### ClueWeb12 corpus"]},{"cell_type":"markdown","metadata":{"cell_id":"c222aa1da29047f189ae9d45ab569333","deepnote_cell_type":"markdown","tags":[]},"source":["The available documents for the document retrieval task have been selected among the ClueWeb12 dataset. We load it using a function that you can find in the '*utils*' directory, if the file is already present the function simply create a class with the already available file."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ac7f6ca3fb224e9c89677f377cc4641f","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["url_corpus = \"https://zenodo.org/record/6802592/files/touche-task2-passages-version-002.jsonl.gz?download=1\"\n","zip_path_corpus = \"corpus.jsonl.gz\"\n","file_path_corpus = \"corpus.jsonl\"\n","\n","download_corpus = utils.manage_files.DownloadFile(file_path_corpus, zip_path_corpus, url_corpus)\n","download_corpus()"]},{"cell_type":"markdown","metadata":{"cell_id":"aa17fa01e2fe4a5abf98d7a0cc22b0c1","deepnote_cell_type":"markdown","tags":[]},"source":["After that we can load the jsonl on a Pandas dataframe and we can explore the data."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"2d607e80b86d4b1694bf4e3e39106a8b","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["corpus_df = pd.read_json(download_corpus.file_name, lines=True)\n","corpus_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"fb156134dda84d6bb663c4f0d6ea9174","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["print(f\"The corpus has {len(corpus_df)} elements.\")"]},{"cell_type":"markdown","metadata":{"cell_id":"b610ebfcf3db4cd3b1f86e7495664a09","deepnote_cell_type":"markdown","tags":[]},"source":["### Topics, quality, relevance and stance"]},{"cell_type":"markdown","metadata":{"cell_id":"606be5df1aaa443ea2504590d80e84aa","deepnote_cell_type":"markdown","tags":[]},"source":["In this subsection we download the list of possible topics, that are the actual queries to submit to the model and the quality, relevance and stance qrels files that will be used to evaluate our different pipelines.  \n","The list of ***topics*** contain 100 arguments but for this task only 50 were selected by the team, therefore we will evaluate our models on these ones.\n","\n","The .qrels files have four columns: TOPIC, Q0, DOC_ID, SCORE."]},{"cell_type":"markdown","metadata":{"cell_id":"4df8ca87aec444baa9b9b7515fc27419","deepnote_cell_type":"markdown","tags":[]},"source":["#### Topics"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"608eb5aeac694d318d8649b0f6d6ea6f","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["# Download and parse the xml file of the 0-50 topics\n","url_topics = \"https://zenodo.org/record/6873559/files/topics-task-2.zip?download=1\"\n","zip_path_topics = \"topics-task-2.zip\"\n","file_path_topics = \"topics-task-2\"\n","\n","download_topics = utils.manage_files.DownloadFile(file_path_topics, zip_path_topics, url_topics)\n","download_topics()\n","# Retrieve a list of strings from the xml\n","topics = utils.manage_files.open_xml(download_topics.file_name + \"/topics-task-2.xml\")\n","\n","# Topics from 51 to 100\n","url_topics_21 = \"https://zenodo.org/record/6873565/files/topics-task-2-2021.zip?download=1\"\n","zip_path_topics_21 = \"topics-task-2-2021.zip\"\n","file_path_topics_21 = \"topics-task-2-2021\"\n","\n","download_topics_21 = utils.manage_files.DownloadFile(file_path_topics_21, zip_path_topics_21, url_topics_21)\n","download_topics_21()\n","topics += utils.manage_files.open_xml(download_topics_21.file_name + \"/topics-task2-51-100.xml\")"]},{"cell_type":"markdown","metadata":{"cell_id":"cbdec9abbb41492b97830d6d81a3a24e","deepnote_cell_type":"markdown","tags":[]},"source":["As you can see below we have a list of 100 topics but for the evaluation we will use only the ones selected by the Touchè team."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f5a76a4eed554b479f6c6e5079f662c9","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["print(f\"There are {len(topics)} topics.\\n{topics}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"c352db73b8ae4c5aafcf052b50fa9d25","deepnote_cell_type":"markdown","tags":[]},"source":["#### Relevance"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"77ab5621271840fb97ceb3376979bac6","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["# Download relevance qrels 50 topics\n","url_relevance = \"https://zenodo.org/record/6873567/files/touche-task2-2022-relevance.qrels?download=1\"\n","file_path_rel = \"relevance.qrels\"\n","\n","download_relevance = utils.manage_files.DownloadFile(file_path_rel, url=url_relevance)\n","download_relevance()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"9639585906744904aa2e119852409dc4","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["relevance_df = pd.read_csv(download_relevance.file_name, index_col=None, \n","                    names=[\"topic\", \"0\", \"doc_id\", \"relevance\"], sep=\" \").drop(\"0\", axis=1)\n","\n","relevance_df.head()"]},{"cell_type":"markdown","metadata":{"cell_id":"624586f8e57e410b85eb9b10733efbb1","deepnote_cell_type":"markdown","tags":[]},"source":["#### Quality"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f8754e3fafa54e5b8ff59630a2b0cd3d","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["# Download relevance qrels 50 topics\n","url_quality = \"https://zenodo.org/record/6873567/files/touche-task2-2022-quality.qrels?download=1\"\n","file_path_qual = \"quality.qrels\"\n","\n","download_quality = utils.manage_files.DownloadFile(file_path_qual, url=url_quality)\n","download_quality()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"732b6e361f7c4dcf8d4df8b9e5774016","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["quality_df = pd.read_csv(download_quality.file_name, index_col=None, \n","                    names=[\"topic\", \"0\", \"doc_id\", \"quality\"], sep=\" \").drop(\"0\", axis=1)\n","\n","quality_df.head()"]},{"cell_type":"markdown","metadata":{"cell_id":"d687e7dafd1a47179d355be173bd3b03","deepnote_cell_type":"markdown","tags":[]},"source":["#### Stance"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4eac04f399d140428735a1637ebb05df","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["# Download relevance qrels 50 topics\n","url_stance = \"https://zenodo.org/record/6873567/files/touche-task2-2022-stance.qrels?download=1\"\n","file_path_stance = \"stance.qrels\"\n","\n","download_stance = utils.manage_files.DownloadFile(file_path_stance, url=url_stance)\n","download_stance()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"3264cb92d80e49fa9abd5415a3d15c8d","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["stance_df = pd.read_csv(download_stance.file_name, index_col=None, \n","                    names=[\"topic\", \"0\", \"doc_id\", \"stance\"], sep=\" \").drop(\"0\", axis=1)\n","\n","stance_df.head()"]},{"cell_type":"markdown","metadata":{"cell_id":"52e15bb2800044099f75407fcd39a82d","deepnote_cell_type":"markdown","tags":[]},"source":["## Document retrieval and ranking"]},{"cell_type":"markdown","metadata":{"cell_id":"3669d78c588b46529baf9f1f399731b5","deepnote_cell_type":"markdown","tags":[]},"source":["During the task all the teams had been provided with an API key for the [ChatNoir](https://www.chatnoir.eu/doc/) system. It is an Elasticsearch-based search engine offering a document retrieval interface for different corpus (including ClueWeb12). The API returns the most relevant documents with respect to a query and further information for each of them, such as the BM25 score, the page rank score and the spam score. Unfortunately we were not able to obtain an API key, therefore we decided to create our own indexes on which to perform document retrieval. There are different types of indexes used in IR, the main ones are:\n","- ***sparse indexes***: is a type of index that only stores a subset of the terms that appear in a document collection. This can be useful for optimizing index size and search efficiency when working with large collections.\n","- ***dense indexes***: we store the embeddings of the documents, in our case created by TCT Colbert, therefore we are capturing a semantic meaning into a fixed length vector. In order to retrieve the most similar to a given query the index computes a distance between the embedding of the query and the others, we decided to use the inner product that is a default metric.\n","\n","To improve the robustness and the quality of our retrieval we decided to mix up the two approaches considering a hybrid pipeline and an approach that uses a sparse index and reranking made by MonoT5. \n"]},{"cell_type":"markdown","metadata":{"cell_id":"ce8e2320a23f4045990edbc29a02597f","deepnote_cell_type":"markdown","tags":[]},"source":["\n","We found on the web different valid libraries that allow to create an index given a set of documents.\n","- The first one is [Pyserini](https://github.com/castorini/pyserini), a Python toolkit for reproducible information retrieval research with sparse and dense representations. The sparse index can be created on a custom collection of documents while the creation of a dense index for our own documents is not currently available.\n","- This led us to look for another library that allows building a dense index and we found [autofaiss](https://github.com/criteo/autofaiss). ***autofaiss*** creates [Faiss](https://github.com/facebookresearch/faiss) knn indexes selecting the most optimal similarity search parameters. It only needs the embedding vectors for each document, that we computed using the ***pyserini encode*** module and [TCT-Colbert](https://arxiv.org/pdf/2010.11386.pdf) pre-trained on the second version of [MS MARCO dataset](https://microsoft.github.io/msmarco/).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"91286cf6f6a2455f9447ea00ba49ef26","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["# Installation of the different libraries for using the indexes\n","!pip install -q pyserini\n","!pip install -q faiss-cpu==1.7.2\n","!pip install -q autofaiss"]},{"cell_type":"markdown","metadata":{"cell_id":"9cf0e11f56824d8fac3e2a3e98b97594","deepnote_cell_type":"markdown","tags":[]},"source":["At this point you can simply run the following cell to import the pre-saved indexes and ignore the subsections that explain the creation and go to the 'Models' section."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"b3eb537035bf4d4886ba2af105937659","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["# Load the indexes from Drive\n","!cp /content/drive/MyDrive/NLP_project/indexes.zip .\n","!unzip indexes.zip\n","!rm indexes.zip"]},{"cell_type":"markdown","metadata":{"cell_id":"c812c850c342424c822443a14f55fe1e","deepnote_cell_type":"markdown","tags":[]},"source":["### Creation of a sparse index "]},{"cell_type":"markdown","metadata":{"cell_id":"bebb039b5bfe4dcd883d863bd853eed3","deepnote_cell_type":"markdown","tags":[]},"source":["In order to create a sparse index given a .jsonl file, Pyserini needs to have only 2 keys, 'id' and 'contents', so we have to remove the other columns from the dataframe and save the new .jsonl file.   \n","First of all we create a 'collections' dir where to put the file and then we save it."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"619e473cd51c49a4971f5ef54bba8370","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["!mkdir collections\n","corpus_df.drop('chatNoirUrl', axis=1).to_json('collections/corpus_index.jsonl', orient=\"records\", lines=True)"]},{"cell_type":"markdown","metadata":{"cell_id":"d91a7c06f54d4c9283b6e83c224a70d7","deepnote_cell_type":"markdown","tags":[]},"source":["Now we can run the command to create the sparse index with different parameters:\n","- ```--collection JsonCollection``` is used to indicate to the documents ingestor that the documents in input are inside a json file.\n","- ```--input collections```, it's simply the directory where to find the json file.\n","- ```--index indexes/sparse_index```, it's the directory where to save the index files.\n","- ```--bm25.accurate``` if set, Anserini uses an algorithm that is more computationally expensive but more accurate. The \"accurate\" variant of BM25 computes the idf of terms by taking into account accurate document lengths. If not set an approximation for idf will be used.\n","- ```--generator DefaultLuceneGenerator```, is the default generator to create the index.\n","- ```--threads 2``` represents the number of threads to use while creating the index.\n","- ```--storePositions``` stores term positios, needed for phrase queries.\n","- ```--storeDocvectors``` stores document vectors, needed for (pseudo) relevance feedback.\n","- ```-storeRaw``` stores raw source documents."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"73443c0285614f0b8d14255829c7778c","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["# Create the sparse index \n","!python -m pyserini.index.lucene --collection JsonCollection --input collections/ --index indexes/sparse_index --bm25.accurate --generator DefaultLuceneDocumentGenerator --threads 2 --storePositions --storeDocvectors --storeRaw"]},{"cell_type":"markdown","metadata":{"cell_id":"6a1ca44c7a2d4966ada26d19ec87a3e7","deepnote_cell_type":"markdown","tags":[]},"source":["By default Pyserini performs stemming with 'porter' and stopwords removal on the input texts."]},{"cell_type":"markdown","metadata":{"cell_id":"72229a882f264ec587dc9b1f42eb22bf","deepnote_cell_type":"markdown","tags":[]},"source":["### Creation of a dense index"]},{"cell_type":"markdown","metadata":{"cell_id":"8aee81a3a1034d0397eb42b9b3b8c223","deepnote_cell_type":"markdown","tags":[]},"source":["As explained before, Pyserini doesn't support yet the creation of a dense index on a custom documents collection. Thus we first created the embedding of the documents with Pyserini and then we created the index with autofaiss.   \n","The parameters passed to the encode module are:\n","- ```--corpus```, that is the .jsonl file where to find the documents (the same as before).\n","- ```--fields```, the key of the json to consider for the embedding.\n","- ```--shard-id 0```, the number of shard in case we want to split the index.\n","- ```--shard-num 1```, in our case we have only one shard, but here you can set multiple shard and then you should run the command multiple times changing the shard-id.\n","- ```--embeddings```, the directory where to put the embeddings once computed.\n","- ```--encoder```, the encoder to use in order to compute the embeddings (in our case TCT Colbert pre-trained on the second version of MS MARCO).\n","- ```--fields```, needs to be equal to the previous --fields parameter.\n","- ```--batch```, the batch to use.\n","- ```--fp16```, to speed up the computation if PyTorch autocast is used for inference."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"75cd70331d1b4ecba9597e1bf3edd61e","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["# Compute the embeddings\n","!python -m pyserini.encode input --corpus collections/corpus_index.jsonl --fields text --shard-id 0 --shard-num 1 output --embeddings embeddings/ encoder --encoder castorini/tct_colbert-v2-hnp-msmarco --fields text --batch 32 --fp16"]},{"cell_type":"markdown","metadata":{"cell_id":"b604deb0ad3f4da9862ac9816e165672","deepnote_cell_type":"markdown","tags":[]},"source":["After that we have the embeddings in a .jsonl file, in the 'vector' column, we need to split this very huge file into smaller ones in order to make the size suitable for the RAM. Moreover autofaiss taks as input .npy files therefore we decided to split the embedding vectors in .npy files that contains 75000 vectors each."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4c0e30787b924034b3d2e809b07fe73d","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["# Create the npy_embeddings directory\n","!mkdir npy_embeddings\n","\n","import tqdm\n","\n","def convert_to_npy(path, file_len, chunksize=75000):\n","    '''\n","        It takes as input a .jsonl file and it creates some .npy files taking\n","        only the 'vector' key, that is the embedding of the documents. It saves\n","        the results in the npy_embeddings directory.\n","        Parameters:\n","            - path: str \n","                The path of the .jsonl file that contains the embeddings in the 'vector' key.\n","            - file_len: int\n","                The number of elements inside the .jsonl file.\n","            - chunksize: int\n","                The number of lines to read at each step, so the number of vectors for each new\n","                .npy file.\n","    '''\n","    steps = file_len//chunksize\n","    for i, chunk in enumerate(tqdm.tqdm(pd.read_json(path, lines=True, chunksize=chunksize), total=steps)):\n","        npy_list = []\n","        for vect in chunk['vector'].to_numpy():\n","            npy_list.append(vect)\n","\n","        # Save different files to avoid RAM consumption\n","        np.save(f'npy_embeddings/embeddings_{i+10}.npy', np.array(npy_list))\n","        del npy_list"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4ae8de39a391436a9c0f43e4bbb2f4c1","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["# Actually call the function\n","file_len = 868655\n","convert_to_npy('embeddings/embeddings.jsonl', file_len)"]},{"cell_type":"markdown","metadata":{"cell_id":"f14947be136345f892d7174f76b9ab5b","deepnote_cell_type":"markdown","tags":[]},"source":["Given the embedding vectors autofaiss automatically creates a dense index executing the following cell:"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"030e9454cf1a47c387e92d6d9494f78a","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["from autofaiss import build_index\n","\n","# Load the .npy files from the \"npy_embeddings\" directory where we saved them\n","build_index(embeddings=\"npy_embeddings\", index_path=\"indexes/knn.index\",\n","            index_infos_path=\"indexes/dense_index_infos.json\", max_index_memory_usage=\"6GB\",\n","            current_memory_available=\"9GB\")"]},{"cell_type":"markdown","metadata":{"cell_id":"998dc26410c846a5a75c40d4daa3e5e8","deepnote_cell_type":"markdown","tags":[]},"source":["## Models"]},{"cell_type":"markdown","metadata":{"cell_id":"fbb3eee1864d43888f823e1114ffe494","deepnote_cell_type":"markdown","tags":[]},"source":["To load the pre-saved indexes from the Drive shared folder run the following cell:"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"efb2465719d44d6b9314c9877f49ebba","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["# Load the indexes from Drive\n","!cp /content/drive/MyDrive/NLP_project/indexes.zip .\n","!unzip indexes.zip\n","!rm indexes.zip"]},{"cell_type":"markdown","metadata":{"cell_id":"21b760611c9248b38f0a1599e7be841a","deepnote_cell_type":"markdown","tags":[]},"source":["We created a ```DocumentsIndex``` class to import the dense and sparse indexes and to set different parameters, you can find it in the ```src``` directory of the project.\n","\n","Before presenting the different pipelines that we implemented for document retrieval, we create the instance for the dense index, since it was quite heavy in memory and we use the same instance for all the pipelines that need it. \n","We also create a directory for saving the results of the search."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7243e7c84a1743049135da89a67ff22c","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["!mkdir results\n","from src.DocumentsIndex import DocumentsIndex"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"e2d8337f96274e129e6bbb54c609e30a","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["dense_index = DocumentsIndex('indexes/knn.index', 'dense')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The following 2 functions can be used to print the nCDG score and to return the list of urls given the corpus and the dictionary of the results."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def print_ndcg(ndcg_scores):\n","    '''\n","        It prints the nDCG score for each key in the input.\n","\n","        Parameters: \n","            - ndcg_scores: dict\n","                The dictionary that contains for each .qrels file\n","                the mean nDCG.\n","    '''\n","    if ndcg_scores and isinstance(ndcg_scores, dict):\n","        for key, ndcg in ndcg_scores.items():\n","            print(f\"The nDCG for {key} qrel is:\")\n","            print(ndcg, '\\n')\n","    else:\n","        print(\"nDCG has not been computed, set 'evaluate=True' to compute it.\")\n","\n","\n","# Retrieve the urls from the corpus given the results of the search on the index\n","def retrieve_docs_ranked(corpus, hits, k=10):\n","    '''\n","        It returns a list of ids and urls retrieved from the corpus\n","        using the ids within hits.\n","\n","        Parameters:\n","            - corpus: pd.DataFrame\n","                The corpus from which we want to retrieve the urls.\n","            - hits: dict\n","                The dictionary that has ids and scores for a specific\n","                topic.\n","            - k: int\n","                The number of elements to returns. \n","    '''\n","    urls = []\n","    for el in hits['ids'][:k]:\n","        urls.append(corpus[corpus['id'] == el]['chatNoirUrl'].item())\n","    ids = [val for val in hits['ids'][:k]]\n","    return ids, urls"]},{"cell_type":"markdown","metadata":{"cell_id":"ec2c0bfde4eb4c4a9d94ec56f299f6d9","deepnote_cell_type":"markdown","tags":[]},"source":["### Sparse index (BM25 score)"]},{"cell_type":"markdown","metadata":{"cell_id":"9dce19e089594a5a95007f11660d8ce4","deepnote_cell_type":"markdown","tags":[]},"source":["It's a probabilistic retrieval model for estimating the relevance of a passage given a query. In this simple pipeline we use the sparse index created before, so considering the accurate BM25 for the ranking and we compute the nDCG@5 as all the teams did during the task. This pipeline is the ***baseline*** for a retrieval approach and it's the less effective in terms of nDCG@5 computed on the quality and relevance. \n","\n","As default value for the parameters of BM25, we leave k=0.9 and b=0.4 because these are the ones that work better in combination with the dense index in the hybrid pipeline. \n","\n","To get the best possible results in terms of nDCG@5 using only the sparse index the best values for the parameters are k=1.15 and b=0.75.\n","\n","To see how the pipeline is implemented you can check ```src/SparsePipeline.py``` file that contains the related class. In the following experiments we retrieve the top-40 documents since for computing the nDCG@5 score we would only need 5 of them."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"27997e0f91d9461ab77e824325bd99fb","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["from src.SparsePipeline import SparsePipeline\n","\n","# The index with the default values (k=0.9, b=0.4)\n","sparse_index = DocumentsIndex('indexes/sparse_index', 'sparse')"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"bb10409e64814c0bb8558741e4f0164b","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["sparse_pipeline = SparsePipeline('results', \"sparse_pipeline\", sparse_index)\n","\n","# Evaluate the pipeline both on relevance and quality .qrels\n","sparse_scores, ndcg_sparse = sparse_pipeline.compute_results(\n","                                ['downloads/relevance.qrels', 'downloads/quality.qrels'],\n","                                topics, k=40, evaluate=True,\n","                                clean_query=False\n","                             )\n","\n","print_ndcg(ndcg_sparse)"]},{"cell_type":"markdown","metadata":{"cell_id":"08a037a0d3ad48f5a6c75d889b0dd506","deepnote_cell_type":"markdown","tags":[]},"source":["In the following cells you can find the best possible result if you want to use only the sparse index."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"a7c70b3302c0475784a29c59053c5c07","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["# Set the parameters of bm25\n","sparse_index_best = DocumentsIndex('indexes/sparse_index', 'sparse', \n","                                    set_bm25=True, k1=1.15, b=0.75)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"08234090ec4840a99a1945bd89b28fa4","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["sparse_pipeline_best = SparsePipeline('results', \"sparse_pipeline_best\", sparse_index_best)\n","\n","# Evaluate the pipeline both on relevance and quality .qrels\n","sparse_scores_best, ndcg_sparse_best = sparse_pipeline_best.compute_results(\n","                                ['downloads/relevance.qrels', 'downloads/quality.qrels'],\n","                                topics, k=40, evaluate=True,\n","                                clean_query=False\n","                             )\n","\n","print_ndcg(ndcg_sparse_best)"]},{"cell_type":"markdown","metadata":{"cell_id":"d935493b5671482d93f2ab04dabb5c09","deepnote_cell_type":"markdown","tags":[]},"source":["### Dense index"]},{"cell_type":"markdown","metadata":{"cell_id":"e7673a2642294367b2fa911d990c5ab3","deepnote_cell_type":"markdown","tags":[]},"source":["In this case the approach is quite different, as seen before the index computes the knn between the query embedding and the document embeddings. Which type of encoder did we use?\n","\n","We decided to use a version of TCT-Colbert pre-trained on the second version of MS MARCO dataset. You can find more information about TCT-Colbert in the [paper](https://arxiv.org/pdf/2010.11386.pdf). \n","The general idea is that the embeddings capture the semantic meaning of the documents, capturing also a notion of terms importance and then we will explore the similarities between embeddings considering the inner product between the vectors.\n","\n","In this pipeline we need to encode the queries with TCT-Colbert before giving to the index, the entire process is done within the 'search' function implemented in the ```DocumentsIndex``` class. After that we encoded the queries we can give the vectors to the index, then it will retrieve the top-k documents with the highest inner product with respect to the query embedding.\n","\n","Moreover we used a parameter to clean the queries removing punctuation and in the case of the dense index this led us to the best nDCG@5."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"58450eb7ad534866943d69f30bc58e74","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["from src.DensePipeline import DensePipeline\n","\n","dense_pipeline = DensePipeline('results', 'dense_pipeline', dense_index)\n","\n","dense_scores, ndcg_dense = dense_pipeline.compute_results(\n","                                ['downloads/relevance.qrels', 'downloads/quality.qrels'],\n","                                topics, corpus_df, k=40, clean_query=True,\n","                                evaluate=True\n","                            )\n","\n","print_ndcg(ndcg_dense)"]},{"cell_type":"markdown","metadata":{"cell_id":"0dfedca0ada942bb82c75ad78cfd55ec","deepnote_cell_type":"markdown","tags":[]},"source":["### Hybrid pipeline (sparse + dense index)"]},{"cell_type":"markdown","metadata":{"cell_id":"b375c24c2b154465a13c26296d382bea","deepnote_cell_type":"markdown","tags":[]},"source":["We also decided to test a combination of the 2 approaches, retrieving k documents from both indexes and combining in a clever way the obtained scores. We read the main idea behind the following algorithm in the HybridSearcher class of Pyserini.\n","\n","**Algorithm**\n","1. First of all we save the minimum and maximum scores of the retrieved documents for both the sparse and the dense indexes (0 if no documents retrieved).\n","2. Then we iterate over the union of the ids that have been retrieved from the indexes, at this point if a document was found by an index, the relative score will be taken, otherwise the minimum score will be considered.\n","3. If we want we can normalize the scores and at the end we sum the scores multiplying the sparse score for an alpha value (in our case alpha=0.2). \n","4. At the end we return the re-ranked list considering the new computed scores.\n","\n","Multiplying by alpha we are giving more weight to the dense index, this is why we know that it performs better and we don't want that the 2 scores have the same weights on the sum."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"e23cf8fa1a2c490595039bdbce6f3046","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":["from src.HybridPipeline import HybridPipeline\n","\n","hybrid_pipeline = HybridPipeline('results', 'hybrid_pipeline', sparse_index, dense_index)\n","\n","scores_hybrid, ndcg_hybrid = hybrid_pipeline.compute_results(\n","                                ['downloads/relevance.qrels', 'downloads/quality.qrels'],\n","                                topics, corpus_df, k=700, alpha=0.2, evaluate=True\n","                             )\n","\n","print_ndcg(ndcg_hybrid)"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown","tags":[]},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ce24eff3-eda0-4403-93ec-34f248019e53' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"f2cb42561d664a6eab2fdd8e858d4bc2","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]"},"orig_nbformat":2,"vscode":{"interpreter":{"hash":"a9960baf88259386db57c734c8604c8e4ab789688672644b3cf73fda24b112c6"}}},"nbformat":4,"nbformat_minor":0}
